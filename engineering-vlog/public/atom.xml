<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Videocall Engineering</title>
    <subtitle>Videocall Engineering</subtitle>
    <link rel="self" type="application/atom+xml" href="https://engineering.videocall.rs/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://engineering.videocall.rs"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2025-06-22T00:00:00+00:00</updated>
    <id>https://engineering.videocall.rs/atom.xml</id>
    <entry xml:lang="en">
        <title>How LLMs See Illusory Faces</title>
        <published>2025-06-22T00:00:00+00:00</published>
        <updated>2025-06-22T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/pareidolia/"/>
        <id>https://engineering.videocall.rs/posts/pareidolia/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/pareidolia/">&lt;h4 id=&quot;background&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Background &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Seeing faces in inanimate objects —a phenomenon called pareidolia— is a common human experience. With today&#x27;s powerful Vision-Language Models (VLMs), a simple question arises: do they see these illusory faces too? Probing their &quot;vision&quot; this way isn&#x27;t just a fun experiment. It&#x27;s a practical way to understand their inner workings, challenge our assumptions when building with them, and explore the gap between artificial and biological sight.&lt;&#x2F;p&gt;
&lt;p&gt;This question grew out of my master&#x27;s research, where I used EEG to study how the human brain processes these very illusions, which led to an &lt;a href=&quot;https:&#x2F;&#x2F;papers.ssrn.com&#x2F;sol3&#x2F;papers.cfm?abstract_id=4341900&quot;&gt;ERP paper&lt;&#x2F;a&gt; on the topic.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-is-spatial-frequency-and-coarse-to-fine-theory&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; What is Spatial Frequency and Coarse-to-Fine Theory? &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;So, how do you fairly test an AI&#x27;s perception? My plan was to see if it falls for the same visual shortcuts our brains do.&lt;&#x2F;p&gt;
&lt;p&gt;This is based on a key idea in human vision called the &lt;strong&gt;Coarse-to-Fine&lt;&#x2F;strong&gt; theory. In short, human brain processes the blurry, general, coarse, &quot;gist&quot; of something first, and then uses that initial guess to figure out the finer details more quickly. The technical way to separate the &quot;gist&quot; from the &quot;details&quot; is with &lt;strong&gt;spatial frequencies&lt;&#x2F;strong&gt;, which can be isolated using techniques like 2D Fourier filtering.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Low Spatial Frequencies (LSF)&lt;&#x2F;strong&gt; are the blurry, large-scale shapes.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;High Spatial Frequencies (HSF)&lt;&#x2F;strong&gt; are the sharp edges and fine textures.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;You experience this all the time. Think about recognizing someone from across the street—you see their overall shape long before you see their eyes. While not about spatial frequency directly, a study on hierarchical processing shows a similar &quot;general first&quot; principle: people can spot an &lt;em&gt;animal&lt;&#x2F;em&gt; in an image in just 120ms, but need longer to identify it as a &lt;em&gt;dog&lt;&#x2F;em&gt; (&lt;a href=&quot;https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;25208739&#x2F;&quot;&gt;Wu et al., 2014&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
&lt;p&gt;My whole experiment was designed around this: would the AI also see a face in the blur, but get confused by the sharp details? To test this, I needed to isolate these frequencies.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;sf.png&quot; alt=&quot;SF&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Visualizing the Coarse-to-Fine theory. The left shows an image broken into coarse (Low Frequency) and fine (High Frequency) information. The right shows how the brain processes the coarse &#x27;gist&#x27; first to guide perception.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;To do this, I used a Butterworth filter from my own Rust tool, which you can try out here: &lt;a href=&quot;https:&#x2F;&#x2F;altunenes.github.io&#x2F;butter2d&#x2F;&quot;&gt;butter2d&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-llms-see-illusions&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; How LLMs see illusions &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Research shows that modern VLMs are not objective, infallible observers; they can be &quot;fooled&quot; by classic visual illusions, and their susceptibility often increases with model scale &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00047&quot;&gt;Shen et al., 2023&lt;&#x2F;a&gt; . This suggests they are learning statistical heuristics from their training data that mimic human perception, rather than developing a deep, structural understanding of the world.&lt;&#x2F;p&gt;
&lt;p&gt;This makes pareidolia a particularly interesting test. It&#x27;s not a geometric trick, but an illusion driven by a powerful, top-down, and likely evolutionary bias to find faces in our environment.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;the-experiment&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; The Experiment &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;To test how Gemini handles pareidolia, I took several images and created three versions of each (using butterworth filter): the original &lt;strong&gt;Broadband (BB)&lt;&#x2F;strong&gt;, a blurry &lt;strong&gt;Low Spatial Frequency (LSF)&lt;&#x2F;strong&gt; version, and a sharp-edged &lt;strong&gt;High Spatial Frequency (HSF)&lt;&#x2F;strong&gt; version. I then fed them to the model with a simple prompt. To avoid any &quot;memory&quot; or context-priming effects, each images was processed in a completely separate session.&lt;&#x2F;p&gt;
&lt;p&gt;Here was the prompt:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&quot;What are the three most prominent objects you see in this image? Respond in a JSON format where each object has a &#x27;name&#x27; and a &#x27;confidence_score&#x27;.&quot;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Here are the results.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;test-1-the-wire-face&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Test 1: The Wire Face &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;This image of server cables, which I found on X (formerly Twitter), has an uncanny facial structure. As hypothesized, the LLM completely missed the face in the broadband and HSF versions, describing only the literal content. However, when presented with the LSF version, where only the coarse, global shape remains, it immediately and confidently identified a &lt;strong&gt;&quot;Face&quot;&lt;&#x2F;strong&gt;. So the fine HSF details of the wires and components seem to break the illusion for the model, while the blurry LSF version provides the ideal template for a face.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;W.png&quot; alt=&quot;W&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;LSF (left), Broadband (middle), HSF (right) versions of the &#x27;Wire Face&#x27;.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h4 id=&quot;test-2-the-church-face&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Test 2: The Church Face &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Next, I used one of the most famous pareidolia images on the internet. My rationale was that if the model&#x27;s perception is purely a function of its training data, it would have surely seen this image and would recognize the face (or illusory face). Once again, it failed to see the face in the BB and HSF versions, focusing only on the architecture. But in the LSF version, it correctly identified a &lt;strong&gt;&quot;Face (Pareidolia)&quot;&lt;&#x2F;strong&gt;. This suggests the model&#x27;s failure isn&#x27;t just about a lack of training data. The high-frequency details of the building&#x27;s facade actively mask the illusion for the AI, even for a classic example.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;C.png&quot; alt=&quot;C&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The famous &#x27;Church Face&#x27; pareidolia.&lt;&#x2F;em&gt;&lt;br&gt;&lt;em&gt;Note that the model only sees a &quot;Face (Pareidolia)&quot; in the LSF version, describing only architectural elements in the others.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h4 id=&quot;test-3-the-oval-hypothesis&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Test 3: The Oval Hypothesis &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;The previous results sparked a new idea: perhaps the LLM&#x27;s internal &quot;face template&quot; is strongly biased towards the oval, rounded shapes of human faces? The &quot;Wire Face&quot; is very angular. To test this, I selected two images with a more circular structure.&lt;&#x2F;p&gt;
&lt;p&gt;The first, an electrical component, followed the now-established pattern. A &lt;strong&gt;&quot;Illusory Face&quot;&lt;&#x2F;strong&gt; was detected in the LSF version, but the BB and HSF versions were seen only as literal machine parts.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;E.png&quot; alt=&quot;E&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;An illusory face in an electrical component.&lt;&#x2F;em&gt;&lt;br&gt;&lt;em&gt;The LSF version triggered a &quot;Pareidolia Face&quot; detection, while the detailed versions only yielded descriptions of machine parts.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The second image, however, gave a breakthrough result. This time, the LLM saw a face in &lt;strong&gt;all three versions!&lt;&#x2F;strong&gt; It identified a &quot;Face&quot; in both LSF and BB, and even a &lt;strong&gt;&quot;Illusory Face&quot;&lt;&#x2F;strong&gt; in the sharp HSF image. This is a brilliant finding. It suggests that when an object&#x27;s structure is a strong enough match for the AI&#x27;s internal face template (a round shape, two distinct &quot;eyes,&quot; a &quot;mouth&quot;), it can overcome the distracting HSF noise. This is also highly consistent with human vision, where HSF information is vital for analyzing the fine features &lt;em&gt;of a face&lt;&#x2F;em&gt; once it has been detected.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;M.png&quot; alt=&quot;M&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;A illusory face on a can or clock mechanism that the LLM saw in all versions.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h4 id=&quot;test-4-the-illusory-robot&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Test 4: The Illusory Robot &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;This next test uses a common object that happens to have face-like features: a set of viewpoint binoculars. The results show another interesting form of interpretation by the model. In the LSF version, the blurry shape with two prominent circles triggers an anthropomorphic classification: &lt;strong&gt;&quot;robot&quot;&lt;&#x2F;strong&gt;. The model defaults to a familiar humanoid template. However, once the HSF details are available in the broadband and sharp versions, the model corrects its initial &quot;guess&quot; and accurately identifies the object as &lt;strong&gt;&quot;viewpoint binoculars&quot;&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;R.png&quot; alt=&quot;R&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;An LSF-induced &quot;robot&quot; is corrected into binoculars with more detail.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h4 id=&quot;test-5-the-holistic-face-paintings&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Test 5: The Holistic Face Paintings &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Finally, I moved from pure pareidolia to a different kind of illusion: composite portraits, famously painted by artists like Giuseppe Arcimboldo. In these images, the face is &lt;strong&gt;intentionally&lt;&#x2F;strong&gt; constructed from other objects. These aren&#x27;t really pareidolia in the same way; they are deliberate artistic constructions where recognizing the face requires &lt;strong&gt;holistic processing&lt;&#x2F;strong&gt;—seeing the overall arrangement rather than just the sum of the individual parts. How would the LLM fare?&lt;&#x2F;p&gt;
&lt;p&gt;The first painting is a face constructed from a landscape. Interestingly, the model identified a &lt;strong&gt;&quot;large face&quot;&lt;&#x2F;strong&gt; in both the broadband and high-frequency versions. This is a departure from the earlier pareidolia examples. Here, the individual components (trees, rocks) don&#x27;t look like facial features on their own, but their careful arrangement creates a powerful holistic impression that the model was able to perceive, even with all the fine details present.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;P.png&quot; alt=&quot;P&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;A composite face, testing holistic perception.&lt;&#x2F;em&gt;&lt;br&gt;&lt;em&gt;Notably, the model identified the face in all three versions, even describing it as an &#x27;optical illusion&#x27; in the BB.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;I tried a second, similar painting of a shepherd in a landscape forming a face. The results were just as intriguing. In the full-detail versions, the model successfully identified both the whole (&lt;strong&gt;&quot;Face&quot;&lt;&#x2F;strong&gt;) and the parts (&lt;strong&gt;&quot;Sheep&quot;&lt;&#x2F;strong&gt;, &lt;strong&gt;&quot;Shepherd&quot;&lt;&#x2F;strong&gt;). It seemed to parse the image on multiple levels simultaneously.&lt;&#x2F;p&gt;
&lt;p&gt;However, an interesting twist occurred in the LSF version. The blur, which helped reveal faces in the pareidolia examples, seemed to &lt;em&gt;weaken&lt;&#x2F;em&gt; the illusion here. The model&#x27;s top guess for the LSF version was &lt;strong&gt;&quot;Tree&quot;&lt;&#x2F;strong&gt;, not &quot;Face&quot;. This might suggest that for these complex, deliberately constructed images, the precise arrangement and HSF details are actually &lt;em&gt;critical&lt;&#x2F;em&gt; for the holistic face to emerge, and blurring them can break the carefully crafted composition. It&#x27;s a fascinating case where the general rule (LSF reveals faces) is reversed, highlighting the complexity of both human and machine perception.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;P2.png&quot; alt=&quot;P2&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Another composite face, where blur seemed to hinder, rather than help, perception.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h3 id=&quot;conclusion-and-final-thoughts&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Conclusion and Final Thoughts &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;For anyone working with or building on top of these AI systems, I believe understanding these kinds of behaviors is important. An AI&#x27;s failure to see a pattern that is obvious to us—or its tendency to see one only under specific conditions like blurring—highlights the inherent differences in how they process visual information.&lt;&#x2F;p&gt;
&lt;p&gt;This method of probing with spatial frequencies and illusions could serve as a simple, fun, intuitive benchmark for tracking the progress of future vision models. As new architectures are developed, seeing how they handle these edge cases can tell us a lot about whether they are developing more robust, human-like perception or simply becoming better at pattern-matching their training data.&lt;&#x2F;p&gt;
&lt;p&gt;Of course, there are clear limitations here. I only used one model, Gemini 2.5 Pro, primarily because company I work provides free access to it. Other powerful models from OpenAI, Anthropic, or elsewhere might react to these images in completely different ways. The number of images was also small.&lt;&#x2F;p&gt;
&lt;p&gt;Also you might be realized that model&#x27;s own distinction between a &quot;Face&quot; and a &quot;Pareidolia Face&quot;. What is the difference? When the model uses the simple &quot;Face&quot; label, does it believe it&#x27;s seeing a real person or animal? Is the &quot;Pareidolia&quot; tag an admission that it recognizes the illusion?&lt;&#x2F;p&gt;
&lt;p&gt;Perhaps the most important takeaway is the value of using novel stimuli. The real test for these models isn&#x27;t showing them the famous &quot;Church Face&quot; again, but presenting them with the new, illusory faces we discover in our daily lives—a pattern in a coffee stain, the front of a new car, or a strangely-shaped vegetable. These &quot;wild&quot; pareidolia images, which the AI could not have been trained on, are the truest test of whether they are learning to &lt;em&gt;see&lt;&#x2F;em&gt; or just to &lt;em&gt;recognize&lt;&#x2F;em&gt;. And for me, that&#x27;s an experiment that never gets old.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Bundling &amp; Notarization GStreamer with Tauri Apps on macOS: A Developer&#x27;s Guide</title>
        <published>2025-04-09T00:00:00+00:00</published>
        <updated>2025-04-09T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/taurigst/"/>
        <id>https://engineering.videocall.rs/posts/taurigst/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/taurigst/">&lt;p&gt;Working with multimedia in desktop applications often requires using GStreamer, a powerful multimedia framework. However, when building a macOS app with Tauri that uses GStreamer, developers face numerous challenges in bundling, signing, and notarizing the application correctly.&lt;&#x2F;p&gt;
&lt;p&gt;After some troubleshooting and experimentation, I&#x27;ve successfully overcome these challenges. This guide shares key insights to help other developers avoid similar headaches.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-challenge&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;The Challenge&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Bundling GStreamer with a Tauri app on macOS involves several complex issues:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;GStreamer&#x27;s architecture&lt;&#x2F;strong&gt; consists of numerous interdependent dynamic libraries that must be correctly bundled and linked&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Apple&#x27;s notarization requirements&lt;&#x2F;strong&gt; conflict with GStreamer&#x27;s recommended configurations&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Path references&lt;&#x2F;strong&gt; in dynamic libraries must be properly relocated&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Code signing&lt;&#x2F;strong&gt; must be applied correctly to each individual binary&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Tauri&#x27;s bundling system&lt;&#x2F;strong&gt; must be properly configured to include GStreamer&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;1-bundling-challenges&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; 1. Bundling Challenges&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;GStreamer is complex because:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;It contains dozens of &lt;code&gt;.dylib&lt;&#x2F;code&gt; files that must be included in your app bundle&lt;&#x2F;li&gt;
&lt;li&gt;These libraries reference each other with absolute paths&lt;&#x2F;li&gt;
&lt;li&gt;They must be bundled for distribution to users who don&#x27;t have GStreamer installed&lt;&#x2F;li&gt;
&lt;li&gt;Missing even one dependency can cause cryptic runtime errors&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;1-1-apple-s-signing-notarization-requirements&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; 1.1 Apple&#x27;s Signing &amp;amp; Notarization Requirements&lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Apple&#x27;s requirements directly conflict with GStreamer&#x27;s documentation:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hardened Runtime&lt;&#x2F;strong&gt;: Apple requires enabling the hardened runtime for notarization, while GStreamer documentation suggests disabling it&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Individual Signing&lt;&#x2F;strong&gt;: Each &lt;code&gt;.dylib&lt;&#x2F;code&gt; must be signed separately with a valid Developer ID&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Secure Timestamps&lt;&#x2F;strong&gt;: All signatures must include a secure timestamp&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Special Entitlements&lt;&#x2F;strong&gt;: GStreamer requires specific entitlements to function with hardened runtime enabled:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;com.apple.security.cs.allow-unsigned-executable-memory&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;com.apple.security.cs.disable-library-validation&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;com.apple.security.cs.allow-dyld-environment-variables&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;1-2-path-handling-solutions&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; 1.2 Path Handling Solutions&lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Getting the library paths right is critical:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Use &lt;code&gt;install_name_tool&lt;&#x2F;code&gt; to modify library references to use &lt;code&gt;@executable_path&lt;&#x2F;code&gt; instead of absolute paths&lt;&#x2F;li&gt;
&lt;li&gt;Add &lt;code&gt;@rpath&lt;&#x2F;code&gt; references to the executable&lt;&#x2F;li&gt;
&lt;li&gt;Set environment variables in a wrapper script and &lt;code&gt;Info.plist&lt;&#x2F;code&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GST_PLUGIN_SYSTEM_PATH&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;GST_PLUGIN_PATH&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;DYLD_LIBRARY_PATH&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;1-3-tauri-integration&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;1.3 Tauri Integration&lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Integrating with Tauri requires special attention:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Configure Tauri&#x27;s resources system to include GStreamer libraries&lt;&#x2F;li&gt;
&lt;li&gt;Modify &lt;code&gt;build.rs&lt;&#x2F;code&gt; to add the correct rpath&lt;&#x2F;li&gt;
&lt;li&gt;Avoid interfering with Tauri&#x27;s DMG creation process&lt;&#x2F;li&gt;
&lt;li&gt;Use a wrapper script for your main executable to set environment variables&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Conclusion&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Successfully bundling GStreamer with a Tauri app on macOS requires navigating the complex interplay between GStreamer&#x27;s architecture, Apple&#x27;s notarization requirements, and Tauri&#x27;s bundling system.&lt;&#x2F;p&gt;
&lt;p&gt;The key is to:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;ALWAYS use custom build scripts to handle library paths, do not rely tauri.conf file manually but edit the tauri file WITH your build script.&lt;&#x2F;li&gt;
&lt;li&gt;Sign each library individually&lt;&#x2F;li&gt;
&lt;li&gt;Use appropriate entitlements&lt;&#x2F;li&gt;
&lt;li&gt;Fix all library paths using &lt;code&gt;install_name_tool&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Ensure required environment variables are set&lt;&#x2F;li&gt;
&lt;li&gt;Verify all required libraries are included&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;With this approach, you can create properly signed, notarized macOS apps that include GStreamer libraries and will work perfectly on customer systems without requiring a separate GStreamer installation.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>You&#x27;ll Finally Understand Lifetimes in Rust After Read This</title>
        <published>2025-03-29T00:00:00+00:00</published>
        <updated>2025-03-29T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/lifetimes/"/>
        <id>https://engineering.videocall.rs/posts/lifetimes/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/lifetimes/">&lt;p&gt;Lifetimes in Rust are often one of the most confusing topics for beginners. In fact, learning about lifetimes is actually same as learning why Rust is forcing you to write them. I&#x27;ll try to keep this post as very simple and short, so if you are already familiar with Rust and lifetimes, this post definitely not for you.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-are-lifetimes-and-why-do-we-need-them&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; What Are Lifetimes and Why Do We Need Them? &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Lifetimes help the Rust compiler understand how long references (borrowed data) are valid. Imagine you have two pieces of paper borrowed from friends. You need to know how long you can safely use each piece before one of your friends asks for it back. Without knowing this, you might accidentally rely on a note that’s no longer available. That’s what lifetimes prevent in your code—they ensure references never outlive the data they point to.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;a-broken-example-when-lifetimes-are-missing&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; A Broken Example: When Lifetimes Are Missing &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Consider this function:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#0f1419;color:#bfbab0;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;&#x2F;&#x2F; This function doesn&amp;#39;t compile because Rust doesn&amp;#39;t know how long the returned reference should be valid.
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;longest&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;x&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;str&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;str&lt;&#x2F;span&gt;&lt;span&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;-&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;str &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;if&lt;&#x2F;span&gt;&lt;span&gt; x&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;gt;&lt;&#x2F;span&gt;&lt;span&gt; y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span&gt;() {
&lt;&#x2F;span&gt;&lt;span&gt;        x
&lt;&#x2F;span&gt;&lt;span&gt;    } &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;else &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;        y
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;main&lt;&#x2F;span&gt;&lt;span&gt;() {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; string1 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#39bae6;&quot;&gt;String&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;::&lt;&#x2F;span&gt;&lt;span&gt;from(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;Hello&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; string2 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#39bae6;&quot;&gt;String&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;::&lt;&#x2F;span&gt;&lt;span&gt;from(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;World&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; result &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;longest&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span&gt;string1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span&gt;string2)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;; &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;&#x2F;&#x2F; Compiler error: missing lifetime annotations
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;println!&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;The longest string is &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;,&lt;&#x2F;span&gt;&lt;span&gt; result)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Output:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#0f1419;color:#bfbab0;&quot;&gt;&lt;code&gt;&lt;span&gt;error[E0106]: missing lifetime specifier
&lt;&#x2F;span&gt;&lt;span&gt; --&amp;gt; src&#x2F;main.rs:1:33
&lt;&#x2F;span&gt;&lt;span&gt;  |
&lt;&#x2F;span&gt;&lt;span&gt;1 | fn longest(x: &amp;amp;str, y: &amp;amp;str) -&amp;gt; &amp;amp;str {
&lt;&#x2F;span&gt;&lt;span&gt;  |               ----     ----     ^ expected named lifetime parameter
&lt;&#x2F;span&gt;&lt;span&gt;  |
&lt;&#x2F;span&gt;&lt;span&gt;  = help: this function&amp;#39;s return type contains a borrowed value, but the signature does not say whether it is borrowed from `x` or `y`
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now, What’s the problem? The compiler is confused. It doesn’t know whether the returned reference is tied to &lt;code&gt;string1&lt;&#x2F;code&gt; or &lt;code&gt;string2&lt;&#x2F;code&gt;, or how long that &lt;span style=&quot;color:orange;&quot;&gt;reference&lt;&#x2F;span&gt;  should remain valid. Without this information, Rust can’t guarantee that the &lt;span style=&quot;color:orange;&quot;&gt;reference&lt;&#x2F;span&gt; won’t point to data that no longer exists.&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#0f1419;color:#bfbab0;&quot;&gt;&lt;code&gt;&lt;span&gt;&#x2F;&#x2F; The Compiler&amp;#39;s View
&lt;&#x2F;span&gt;&lt;span&gt;                                                  
&lt;&#x2F;span&gt;&lt;span&gt;&amp;amp;string1 ──┐                 
&lt;&#x2F;span&gt;&lt;span&gt;           ├─▶ longest() ──▶ returns &amp;amp;str from... where?
&lt;&#x2F;span&gt;&lt;span&gt;&amp;amp;string2 ──┘                 
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;&#x2F;&#x2F; Rust can&amp;#39;t tell if the returned reference will outlive its source!
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now let&#x27;s simply fix this by adding lifetimes:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#0f1419;color:#bfbab0;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;longest&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;x&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;&amp;#39;a str&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;&amp;#39;a str&lt;&#x2F;span&gt;&lt;span&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;-&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;&amp;#39;a str &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;if&lt;&#x2F;span&gt;&lt;span&gt; x&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;gt;&lt;&#x2F;span&gt;&lt;span&gt; y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span&gt;() {
&lt;&#x2F;span&gt;&lt;span&gt;        x
&lt;&#x2F;span&gt;&lt;span&gt;    } &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;else &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;        y
&lt;&#x2F;span&gt;&lt;span&gt;    }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;main&lt;&#x2F;span&gt;&lt;span&gt;() {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; string1 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#39bae6;&quot;&gt;String&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;::&lt;&#x2F;span&gt;&lt;span&gt;from(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;Hello&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; string2 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#39bae6;&quot;&gt;String&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;::&lt;&#x2F;span&gt;&lt;span&gt;from(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;World&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; result &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;longest&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span&gt;string1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span&gt;string2)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;println!&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;The longest string is &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;,&lt;&#x2F;span&gt;&lt;span&gt; result)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;println!&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;We can still use string1: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;,&lt;&#x2F;span&gt;&lt;span&gt; string1)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;; &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;&#x2F;&#x2F; Still valid! thanks to lifetimes!!
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre style=&quot;background-color:#0f1419;color:#bfbab0;&quot;&gt;&lt;code&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;&amp;amp;string1 (&amp;#39;a) ──┐                 
&lt;&#x2F;span&gt;&lt;span&gt;                ├─▶ longest&amp;lt;&amp;#39;a&amp;gt;() ──▶ returns &amp;amp;&amp;#39;a str 
&lt;&#x2F;span&gt;&lt;span&gt;&amp;amp;string2 (&amp;#39;a) ──┘                 
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;&#x2F;&#x2F; Now Rust knows the returned reference lives as long as both inputs!
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ul&gt;
&lt;li&gt;We added a lifetime parameter &lt;code&gt;&#x27;a&lt;&#x2F;code&gt; to the function signature.&lt;&#x2F;li&gt;
&lt;li&gt;We used this lifetime parameter to specify that the returned reference will live as long as the references passed in as arguments.&lt;&#x2F;li&gt;
&lt;li&gt;Now, the compiler knows how long the returned reference should be valid, and the code compiles successfully.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;when-do-you-need-lifetimes&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; When Do You Need Lifetimes? &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;I think this is the most important question to understand lifetimes. Generally, lifetimes are only necessary when you work with references. If you write a function that takes ownership of values, like a simple subtraction or sum, lifetimes aren’t needed. For example:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#0f1419;color:#bfbab0;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;sum&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;x&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;i32&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;i32&lt;&#x2F;span&gt;&lt;span&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;-&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;i32 &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;    x &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span&gt; y
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;main &lt;&#x2F;span&gt;&lt;span&gt;() {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; x &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; y &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;10&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span&gt; result &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;sum&lt;&#x2F;span&gt;&lt;span&gt;(x&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;,&lt;&#x2F;span&gt;&lt;span&gt; y)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;println!&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;The sum is &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;,&lt;&#x2F;span&gt;&lt;span&gt; result)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;println!&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;We can still use x: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;,&lt;&#x2F;span&gt;&lt;span&gt; x)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;; 
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Here, no lifetimes are required because:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;No references (&lt;code&gt;&amp;amp;x&lt;&#x2F;code&gt; or &lt;code&gt;&amp;amp;y&lt;&#x2F;code&gt;): The function simple takes ownership of &lt;code&gt;x&lt;&#x2F;code&gt; and &lt;code&gt;y&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;Returns a new value: The result &lt;code&gt;x + y&lt;&#x2F;code&gt; is a brand-new &lt;code&gt;i32&lt;&#x2F;code&gt;, not a &lt;span style=&quot;color:orange;&quot;&gt;reference&lt;&#x2F;span&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;Nothing borrowed: Rust doesn&#x27;t need to track how long &lt;code&gt;x&lt;&#x2F;code&gt; or &lt;code&gt;y&lt;&#x2F;code&gt; live because the values are already copied.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;span style=&quot;color:orange;&quot;&gt;  Extra note: &lt;&#x2F;span&gt;
You might ask why this still works:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#0f1419;color:#bfbab0;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;println!&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;We can still use x: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;,&lt;&#x2F;span&gt;&lt;span&gt; x)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;; 
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The reason &lt;code&gt;x&lt;&#x2F;code&gt; is still usable after calling &lt;code&gt;sum&lt;&#x2F;code&gt; is that integers are &quot;&lt;code&gt;Copy&lt;&#x2F;code&gt;&quot; types in Rust. When you pass them to a function, they get copied, not moved. This is because integers are small, simple values that are cheap to duplicate.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;the-lifetime-elision-rules&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; The Lifetime Elision Rules &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Good thing is when you program in Rust in real-world projects, you don&#x27;t need to write lifetimes all the time. Rust has smart defaults that often let you skip writing lifetimes explicitly. These are called &quot;lifetime elision rules.&quot; You can read more about them in the &lt;a href=&quot;https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;nomicon&#x2F;lifetime-elision.html&quot;&gt;official Rust book&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;But still, since I think that understanding the concept of lifetimes is an important cornerstone in understanding the overall paradigm of the Rust programming language, if I were to start Rust again, I would refer to and experiment with lifetimes even where I don&#x27;t need to.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>How I Implemented Hot Reloading for WGSL Shaders in Rust</title>
        <published>2025-03-15T00:00:00+00:00</published>
        <updated>2025-03-15T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/hotreload/"/>
        <id>https://engineering.videocall.rs/posts/hotreload/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/hotreload/">&lt;p&gt;&lt;span style=&quot;color:orange;&quot;&gt;My solution&lt;&#x2F;span&gt;&lt;&#x2F;p&gt;
&lt;p&gt;When developing WGSL shaders for my Rust-based graphics engine, I needed a solution to avoid constantly restarting the application to see changes. I built a hot reload system that watches shader files and automatically recompiles them when modifications are detected. The core of this approach uses Rust&#x27;s notify crate to monitor file system events, combined with a debouncing mechanism to prevent multiple reloads during rapid file saves. When a change is detected, the engine creates new shader modules with &lt;code&gt;core.device.create_shader_module()&lt;&#x2F;code&gt; and carefully rebuilds the render pipeline while maintaining the original bind group layouts.&lt;&#x2F;p&gt;
&lt;p&gt;Important struct:
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;cuneus&#x2F;blob&#x2F;b068041c7902df29d33c3100ea4b74a1a38164ff&#x2F;src&#x2F;hot.rs#L9-L231&quot;&gt;Source Code&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;rust&quot; style=&quot;background-color:#0f1419;color:#bfbab0;&quot; class=&quot;language-rust &quot;&gt;&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;pub struct &lt;&#x2F;span&gt;&lt;span style=&quot;color:#59c2ff;&quot;&gt;ShaderHotReload &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;pub &lt;&#x2F;span&gt;&lt;span&gt;vs_module&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span&gt;wgpu&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;::&lt;&#x2F;span&gt;&lt;span&gt;ShaderModule,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;pub &lt;&#x2F;span&gt;&lt;span&gt;fs_module&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span&gt;wgpu&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;::&lt;&#x2F;span&gt;&lt;span&gt;ShaderModule,
&lt;&#x2F;span&gt;&lt;span&gt;    device&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span&gt;Arc&amp;lt;wgpu&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;::&lt;&#x2F;span&gt;&lt;span&gt;Device&amp;gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    shader_paths&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#39bae6;&quot;&gt;Vec&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;PathBuf&amp;gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    last_vs_content&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;:&lt;&#x2F;span&gt;&lt;span&gt; String,
&lt;&#x2F;span&gt;&lt;span&gt;    last_fs_content&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;:&lt;&#x2F;span&gt;&lt;span&gt; String,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;#&lt;&#x2F;span&gt;&lt;span&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;allow&lt;&#x2F;span&gt;&lt;span&gt;(dead_code)]
&lt;&#x2F;span&gt;&lt;span&gt;    watcher&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span&gt;notify&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;::&lt;&#x2F;span&gt;&lt;span&gt;RecommendedWatcher,
&lt;&#x2F;span&gt;&lt;span&gt;    rx&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span&gt;Receiver&amp;lt;notify&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;::&lt;&#x2F;span&gt;&lt;span&gt;Event&amp;gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    _watcher_tx&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span&gt;std&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;::&lt;&#x2F;span&gt;&lt;span&gt;sync&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;::&lt;&#x2F;span&gt;&lt;span&gt;mpsc&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;::&lt;&#x2F;span&gt;&lt;span&gt;Sender&amp;lt;notify&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;::&lt;&#x2F;span&gt;&lt;span&gt;Event&amp;gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    last_update_times&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span&gt;HashMap&amp;lt;PathBuf, Instant&amp;gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;&#x2F;&#x2F;Keeps track of when each shader file was last updated.
&lt;&#x2F;span&gt;&lt;span&gt;    debounce_duration&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;:&lt;&#x2F;span&gt;&lt;span&gt; Duration, &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;&#x2F;&#x2F;Defines how long to wait before allowing another reload of the same file. The default is 100ms.
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;My ShaderHotReload struct stores references to shader files, tracks the last update times for debouncing, and maintains the original shader content for comparison. When a file change is detected, it reads the new shader content, compares it to the previous version, and only triggers a reload if there&#x27;s an actual change.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Why do &#x27;we&#x27; create art?</title>
        <published>2025-03-03T00:00:00+00:00</published>
        <updated>2025-03-03T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/whyart/"/>
        <id>https://engineering.videocall.rs/posts/whyart/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/whyart/">&lt;p&gt;From my inner perspective, art is a way to satisfy the inherent urge to experiment. I eventually discovered that engaging in art means simply “experimenting”—testing a hypothesis: &quot;Will this color evoke satisfaction?&quot;, &quot;Can this formula generate a more intriguing pattern?&quot;, &quot;Might this technique better express the idea that enlivens my mind?&quot;&lt;&#x2F;p&gt;
&lt;p&gt;As a shader programmer, I work within a fully experimental process. Every line of code is an experiment. If you enjoy the thrill of experimentation, you likely share a passion for the intersection of art and math.&lt;&#x2F;p&gt;
&lt;p&gt;Ultimately, each of these questions represents an experiment with no room for deceit, as you are the only impartial judge. In this way, the reliability of your creative process can even surpass conventional scientific knowledge.&lt;&#x2F;p&gt;
&lt;p&gt;At a deeper level, what we often call &quot;aesthetic&quot; in art—seemingly subjective and random—reveals an essence that closely mirrors scientific inquiry. Whether it’s the dawn of art or the invention of the wheel, the underlying purpose is the same: to experience that indescribable satisfaction of making an experiment.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>the difference between reality and dream</title>
        <published>2025-02-03T00:00:00+00:00</published>
        <updated>2025-02-03T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/dream/"/>
        <id>https://engineering.videocall.rs/posts/dream/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/dream/">&lt;h2 id=&quot;what-s-real&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;What&#x27;s real?&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Reality&lt;&#x2F;strong&gt; is experienced in real-time—imagine living at &quot;60 fps&quot;. Every movement and thought flows continuously, with our consciousness reacting instantly to the world around us while drawing on past experiences to shape future plans. This process even tackles complex challenges like &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Feature_integration_theory&quot;&gt;feature binding&lt;&#x2F;a&gt; &lt;em&gt;(in our context, it&#x27;s the brain’s method for uniting various sensory inputs in real-time)&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Dreams&lt;&#x2F;strong&gt;, however, are not bound by real-time. Instead, they are constructed within a context, simulating scenarios (like escaping from a bear) that don’t occur in the moment. During sleep, the &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Cholinergic_system&quot;&gt;cholinergic system&lt;&#x2F;a&gt; &lt;em&gt;(here, I refer to the neural activations that spark random activity, which the brain later organizes into coherent dream narratives)&lt;&#x2F;em&gt; becomes active. Amidst this neural chaos, the brain pieces together a handful of seemingly random &quot;scripts&quot; to create the dream experience.&lt;&#x2F;p&gt;
&lt;p&gt;This contrast highlights the distinct nature of our waking reality and the imaginative, context-driven world of dreams.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Why Rust&#x27;s Structure Resonates with the ADHD Brain</title>
        <published>2025-01-16T00:00:00+00:00</published>
        <updated>2025-01-16T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/rust/"/>
        <id>https://engineering.videocall.rs/posts/rust/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/rust/">&lt;h2 id=&quot;programming-through-the-lens-of-cognitive-function&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Programming Through the Lens of Cognitive Function&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Software development places significant demands on several key cognitive functions. Abilities like &lt;strong&gt;working memory&lt;&#x2F;strong&gt; (holding and manipulating information mentally), &lt;strong&gt;executive functions&lt;&#x2F;strong&gt; (planning, organizing, sequencing tasks), sustained &lt;strong&gt;attention&lt;&#x2F;strong&gt;, and the regulation of &lt;strong&gt;impulse&lt;&#x2F;strong&gt; or the drive for immediate outcomes are constantly engaged.&lt;&#x2F;p&gt;
&lt;p&gt;Individuals with an ADHD cognitive profile often exhibit a distinct pattern in these areas. While challenges in sustaining focus on non-preferred tasks or managing working memory load are common, ADHD is also frequently associated with strengths like high &lt;strong&gt;creativity&lt;&#x2F;strong&gt;, intense &lt;strong&gt;energy&lt;&#x2F;strong&gt; for novel problems, and the ability to make unique &lt;strong&gt;connections&lt;&#x2F;strong&gt;. The drive for &lt;strong&gt;immediacy&lt;&#x2F;strong&gt; – wanting to see results quickly, as often experienced in languages like Python – is also a relevant factor in the programming context. How these cognitive patterns interact with the specific structures and feedback mechanisms of a programming language can significantly influence a developer&#x27;s experience and productivity.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;rust-s-design-potential-cognitive-interactions&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Rust&#x27;s Design: Potential Cognitive Interactions&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;From a psychological perspective, the design of the Rust programming language presents an interesting case study for interaction with ADHD cognitive patterns. Rust is known for its strict compile-time checks, particularly its &lt;strong&gt;ownership and borrowing&lt;&#x2F;strong&gt; system. This upfront rigor contrasts sharply with the immediate feedback loops many individuals with ADHD thrive on, or the rapid iteration often possible in less strict languages. However, this very strictness may offer potential cognitive support.&lt;&#x2F;p&gt;
&lt;p&gt;Consider &lt;strong&gt;working memory&lt;&#x2F;strong&gt;. Manually tracking memory safety and data lifetimes in other languages requires significant ongoing mental effort. Rust&#x27;s ownership rules define clear responsibility for data, and the compiler enforces these rules rigorously. This system potentially reduces the active mental load required to maintain data validity, lessening the strain on working memory resources, which can be a specific challenge area in ADHD profiles. The compiler performs much of the tracking, alleviating the need for constant internal monitoring.&lt;&#x2F;p&gt;
&lt;p&gt;Regarding &lt;strong&gt;executive functions&lt;&#x2F;strong&gt;, particularly planning and organization, Rust&#x27;s borrow checker necessitates careful consideration of data flow and mutability &lt;em&gt;before&lt;&#x2F;em&gt; code compiles successfully. This requirement for upfront structural thinking can act as an external framework, potentially supporting the organizational aspects of coding that might otherwise be challenging. It encourages a methodical approach to data interaction.&lt;&#x2F;p&gt;
&lt;p&gt;The nature of &lt;strong&gt;feedback&lt;&#x2F;strong&gt; is also critical. Delayed runtime errors can be difficult to trace and resolve, especially if attention has shifted. Rust&#x27;s compile-time error reporting provides immediate, specific information about problems, often directing the developer to the exact location and nature of the issue. This type of prompt, concrete feedback aligns well with learning patterns often observed in ADHD, helping to close the loop between action and consequence quickly and reducing the cognitive burden of debugging ambiguous, delayed issues.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;acknowledging-the-cognitive-friction&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Acknowledging the Cognitive Friction&lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;This potential alignment doesn&#x27;t negate the real cognitive friction Rust can introduce, especially initially. The demand for adherence to strict rules before code compiles can directly conflict with the desire for rapid results and experimentation often seen in ADHD. Waiting for compilation can interrupt flow and reduce the immediate reinforcement that helps maintain engagement.&lt;&#x2F;p&gt;
&lt;p&gt;Furthermore, the complexity of mastering concepts like ownership and lifetimes requires sustained focus and deliberate cognitive effort – resources which might be taxed or allocated differently in an ADHD profile. The compiler&#x27;s strictness, while potentially beneficial long-term, can certainly generate frustration during the learning process or when rapid exploration is desired.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;potential-long-term-cognitive-benefits&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Potential Long-Term Cognitive Benefits&lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The key consideration is the potential trade-off: increased upfront cognitive effort for potentially reduced long-term cognitive strain. By catching a wide range of errors (especially memory safety and data races) at compile time, Rust aims to prevent complex, difficult-to-diagnose runtime issues. Debugging these subtle, delayed errors often requires significant sustained attention and complex problem-solving, which can be particularly taxing for individuals managing ADHD symptoms.&lt;&#x2F;p&gt;
&lt;p&gt;While the ADHD cognitive style might often gravitate towards tools offering rapid initial progress and immediate results, the reality of large-scale software development presents a different challenge. As complexity grows in massive projects, the very structure enforced by the Rust compiler can become a significant asset. It helps manage the intricate dependencies and state interactions that can otherwise easily overwhelm cognitive resources, especially working memory and organizational functions. In these demanding, real-world contexts, the compiler&#x27;s tireless vigilance acts as a stabilizing force, potentially preventing the kind of accumulating complexity that leads to burnout or project abandonment.&lt;&#x2F;p&gt;
&lt;p&gt;If Rust&#x27;s rigorous checks successfully reduce the frequency and complexity of these later-stage debugging efforts, it creates a more predictable development environment. This stability might, in turn, free up cognitive resources. Instead of being consumed by low-level error hunting, mental energy could potentially be redirected towards higher-level design, creative problem-solving, and leveraging the divergent thinking strengths often associated with ADHD. The structure imposed by Rust, while initially demanding, could ultimately provide a foundation that supports sustained productivity and reduces certain types of cognitive overload common in complex software development.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Conclusion&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Ultimately, there is no single &quot;best&quot; programming language for any cognitive profile. However, understanding the specific ways a language&#x27;s design interacts with cognitive functions like working memory, executive control, and attention regulation is crucial. Rust&#x27;s emphasis on compile-time safety and explicit structure, while presenting initial hurdles, offers a compelling example of how language design choices might inadvertently provide valuable support for managing some of the cognitive challenges associated with ADHD, particularly in the context of complex, long-term software projects. Recognizing these potential alignments allows for more informed choices about the tools that best enable diverse minds to thrive in the demanding field of software engineering.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Illusory Movement of Dotted Lines but Gabor Version</title>
        <published>2024-12-05T00:00:00+00:00</published>
        <updated>2024-12-05T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/gabordots/"/>
        <id>https://engineering.videocall.rs/posts/gabordots/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/gabordots/">&lt;h2 id=&quot;introduction&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Introduction&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Today, I was exploring &lt;a href=&quot;https:&#x2F;&#x2F;michaelbach.de&#x2F;ot&#x2F;mot-dottedLines&#x2F;index.html&quot;&gt;Michael Bach&#x27;s&lt;&#x2F;a&gt; comments about some illusions and I stumbled upon an illusion called &quot;Dotted Line Motion Illusion&quot; that I hadn&#x27;t known before. I wanted to read more about it because the effect didn&#x27;t seem to work well for me. Both Bach and the original article used a rectangular checkerboard design for the illusion. I tried to reproduce the code in ShaderToy and noticed that the scale of the rectangles and the background color significantly affect the strength of the illusion, at least from my perception.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;plain-version&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Plain Version&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;First, investigate the illusion below. Click the play button, track the red disc as it moves, and notice how the checkered lines seem to shift. It works best on a big screen.&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;450&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;XcKXRV?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;After finishing the coding, I pondered whether &quot;The reason is that the black&#x2F;white contrast signals between adjacent dots along the length of the line are stronger than black&#x2F;grey or white&#x2F;grey contrast signals across the line, and the motion is computed as a vector sum of local contrast-weighted motion signals.&quot; could be an explanation, and then, could Gabor patches be more effective here?&lt;&#x2F;p&gt;
&lt;h2 id=&quot;gabor-version&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Gabor Version&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Here&#x27;s what I did. Now, try this and see which one appears stronger. Interestingly, even on a smaller screen, this version works much better for me and it&#x27;s really functioning very nicely.&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;450&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;McKSRK?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;h2 id=&quot;exploring-reverse-effects&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Exploring Reverse Effects&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Even more interestingly, I discovered a reverse effect when I animated the phase offset. Follow the red dot again, and you&#x27;ll notice that the phase movement stops at some point.&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;450&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;4fyXz3?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;h2 id=&quot;reference-paper&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Reference Paper&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;For more detailed information on the scientific background of these visual phenomena, refer to the following paper:&lt;&#x2F;p&gt;
&lt;p&gt;Ito, H., Anstis, S., &amp;amp; Cavanagh, P. (2009). Illusory Movement of Dotted Lines. Perception, 38(9), 1405-1409. &lt;a href=&quot;https:&#x2F;&#x2F;doi.org&#x2F;10.1068&#x2F;p6383&quot;&gt;https:&#x2F;&#x2F;doi.org&#x2F;10.1068&#x2F;p6383&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Gaussian Splats with Simple Linear Iterative Clustering</title>
        <published>2024-04-19T00:00:00+00:00</published>
        <updated>2024-04-19T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/gauss/"/>
        <id>https://engineering.videocall.rs/posts/gauss/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/gauss/">&lt;h4 id=&quot;gaussian-splats-using-superpixels&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Gaussian Splats Using Superpixels&lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Ever since I stumbled upon ShaderToy, I&#x27;ve been captivated by how some creators integrate complex imagery directly into GLSL shaders without using any external images. The concept of painting with algorithms, particularly using gaussian splats to create intricate effects, intrigued me deeply. It was a challenge I couldn&#x27;t resist diving into.&lt;&#x2F;p&gt;
&lt;p&gt;My journey began with a desire to understand how to generate these visual elements from scratch. How could one translate a photograph into a format suitable for procedural rendering in shaders in really easy way (so I dont have to spent too much time on a single image)? The answer lay in a fusion of deep learning and traditional image processing techniques.&lt;&#x2F;p&gt;
&lt;p&gt;From Deep Learning to Superpixels:&lt;&#x2F;p&gt;
&lt;p&gt;Last year, I was reading an article about the new superpixel method called &quot;Simple Linear Iterative Clustering&quot; (SLIC), and the authors claim that the method is not only very accurate but its superfast. So I decided to give it a try &lt;a href=&quot;https:&#x2F;&#x2F;www.epfl.ch&#x2F;labs&#x2F;ivrl&#x2F;research&#x2F;slic-superpixels&#x2F;&quot;&gt;source)&lt;&#x2F;a&gt;. I started by segmenting images using a pre-trained DeepLabV3 model, a state-of-the-art tool in semantic image segmentation. This model identifies and isolates various elements of an image, providing a granular breakdown that serves as our foundation. This is really important step.&lt;&#x2F;p&gt;
&lt;p&gt;To enhance the texture and depth, I incorporated SLIC superpixels. And the good thing is, this method clusters pixels not just based on color but spatial proximity, creating more cohesive and visually appealing segments.&lt;&#x2F;p&gt;
&lt;p&gt;The final touch was alignment. Using PCA (Principal Component Analysis), I thought I could determine the orientation of each segment. By calculating the arctan2 of the principal components, I aligned our gaussian splats precisely, ensuring that each segment not only had the correct position and color but also the correct orientation.&lt;&#x2F;p&gt;
&lt;p&gt;And the all this process took 15 seconds in the Google-Colab, yes without GPU. Here is te output for the 512x512 Lena image:&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;280&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;4cVGWt?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;&lt;em&gt;click to &quot;play&quot; button to see animation&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Note, you can render your own outputs using the my rust backend also, it gives more performance and I included some GUI stuff:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;blob&#x2F;master&#x2F;src&#x2F;gaussiansplat.rs&quot;&gt;source rust code for rendering)&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;here is the python code to rendering your own images:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#0f1419;color:#bfbab0;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;torch
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;torchvision&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;transforms &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span&gt;T
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;torchvision&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;models&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;segmentation &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;deeplabv3_resnet101
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;PIL &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;Image
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;numpy &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span&gt;np
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;skimage&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;segmentation &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;slic
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;skimage &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;img_as_float&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span&gt;img_as_ubyte
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;skimage&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;color &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;rgb2gray&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span&gt;label2rgb
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;skimage&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;filters &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;gaussian&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span&gt;laplace 
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;skimage&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;feature &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;canny
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;sklearn&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;decomposition &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;PCA
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;matplotlib&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;pyplot &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span&gt;plt
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;sklearn&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;cluster &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;KMeans
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;load_model&lt;&#x2F;span&gt;&lt;span&gt;():
&lt;&#x2F;span&gt;&lt;span&gt;    model &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;deeplabv3_resnet101&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;pretrained&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;True&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    model&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;eval&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;model
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;process_image&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;image_path&lt;&#x2F;span&gt;&lt;span&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    input_image &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;Image&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;open&lt;&#x2F;span&gt;&lt;span&gt;(image_path)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;convert&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;#39;RGB&amp;#39;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    preprocess &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;T&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;Compose&lt;&#x2F;span&gt;&lt;span&gt;([
&lt;&#x2F;span&gt;&lt;span&gt;        T&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;Resize&lt;&#x2F;span&gt;&lt;span&gt;((&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;512&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;512&lt;&#x2F;span&gt;&lt;span&gt;))&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;##  you can decrease, but if you increase you have to adjust pack data fn too
&lt;&#x2F;span&gt;&lt;span&gt;        T&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;ToTensor&lt;&#x2F;span&gt;&lt;span&gt;()&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;        T&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;Normalize&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;mean&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;485&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;456&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;406&lt;&#x2F;span&gt;&lt;span&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;std&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;229&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;224&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;225&lt;&#x2F;span&gt;&lt;span&gt;])
&lt;&#x2F;span&gt;&lt;span&gt;    ])
&lt;&#x2F;span&gt;&lt;span&gt;    input_tensor &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;preprocess&lt;&#x2F;span&gt;&lt;span&gt;(input_image)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;unsqueeze&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;input_tensor, input_image
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;segment_image&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;model&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;input_tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;input_image&lt;&#x2F;span&gt;&lt;span&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;with &lt;&#x2F;span&gt;&lt;span&gt;torch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;no_grad&lt;&#x2F;span&gt;&lt;span&gt;():
&lt;&#x2F;span&gt;&lt;span&gt;        output &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;model&lt;&#x2F;span&gt;&lt;span&gt;(input_tensor)[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;#39;out&amp;#39;&lt;&#x2F;span&gt;&lt;span&gt;][&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;    semantic_segmentation &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;output&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;argmax&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;numpy&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;    resized_input_image &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;input_image&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;resize&lt;&#x2F;span&gt;&lt;span&gt;((semantic_segmentation&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;shape[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span&gt;semantic_segmentation&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;shape[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;])&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span&gt;Image&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;LANCZOS)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    gray_image &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;img_as_float&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;rgb2gray&lt;&#x2F;span&gt;&lt;span&gt;(np&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;array&lt;&#x2F;span&gt;&lt;span&gt;(resized_input_image)))
&lt;&#x2F;span&gt;&lt;span&gt;    log_image &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;laplace&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;gaussian&lt;&#x2F;span&gt;&lt;span&gt;(gray_image&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;sigma&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    log_image &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;(log_image &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;- &lt;&#x2F;span&gt;&lt;span&gt;log_image&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;min&lt;&#x2F;span&gt;&lt;span&gt;()) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&#x2F; &lt;&#x2F;span&gt;&lt;span&gt;(log_image&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;max&lt;&#x2F;span&gt;&lt;span&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;- &lt;&#x2F;span&gt;&lt;span&gt;log_image&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;min&lt;&#x2F;span&gt;&lt;span&gt;())
&lt;&#x2F;span&gt;&lt;span&gt;    edge_enhanced_image &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;img_as_ubyte&lt;&#x2F;span&gt;&lt;span&gt;(np&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;clip&lt;&#x2F;span&gt;&lt;span&gt;(gray_image &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span&gt;log_image&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;# Apply SLIC with refined parameters. If you increase compactness, the output will be more &amp;quot;compact&amp;quot; more &amp;quot;circular&amp;quot;, so I suggest decrease it as much as possible for the nice &amp;quot;brush&amp;quot; effect for gaussian splats
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    slic_segments &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;slic&lt;&#x2F;span&gt;&lt;span&gt;(edge_enhanced_image&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;n_segments&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;800&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;compactness&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;30&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;sigma&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;) 
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    image_array &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;array&lt;&#x2F;span&gt;&lt;span&gt;(resized_input_image)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;reshape&lt;&#x2F;span&gt;&lt;span&gt;((&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;    n_clusters &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;min&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;30&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span&gt;(np&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;unique&lt;&#x2F;span&gt;&lt;span&gt;(slic_segments)) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&#x2F;&#x2F; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;10&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    kmeans &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;KMeans&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;n_clusters&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;n_clusters)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;fit&lt;&#x2F;span&gt;&lt;span&gt;(image_array)
&lt;&#x2F;span&gt;&lt;span&gt;    quantized_colors &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;kmeans&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;cluster_centers_[kmeans&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;labels_]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;reshape&lt;&#x2F;span&gt;&lt;span&gt;(resized_input_image&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;size[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;::&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;,&lt;&#x2F;span&gt;&lt;span&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    combined_segments &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;slic&lt;&#x2F;span&gt;&lt;span&gt;(quantized_colors&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;n_segments&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;800&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;compactness&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;30&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;sigma&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;combined_segments, resized_input_image
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;visualize_segmentation&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;segmentation&lt;&#x2F;span&gt;&lt;span&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    plt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;figure&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;figsize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;10&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;    plt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;imshow&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;label2rgb&lt;&#x2F;span&gt;&lt;span&gt;(segmentation&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;bg_label&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;interpolation&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;#39;nearest&amp;#39;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    plt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;colorbar&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;    plt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;title&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;Segmentation Output&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    plt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;show&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;pack_data&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;x&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;w&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;h&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;r&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;g&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;b&lt;&#x2F;span&gt;&lt;span&gt;): &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;##if you change image size (on torchvision pipeline), you need to change this function too
&lt;&#x2F;span&gt;&lt;span&gt;    x &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;clamp&lt;&#x2F;span&gt;&lt;span&gt;(x&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;511&lt;&#x2F;span&gt;&lt;span&gt;)  &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;# 9 bits 
&lt;&#x2F;span&gt;&lt;span&gt;    y &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;clamp&lt;&#x2F;span&gt;&lt;span&gt;(y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;511&lt;&#x2F;span&gt;&lt;span&gt;)  &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;# 9 bits
&lt;&#x2F;span&gt;&lt;span&gt;    w &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;clamp&lt;&#x2F;span&gt;&lt;span&gt;(w&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;255&lt;&#x2F;span&gt;&lt;span&gt;)  &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;# 8 bits
&lt;&#x2F;span&gt;&lt;span&gt;    h &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;clamp&lt;&#x2F;span&gt;&lt;span&gt;(h&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;255&lt;&#x2F;span&gt;&lt;span&gt;)  &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;# 8 bits
&lt;&#x2F;span&gt;&lt;span&gt;    a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;clamp&lt;&#x2F;span&gt;&lt;span&gt;(a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;255&lt;&#x2F;span&gt;&lt;span&gt;)  &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;# 8 bits
&lt;&#x2F;span&gt;&lt;span&gt;    r &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;clamp&lt;&#x2F;span&gt;&lt;span&gt;(r&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;255&lt;&#x2F;span&gt;&lt;span&gt;)  &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;# 8 bits
&lt;&#x2F;span&gt;&lt;span&gt;    g &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;clamp&lt;&#x2F;span&gt;&lt;span&gt;(g&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;255&lt;&#x2F;span&gt;&lt;span&gt;)  &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;# 8 bits
&lt;&#x2F;span&gt;&lt;span&gt;    b &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;clamp&lt;&#x2F;span&gt;&lt;span&gt;(b&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;255&lt;&#x2F;span&gt;&lt;span&gt;)  &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;# 8 bits
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    xy &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;(x &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;lt;&amp;lt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;23&lt;&#x2F;span&gt;&lt;span&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;| &lt;&#x2F;span&gt;&lt;span&gt;(y &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;lt;&amp;lt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;14&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    whag &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;(w &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;lt;&amp;lt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;24&lt;&#x2F;span&gt;&lt;span&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;| &lt;&#x2F;span&gt;&lt;span&gt;(h &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;lt;&amp;lt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;16&lt;&#x2F;span&gt;&lt;span&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;| &lt;&#x2F;span&gt;&lt;span&gt;(a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;lt;&amp;lt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;8&lt;&#x2F;span&gt;&lt;span&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;| &lt;&#x2F;span&gt;&lt;span&gt;g
&lt;&#x2F;span&gt;&lt;span&gt;    rgb &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;(r &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;lt;&amp;lt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;16&lt;&#x2F;span&gt;&lt;span&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;| &lt;&#x2F;span&gt;&lt;span&gt;g &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;| &lt;&#x2F;span&gt;&lt;span&gt;(b &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;lt;&amp;lt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;8&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;(xy&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span&gt;whag&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span&gt;rgb)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;clamp&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;value&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;max_value&lt;&#x2F;span&gt;&lt;span&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#39bae6;&quot;&gt;int&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;max&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;min&lt;&#x2F;span&gt;&lt;span&gt;(value&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span&gt;max_value)))
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;main&lt;&#x2F;span&gt;&lt;span&gt;():
&lt;&#x2F;span&gt;&lt;span&gt;    model &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;load_model&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;    input_tensor, input_image &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;process_image&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;#39;enes.png&amp;#39;&lt;&#x2F;span&gt;&lt;span&gt;)  &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5c6773;&quot;&gt;## of course your image... 
&lt;&#x2F;span&gt;&lt;span&gt;    segmentation, resized_input_image &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;segment_image&lt;&#x2F;span&gt;&lt;span&gt;(model&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span&gt;input_tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span&gt;input_image)
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;visualize_segmentation&lt;&#x2F;span&gt;&lt;span&gt;(segmentation)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    resized_image &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;array&lt;&#x2F;span&gt;&lt;span&gt;(resized_input_image)
&lt;&#x2F;span&gt;&lt;span&gt;    unique_segments &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;unique&lt;&#x2F;span&gt;&lt;span&gt;(segmentation)
&lt;&#x2F;span&gt;&lt;span&gt;    data &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;[]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;seg_id &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span&gt;unique_segments:
&lt;&#x2F;span&gt;&lt;span&gt;        mask &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;segmentation &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;== &lt;&#x2F;span&gt;&lt;span&gt;seg_id
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span&gt;np&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;count_nonzero&lt;&#x2F;span&gt;&lt;span&gt;(mask) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;lt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;continue
&lt;&#x2F;span&gt;&lt;span&gt;        segment_coords &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;argwhere&lt;&#x2F;span&gt;&lt;span&gt;(mask)
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span&gt;segment_coords&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;size &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;== &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;continue
&lt;&#x2F;span&gt;&lt;span&gt;        y, x &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;mean&lt;&#x2F;span&gt;&lt;span&gt;(segment_coords&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;axis&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;        h, w &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;ptp&lt;&#x2F;span&gt;&lt;span&gt;(segment_coords[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;:&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;]), np&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;ptp&lt;&#x2F;span&gt;&lt;span&gt;(segment_coords[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;:&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;])
&lt;&#x2F;span&gt;&lt;span&gt;        color &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;mean&lt;&#x2F;span&gt;&lt;span&gt;(resized_image[mask]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;axis&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span&gt;(segment_coords) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;: 
&lt;&#x2F;span&gt;&lt;span&gt;            pca &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;PCA&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;n_components&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;            pca&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;fit&lt;&#x2F;span&gt;&lt;span&gt;(segment_coords)
&lt;&#x2F;span&gt;&lt;span&gt;            angle &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;arctan2&lt;&#x2F;span&gt;&lt;span&gt;(pca&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;components_[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span&gt;pca&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;components_[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;]) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;* &lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;180 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&#x2F; &lt;&#x2F;span&gt;&lt;span&gt;np&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span&gt;pi) 
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;else&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;            angle &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;4
&lt;&#x2F;span&gt;&lt;span&gt;        packed_data &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;pack_data&lt;&#x2F;span&gt;&lt;span&gt;(x&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span&gt;y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span&gt;w&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span&gt;h&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span&gt;angle&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span&gt;color&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;astype&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#39bae6;&quot;&gt;int&lt;&#x2F;span&gt;&lt;span&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;        data&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;extend&lt;&#x2F;span&gt;&lt;span&gt;(packed_data)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span&gt;data:
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;const uint data[] = uint[](&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;end&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;#39;&amp;#39;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;#39;,&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;join&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;0x&lt;&#x2F;span&gt;&lt;span&gt;{d&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;:08x&lt;&#x2F;span&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;u&amp;quot; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;d &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span&gt;data)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#bfbab0cc;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;end&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;#39;&amp;#39;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;);&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;Total data points: &lt;&#x2F;span&gt;&lt;span&gt;{&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span&gt;(data)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;&#x2F;&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29718;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;) 
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;else&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f07178;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;quot;No data produced; consider adjusting segment size filter or model parameters.&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#ff7733;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span&gt;__name__ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f29668;&quot;&gt;== &lt;&#x2F;span&gt;&lt;span style=&quot;color:#c2d94c;&quot;&gt;&amp;#39;__main__&amp;#39;&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#ffb454;&quot;&gt;main&lt;&#x2F;span&gt;&lt;span&gt;()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Gabor Patches on the Texture!</title>
        <published>2023-10-16T00:00:00+00:00</published>
        <updated>2023-10-16T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/imgabor/"/>
        <id>https://engineering.videocall.rs/posts/imgabor/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/imgabor/">&lt;p&gt;Below is a video featuring the outcome of my experiment. Take a close look and tell me, do you perceive the face in the image more in the horizontal or vertical orientation of the Gabor patches?&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-media-max-width=&quot;560&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;with different orientations of Gabors... Actually interesting, because this reminds me of research underscoring the important role of horizontal info in face perception, revealing its crucial impact on facial encoding&#x2F;processing while vertical info often reduces these effects. 🙂 &lt;a href=&quot;https:&#x2F;&#x2F;t.co&#x2F;mUcCwJ4SBc&quot;&gt;https:&#x2F;&#x2F;t.co&#x2F;mUcCwJ4SBc&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;t.co&#x2F;ghXyMWS7CO&quot;&gt;pic.twitter.com&#x2F;ghXyMWS7CO&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;&amp;mdash; enes altun (@emportent) &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;emportent&#x2F;status&#x2F;1713689728195690576?ref_src=twsrc%5Etfw&quot;&gt;October 15, 2023&lt;&#x2F;a&gt;&lt;&#x2F;blockquote&gt; &lt;script async src=&quot;https:&#x2F;&#x2F;platform.twitter.com&#x2F;widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;&#x2F;script&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;If you&#x27;ve been following my previous posts, you&#x27;re already familiar with the concept of Gabor patches and how they&#x27;re a fantastic tool for understanding various aspects of visual perception. What&#x27;s particularly intriguing is how these patches can alter our perception of orientation.&lt;&#x2F;p&gt;
&lt;p&gt;findings of &lt;a href=&quot;https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;19757911&#x2F;&quot;&gt;Dakin and Watt (2009)&lt;&#x2F;a&gt; and others(there are a lot of papers about that issue), our perception of faces is significantly influenced by horizontal information compared to vertical. Some call this as &lt;a href=&quot;https:&#x2F;&#x2F;royalsocietypublishing.org&#x2F;doi&#x2F;10.1098&#x2F;rspb.2023.1118#:~:text=The%20radial%20bias%20may%20modulate,the%20individual%20differences%20we%20observe.&quot;&gt;radial bias&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Building on this intriguing concept, Dakin and Watt&#x27;s study delves even deeper into our visual system&#x27;s preference for horizontal features in faces. They introduced the idea of facial &#x27;bar codes,&#x27; unique clusters of horizontal lines, akin to commercial bar codes, that our brains use for quick and efficient face recognition. This theory elegantly explains why we&#x27;re so adept at recognizing faces under various conditions and why certain transformations, like inverting a face, make recognition remarkably challenging. It&#x27;s a compelling reminder of how our visual system has fine-tuned itself over millennia, optimizing certain perceptual shortcuts for survival.&lt;&#x2F;p&gt;
&lt;p&gt;In a nutshell, when the Gabor patches are aligned horizontally, we&#x27;re likely to perceive the face more clearly. This phenomenon ties back to how our brains process faces, giving preferential treatment to horizontal features. It&#x27;s all about how the human visual system has evolved to prioritize certain spatial frequencies and orientations, especially when it comes to recognizing faces - one of the most crucial visual tasks we perform.&lt;&#x2F;p&gt;
&lt;p&gt;But hey, don&#x27;t just take my word for it! Dive into the research, play around with the code, and explore the captivating realm of Gabor patches and visual perception. Who knows what other secrets are waiting to be uncovered?&lt;&#x2F;p&gt;
&lt;p&gt;Note: This code is computationally intensive, so only run it if you&#x27;re confident in your GPU&#x27;s capabilities. Also, please be cautious when adjusting the numbers—small changes can have big impacts!&quot;&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;420&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;Dd3fRB?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Reverse Phi Motion Using Gabors</title>
        <published>2023-10-15T00:00:00+00:00</published>
        <updated>2023-10-15T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/gaborillusion/"/>
        <id>https://engineering.videocall.rs/posts/gaborillusion/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/gaborillusion/">&lt;h3 id=&quot;new-gabor-illusion&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; New Gabor Illusion &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;While diving into some hands-on coding with &lt;a href=&quot;https:&#x2F;&#x2F;altunenes.github.io&#x2F;posts&#x2F;2023&#x2F;10&#x2F;gabor&#x2F;&quot;&gt;Gabor patches&lt;&#x2F;a&gt;, a fascinating idea struck me: What if I sculpted shapes like triangles or ellipses, infused them with some sine phases, and voila—could I possibly evoke a simple reverse phi effect? 🤔🔄&lt;&#x2F;p&gt;
&lt;p&gt;To my delight, I believe I succeeded! 🎉 Navigate your cursor and hit the space bar to unveil the effect. 🖱️⌨️ Your insights and thoughts are eagerly awaited! =)&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;dsdfzS?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;or look at that =) not: unlike other &quot;reverse phi motion&quot; effects, this one doesn&#x27;t have &quot;thin edges&quot; with sine phases. Its just Gabors... 😁&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;dscfzs?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Asahi illusion on the Fractal!</title>
        <published>2023-09-30T00:00:00+00:00</published>
        <updated>2023-09-30T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/asahi/"/>
        <id>https://engineering.videocall.rs/posts/asahi/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/asahi/">&lt;h2 id=&quot;background&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Background &lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;While experimenting with shader code, I stumbled upon a fascinating visual phenomenon. When focusing on the center of a particular design, surrounded by colorful petals, the center appears brighter than it actually is.&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;DsfyRX?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;Dr. Bruno Laeng&#x27;s study unveiled that our brain is deceived into triggering a pupillary light reflex, causing our pupils to constrict as if protecting our eyes from intense light, like the sun&#x27;s rays.&lt;&#x2F;p&gt;
&lt;p&gt;The Asahi illusion&#x27;s effect on the pupil isn&#x27;t instantaneous. In humans, there&#x27;s a notable delay between the onset of the illusion and the pupillary response. This delay might be attributed to the time required for the brain&#x27;s processing mechanisms to influence the pupillary light reflex.&lt;&#x2F;p&gt;
&lt;p&gt;Interestingly, the Visual Cortex (V1) seems to play a pivotal role. The V1 response to the Asahi illusion precedes the pupil constriction, suggesting its potential involvement in modulating the Autonomic Nervous System (ANS). However, the exact pathways, be it direct projections or intricate subcortical synapses, remain a topic of ongoing &lt;a href=&quot;https:&#x2F;&#x2F;academic.oup.com&#x2F;cercor&#x2F;article&#x2F;33&#x2F;12&#x2F;7952&#x2F;7084649?login=false&quot;&gt;research&lt;&#x2F;a&gt; .&lt;&#x2F;p&gt;
&lt;p&gt;here is another one I coded after the above one.&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;MX23Wz?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;43SGDh?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;Note, you can download the asahi illusion demos with using interactive GUI I implemented (easy to change the parameters like colors etc) on here: (I compiled them with Rust :-) &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&quot;&gt;Source code:&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;asahi-demo-downloads&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Asahi Demo Downloads &lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Software Version&lt;&#x2F;th&gt;&lt;th&gt;Operating System&lt;&#x2F;th&gt;&lt;th&gt;Download Link&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Asahi&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;macOS&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi-macos-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;Ubuntu&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi-ubuntu-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;Windows&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi-windows-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Asahi2&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;macOS&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi2-macos-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;Ubuntu&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi2-ubuntu-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;Windows&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi2-windows-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>About the Reverse Phi Motion Effect</title>
        <published>2023-09-12T00:00:00+00:00</published>
        <updated>2023-09-12T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/rphi/"/>
        <id>https://engineering.videocall.rs/posts/rphi/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/rphi/">&lt;h3 id=&quot;decoding-the-reverse-phi-illusion&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Decoding the Reverse Phi Illusion &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;After diving into the code for reverse phi motion, I found myself pondering its intricacies for days. While I can articulate the phenomenon in terms of programming or mathematics, the scientific literature seemed to offer limited insights into whether this illusion is purely retinal or a result of complex neural interactions, such as those involving the geniculate nucleus or the motion-sensitive neurons in V1.&lt;&#x2F;p&gt;
&lt;p&gt;an example of Reverse Phi illusion I coded in Rust &lt;a href=&quot;https:&#x2F;&#x2F;journals.sagepub.com&#x2F;doi&#x2F;full&#x2F;10.1177&#x2F;2041669518815708&quot;&gt;Flynn &amp;amp; Shapiro, 2018 &lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;video width=&quot;320&quot; height=&quot;240&quot; controls&gt;
    &lt;source src=&quot;https:&#x2F;&#x2F;user-images.githubusercontent.com&#x2F;54986652&#x2F;248949171-4d361b74-e377-4409-9286-525614ff92bf.mp4&quot; type=&quot;video&#x2F;mp4&quot;&gt;
    Your browser does not support the video tag.
  &lt;&#x2F;video&gt;
&lt;&#x2F;div&gt;
&lt;h4 id=&quot;a-glimpse-into-comparative-vision-science&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; A Glimpse into Comparative Vision Science &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;While exploring this topic, I stumbled upon an intriguing article titled &lt;a href=&quot;https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;nn.4050&quot;&gt;&quot;Common Circuit Design in Fly and Mammalian Motion Vision&quot;&lt;&#x2F;a&gt;. Although the paper primarily focuses on flies and mice, its findings can be extrapolated to human vision.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;on-and-off-pathways-the-universal-language-of-vision&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; ON and OFF Pathways: The Universal Language of Vision &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Both flies and humans possess separate pathways for processing ON and OFF signals generated by photoreceptors that respond to changes in luminance. These pathways originate at different synapses in the visual system and have distinct anatomical and functional properties. According to a &lt;a href=&quot;https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;29261684&quot;&gt;2017 study&lt;&#x2F;a&gt;, these ON and OFF pathways are sensitive to specific interactions, which may also be applicable to human psychophysics.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;demo-example&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Demo Example &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;To better understand ON and OFF pathways, check out this interactive shader: &lt;a href=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;view&#x2F;XtdyWj&quot;&gt;ShaderToy Example&lt;&#x2F;a&gt;. This shader illustrates how ON pathways are more activated by a bright stimulus on a dark background, while OFF pathways are more activated by a dark stimulus on a bright background.&lt;&#x2F;p&gt;
&lt;p&gt;Note: That shader code not belong to me!&lt;&#x2F;p&gt;
&lt;h4 id=&quot;the-role-of-correlation-based-algorithms&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; The Role of Correlation-Based Algorithms &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;The article also discusses how both species utilize correlation-based algorithms for motion detection. These algorithms compare temporal changes in luminance at different spatial locations using a multiplication or correlation process. This is in line with another &lt;a href=&quot;https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;18848956&quot;&gt;study&lt;&#x2F;a&gt; that showed equal efficiency in correlating dots of opposite contrast and of similar contrast in reverse-phi motion stimuli.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;reverse-phi-more-than-just-stimulus-properties&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Reverse Phi: More Than Just Stimulus Properties &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;In essence, reverse phi motion is not merely a result of low-level stimulus properties. It appears to be a complex interplay between ON and OFF signals processed through discrete pathways. These interactions can even reverse the direction of motion when contrast changes accompany the discrete motion, as suggested by &lt;a href=&quot;https:&#x2F;&#x2F;dblp.org&#x2F;rec&#x2F;journals&#x2F;neco&#x2F;MoK03&quot;&gt;biophysical simulations&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;why-parallel-processing&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Why Parallel Processing? &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;You might wonder why our visual system even employs this kind of parallel processing. The authors of the article provide an insightful explanation. They suggest that separating ON and OFF signals simplifies the task for motion-sensitive neurons, making it easier to correlate two positive input signals. This seems to resolve a significant biophysical challenge in implementing correlation mechanisms.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;final-thoughts-the-elegance-of-on-and-off-pathways-in-motion-perception&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Final Thoughts: The Elegance of ON and OFF Pathways in Motion Perception &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;As we draw this exploration to a close, let&#x27;s revisit a profound insight from the paper &quot;Common Circuit Design in Fly and Mammalian Motion Vision&quot;. The paper asks an essential question: What could be the advantage of separating ON and OFF signals in both the mouse and fly visual systems?&lt;&#x2F;p&gt;
&lt;p&gt;The separation of ON and OFF pathways in our visual system serves a practical, computational purpose. Motion perception, at its core, involves the temporal correlation of similar events at distinct spatial locations. When a bright object moves across our field of vision, the neurons responsible for detecting this motion have to process both the increasing and the decreasing luminance as the object passes by.&lt;&#x2F;p&gt;
&lt;p&gt;The challenge lies in the biophysics: there&#x27;s no known mechanism that allows a postsynaptic neuron to get excited both when two inputs increase and when they decrease their membrane potential. This is where the genius of biological design comes into play. By segregating ON and OFF signals, each dealing exclusively with brightness increments or decrements, the visual system effectively simplifies the task for motion-sensitive neurons. Now, they only have to correlate two positive input signals, significantly easing the computational burden.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>5 main root causes of psychological disorders</title>
        <published>2023-09-07T00:00:00+00:00</published>
        <updated>2023-09-07T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/disorders/"/>
        <id>https://engineering.videocall.rs/posts/disorders/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/disorders/">&lt;h2 id=&quot;5-causes-of-psychiatric-illness&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;5 Causes of Psychiatric Illness&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:orange;&quot;&gt;Existence of Society&lt;&#x2F;span&gt;&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
Society provides the framework against which behavior is judged, allowing us to label actions as normal or abnormal. Without a community to compare against, it becomes challenging to define deviations.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:orange;&quot;&gt;Existence of Culture&lt;&#x2F;span&gt;&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
Culture, with its own set of rules and traditions, shapes our understanding of mental health and acceptable behavior. Deviations from these norms can lead to an increase in psychiatric diagnoses.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:orange;&quot;&gt;Mathematical Concept of &quot;Normality&quot;&lt;&#x2F;span&gt;&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
Normality is inherently statistical, requiring a baseline to identify deviations as abnormal. This concept underpins how psychiatric disorders are classified as variations from societal norms.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:orange;&quot;&gt;Existence of an Educational System&lt;&#x2F;span&gt;&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
Structured academic environments often uncover conditions like ADHD through systematic evaluation. In settings without formal testing, such conditions might not be recognized at all.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;color:orange;&quot;&gt;Presence of High-Level Intelligence and Cognition&lt;&#x2F;span&gt;&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
Advanced cognitive abilities can sometimes be accompanied by psychological challenges, with schizophrenia being an extreme example. This may be seen as a genetic side effect of heightened intelligence and creativity.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>My thesis in TLDR</title>
        <published>2023-01-01T00:00:00+00:00</published>
        <updated>2023-01-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/thesis/"/>
        <id>https://engineering.videocall.rs/posts/thesis/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/thesis/">&lt;h4 id=&quot;on-which-stage-we-perceive-differently-when-we-classified-faces&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;On which stage we perceive differently when we classified faces?&lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;When I started reading about face perception, I found many studies. Most of these studies say that people see faces from their own race or species differently than others. But none of these studies answered my main question: At what point do we actually start to see these faces differently?&lt;&#x2F;p&gt;
&lt;p&gt;When does this happen? Right when the light hits our retinas? Or maybe a bit later, when the signals reach the occipital lobe at the back of our brains? Or is it even later, when our brain&#x27;s &quot;face recognition software&quot; kicks in?&lt;&#x2F;p&gt;
&lt;h3 id=&quot;clarifying-the-term-race-in-my-research&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Clarifying the Term &#x27;Race&#x27; in My Research &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;In my thesis, I use the term &quot;Race&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;I want to make it clear that I don&#x27;t see &quot;race&quot; as a biological concept, and I&#x27;m not approaching this from a &quot;racial&quot; perspective. Instead, I borrowed the term from scientific studies that discuss the &quot;other-race effect in face perception.&quot;&lt;&#x2F;p&gt;
&lt;p&gt;In simpler words, when I say &quot;Race&quot; in my research, I&#x27;m talking about faces that you don&#x27;t usually see in your day-to-day life. For example, in my experiments conducted in Antalya, I used &quot;Asian&quot; faces as the &quot;other&quot; category. Why? Because there aren&#x27;t many Asian people in Antalya, so these faces would be less &quot;familiar&quot; to the participants.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;how-i-tried-to-answer-this-question&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; How I tried to answer this question? &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;In the world of vision science, the visual hierarchy theory is a cornerstone. It tells us that there are three main stages when our brain classifies what we see:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Superordinate Level: Is it alive or not?&lt;&#x2F;li&gt;
&lt;li&gt;Basic Level: What kind of thing is it? (e.g., a dog)&lt;&#x2F;li&gt;
&lt;li&gt;Subordinate Level: What specific type is it? (e.g., a pitbull)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Interestingly, &lt;a href=&quot;https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;25208739&#x2F;&quot;&gt;research&lt;&#x2F;a&gt; has shown that our brain can spot an animal in just 120 milliseconds but takes a bit longer to figure out it&#x27;s a dog. This quick initial categorization happens at the Basic Level, which aligns perfectly with the focus of my experiments.&lt;&#x2F;p&gt;
&lt;p&gt;So, if you see a pitbull, your brain first decides it&#x27;s a living thing, then identifies it as a dog, and finally as a pitbull. Given this, I chose to zoom in on the &quot;Basic Level&quot; for my experiments. Participants were asked a simple question: &quot;If you see a face, click on X; if you don&#x27;t see a face, click on Y.&quot;&lt;&#x2F;p&gt;
&lt;p&gt;You might be wondering, &quot;Why did I choose to focus on the &#x27;Basic Level&#x27; and not go deeper into the &#x27;Subordinate Level&#x27;?&quot; The answer lies in my interest in low-level visual properties. By keeping the research at the Basic Level, I could delve into the nitty-gritty of early visual processing without getting entangled in the complexities that come with higher-level categorizations like gender or specific facial features.&lt;&#x2F;p&gt;
&lt;p&gt;Moreover, a glance at the current ERP-face literature reveals that many studies are already exploring questions like &quot;What gender is this face?&quot; or &quot;Is this face familiar?&quot;, even some attentional oddbal tasks... These questions often require processing at the Subordinate Level, which involves more complex and high-level facial features. My aim was to strip away these complexities and get down to the basics—literally. This approach allowed me to isolate and study the fundamental visual cues that our brains use for face perception, making the research as low-level as possible.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;but-thats-not-enough&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; But thats not enough... &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;This could be answer my question partly but I wanted to know more. So I decided to use &quot;Spatial Frequency&quot;, more precisely &quot;Coarse-to-Fine&quot; theory in human vision. According to this theory, when we see an object in the world, we first perceive it&#x27;s coarse features (low spatial frequency) and then
fine features (high spatial frequency) so in the brain a fast-forward mechanism works like this: coarse features -&amp;gt; fine features -&amp;gt; classification.&lt;&#x2F;p&gt;
&lt;p&gt;See bellow image for more details:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;sf.png&quot; alt=&quot;SF&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;How brain process a visual object&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h4 id=&quot;so-what-i-did&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; So what I did? &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Armed with Python code and some nifty image processing techniques like Fourier Transform, RMSE, and luminance normalization, I set out to explore Spatial Frequencies (SFs). I designed my experiment using E-Prime software. A big shoutout to my awesome colleagues Nurullah, Furkan, and Semih for their invaluable help! Together, we conducted the experiment in our lab and managed to gather data from 30 participants using EEG, all within two weeks.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-i-analyzed&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; What I analyzed? &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;I dived deep into the Event-Related Potentials (ERPs) for each participant. Specifically, I looked at:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;P100&lt;&#x2F;strong&gt;: Important for early visual processing&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;N170&lt;&#x2F;strong&gt;: Crucial for face perception&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;N250&lt;&#x2F;strong&gt;: Relevant for recognizing familiar faces&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;the-tools-which-electrodes-did-i-use&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; The Tools: Which Electrodes Did I Use? &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;For the EEG recordings, I used a variety of electrodes but focused my reporting on those placed on the occipital lobe, which is the brain&#x27;s visual processing center. The electrodes I reported are:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;O1 and O2&lt;&#x2F;strong&gt;: Standard occipital electrodes&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;PO3 and PO4&lt;&#x2F;strong&gt;: Parieto-occipital electrodes&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;PO7 and PO8&lt;&#x2F;strong&gt;: Additional parieto-occipital electrodes&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;what-i-found-the-key-takeaways&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; What I Found: The Key Takeaways&lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;I discovered quite a bit through my research, but I&#x27;ve focused on reporting the most crucial findings.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;No Other-Race Effect in Early Stages&lt;&#x2F;strong&gt;: While I did find some significant results in the N250 ERP (especially in the PO electrodes), there were no significant differences in the P100 and N170 ERPs between Asian and Caucasian faces. This suggests that, at least in the early stages of visual processing, we don&#x27;t perceive faces of different races differently.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Species Differences in Higher Frequencies&lt;&#x2F;strong&gt;: Interestingly, faces of other species didn&#x27;t differ from human faces in the low spatial frequency domain. However, we do perceive them differently in the high spatial frequency (HSF) or broadband, especially in the P100 ERP. This difference becomes even more distinct in the N170 and N250 ERPs.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;My thesis is still under review, but I&#x27;ve conducted many detailed analyses and learned a lot. You can find the published version of my thesis &lt;a href=&quot;https:&#x2F;&#x2F;www.tandfonline.com&#x2F;doi&#x2F;full&#x2F;10.1080&#x2F;13506285.2024.2415721#abstract&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;&#x2F;strong&gt;: Due to the length of the paper, I didn&#x27;t include the N250 results in the pre-print version. Rest assured, they are thoroughly reported in my full thesis.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;softwares-and-tools-i-used&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Softwares and Tools I Used&lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;open-source-contributions&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Open-Source Contributions&lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;I love open-source and have contributed some tools that I wrote myself:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;easylab&quot;&gt;EasyLab&lt;&#x2F;a&gt;&lt;&#x2F;strong&gt;: I created a Python GUI as a gift to my lab! It&#x27;s perfect for resizing images, renaming them with a prefix for easy reading in E-Prime 3.0, and even processing Spatial Frequencies using Fourier and Butterworth filters.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;scramblery&quot;&gt;Scramblery&lt;&#x2F;a&gt;&lt;&#x2F;strong&gt;: This tool is for scrambling images. I used scrambled images as a control condition in my research. It&#x27;s open-source, and there&#x27;s even a cool JavaScript version that runs online!&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Try it out here: &lt;a href=&quot;https:&#x2F;&#x2F;altunenes.github.io&#x2F;scramblery&#x2F;scramblerydemo.html&quot;&gt;Demo&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;butter2d&quot;&gt;butter2d&lt;&#x2F;a&gt;&lt;&#x2F;strong&gt;: Rust implementation for Butterworth filter.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;proprietary-software&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Proprietary Software&lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;BrainVision&lt;&#x2F;strong&gt;: I used this for my EEG recordings and initial data filtering. Unfortunately, it&#x27;s not open-source.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;E-Prime 3.0&lt;&#x2F;strong&gt;: I used this for my experiment design. It&#x27;s buggy and makes you want to pull your hair out, but it&#x27;s the best option out there for designing experiments. It&#x27;s also not open-source. Made me think about creating an open-source alternative in the future using Rust while I was using it. Maybe one day!&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;data-analysis&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Data Analysis&lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;R&lt;&#x2F;strong&gt;: After collecting the data, I did all my analyses using R. You can check out my workflow in this &lt;a href=&quot;https:&#x2F;&#x2F;gist.github.com&#x2F;altunenes&#x2F;7081d34140335dd7764a92a7bfd12f1d&quot;&gt;gist file&lt;&#x2F;a&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;the-fuel-coffee-music-and-friends&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; The Fuel: Coffee, Music and Friends&lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Coffee&lt;&#x2F;strong&gt;: Consumed in large quantities. I wasn&#x27;t picky about the brand.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Music&lt;&#x2F;strong&gt;: Candlemass kept me company during those long hours of research. I dig into the doom.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Friends&lt;&#x2F;strong&gt;: I couln&#x27;t have done it without my friends. They were both my emotional and technical support. Thank you Nurullah, Furkan and Semih! I&#x27;m forever grateful for your help. &amp;lt;3
Friendship in the lab is the best thing ever. I&#x27;m so lucky to have you guys!&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;paper-link&quot;&gt;Paper Link&lt;&#x2F;h3&gt;
&lt;p&gt;You can read the more boring parts on &lt;a href=&quot;https:&#x2F;&#x2F;www.tandfonline.com&#x2F;doi&#x2F;full&#x2F;10.1080&#x2F;13506285.2024.2415721#abstract&quot;&gt;here&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Why I love optical illusions</title>
        <published>2022-08-02T00:00:00+00:00</published>
        <updated>2022-08-02T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/illusions/"/>
        <id>https://engineering.videocall.rs/posts/illusions/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/illusions/">&lt;p&gt;Ever stumbled upon something so interesting that you just couldn&#x27;t let it go? That&#x27;s what happened to me with optical illusions. My first exposure to optical illusions was in my undergraduate intro textbooks, and that was the point from which I was captured. So, whenever I see a new illusion, mostly on Twitter, my day is ruined because my brain just won&#x27;t let it go until I try to reproduce it with coding and understand the mechanism at a low level.&lt;&#x2F;p&gt;
&lt;p&gt;Four years (2018-2022) ago my coding skills were. well, let&#x27;s say, they were just born. But the challenge to create these optical illusions was very tempting. I wanted to dig deeper into this, so as to control the illusion. I saw how with just small changes in the code, it became stronger or disappeared.&lt;&#x2F;p&gt;
&lt;p&gt;It had been a very steep learning curve, but every line of code was a step closer to finally unraveling the mysteries of human vision. After so much coding and debugging, I finally reached a repository using which you can display and manipulate almost all famous optical illusions with just a few lines of code &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;sorceress&quot;&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;sorceress&lt;&#x2F;font&gt;&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;. And for those curious minds, I&#x27;ve also documented the science and explanations behind each illusion. Not only illusions explained, but you can also find the API documentation for humans, of course &lt;a href=&quot;https:&#x2F;&#x2F;altunenes.github.io&#x2F;sorceress&#x2F;explanations%20of%20illusions&#x2F;&quot;&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;here&lt;&#x2F;font&gt;&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Sometimes I look at my old code and cringe a little, but then I smile—it shows me only how far I came. From a total beginner to this comprehensive repo on Optical Illusions, the journey has been quite something. I don&#x27;t really contribute to this repository since I mostly write my code in Rust. So if you are in search of more dynamic, solid, and the latest illusions, check my other repository
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&quot;&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;Rust&lt;&#x2F;font&gt;&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;.
.&lt;&#x2F;p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;video width=&quot;320&quot; height=&quot;240&quot; controls&gt;
    &lt;source src=&quot;https:&#x2F;&#x2F;user-images.githubusercontent.com&#x2F;54986652&#x2F;248949171-4d361b74-e377-4409-9286-525614ff92bf.mp4&quot; type=&quot;video&#x2F;mp4&quot;&gt;
    Your browser does not support the video tag.
  &lt;&#x2F;video&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;But why in the world, you may ask, why all this effort? Well, to be totally honest, I really don&#x27;t know.&lt;&#x2F;p&gt;
&lt;p&gt;This is actually a question that I always come across, especially on interviews that I mostly bomb. But the answer probably lies in the complexity of human vision. I believe most of the understanding of vision from us is based on implicit assumptions. Optical illusions challenge these assumptions more like little experiments and with an irresistibly captivating factor about them: seeing how easily our vision can be played with. And let&#x27;s not forget the human curiosity factor. Perhaps one of the questions for research could be the fascination of optical illusions and, in fact, lifelong attempts to understand them. Yes, I do realize that up to now I haven&#x27;t had any concrete answer to the question. I tried to play tricks with your brain through the illusion to let you think I have a concrete answer. I am sorry for that.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Theory of Mind and Other Controversies in Animal Cognition Research</title>
        <published>2021-04-19T00:00:00+00:00</published>
        <updated>2021-04-19T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/animals/"/>
        <id>https://engineering.videocall.rs/posts/animals/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/animals/">&lt;h3 id=&quot;the-story-of-hans&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; The Story of Hans &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The story of Clever Hans is a well-known tale in the field of animal behavior and cognition. It is often cited as an example of the need for careful consideration and methodology in studying animal intelligence.&lt;&#x2F;p&gt;
&lt;p&gt;For those unfamiliar with the story, Hans was a horse trained by Wilhelm Von Osten, a math teacher, to perform basic mathematical tasks. Through extensive training, Hans was able to demonstrate an understanding of addition, subtraction, and other mathematical concepts. When asked a question, such as &quot;What is 2+3?&quot;, Hans would tap his hoof on the ground five times to indicate the correct answer.&lt;&#x2F;p&gt;
&lt;p&gt;This story serves as a reminder of the need for rigorous scientific methods in studying animal cognition. It highlights the potential for unconscious biases and errors in interpretation, and emphasizes the importance of careful experimentation and analysis in understanding the mental abilities of non-human animals&lt;&#x2F;p&gt;
&lt;p&gt;The news of Hans&#x27;s abilities quickly spread throughout Germany, leading to the formation of a commission to examine his skills. The commission was headed by psychologist Carl Stumpf, who believed that Osten was a fraud and that Hans&#x27;s abilities were due to Osten&#x27;s trickery. As a result, Osten was removed from Hans&#x27;s line of sight during testing. However, Hans continued to correctly answer questions. Subsequently, psychologist Oskar Pfungst from the University of Berlin was called in to conduct further analysis.&lt;&#x2F;p&gt;
&lt;p&gt;Pfungst removed the questioner from Hans&#x27; line of sight, which resulted in the horse experiencing difficulty in answering questions. It became clear that Hans was receiving some form of information. Pfungst conducted a thorough experiment to determine the source of this information. The results indicated that Hans was not looking at the blackboard or listening to his owner&#x27;s questions. Instead, he was picking up on unconscious cues from the owner, such as slight movements in their head or eyebrows. If the questioner could not see the question on the blackboard, Hans would continue to search for a cue that never came. While Hans may have appeared to be clever, his abilities were actually derived from his ability to detect and interpret these subtle cues. He had learned to associate these cues with a reward in a complex situation.&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;hans.png&quot; alt=&quot;hans&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Hans, the horse who could do math&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;&#x2F;div&gt;
&lt;h3 id=&quot;superpowers-of-animals&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Superpowers of Animals &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The story of Clever Hans serves as a reminder to always approach claims of extraordinary animal abilities with skepticism. While it is clear that animals possess unique cognitive abilities, it is important to conduct thorough experiments and analysis in order to accurately discern the extent of these abilities. The lack of clear environmental parameters and a defined baseline of normal behavior in many videos depicting &quot;unusual animal behavior&quot; make it difficult to accurately assess the true abilities of the animal in question. It is important for researchers to take these factors into consideration in order to avoid the pitfalls of the Clever Hans phenomenon.&lt;&#x2F;p&gt;
&lt;p&gt;It is well known that laboratory experiments often provide more controlled conditions for examining animal behavior, leading to clearer insights into their cognitive abilities. However, even in these highly controlled settings, it is important to remain cautious about making sweeping conclusions about animal cognition. This is exemplified by the case of Clever Hans, a horse whose apparent ability to perform complex mathematical calculations was later revealed to be a result of his ability to pick up on unconscious cues from his human questioner.&lt;&#x2F;p&gt;
&lt;p&gt;In a similar vein, the work of Inoue and Matsuzawa (2007) on chimpanzees&#x27; working memory capabilities has garnered significant attention. Their experiments seemingly demonstrated that young chimpanzees possess an extraordinary ability to recall numerical information, outperforming human adults. However, it is worth noting that the human participants in the study did not receive the same level of training as the chimpanzees. Despite this, Inoue and Masuzawa concluded that their findings align with our understanding of eidetic imagery in humans. &lt;strong&gt;Our study shows that young chimpanzees have an extraordinary working memory capability for numerical recollection better than that of human adults. The results fit well with what we know about the eidetic imagery in humans.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;It is essential to approach such findings with skepticism and to carefully consider the implications of our conclusions. As always, further research is needed to fully comprehend the cognitive abilities of non-human animals.&lt;&#x2F;p&gt;
&lt;p&gt;After 3 years, another study was conducted by Cook and Wilson (2010). Their results indicate that when human participants are trained to the same extent as Matsuzawa&#x27;s chimpanzees, they outperform chimpanzees in terms of accuracy (with 93% and 97% accuracy). In conclusion, the impressive working memory performances of chimpanzees can be explained by their extensive training. Ayumu (one of the chimpanzees in Matsuzawa&#x27;s study) excelled due to rigorous training sessions.
However, some have argued that Ayumu may have possessed unique abilities. Humphrey (2012) hypothesized that Ayumu outperformed humans because he had synesthesia (a neurological trait that results in the merging of senses that are not normally connected, such as associating numbers and faces with colors). In my opinion, this is a compelling explanation and may be true. However, there is currently no robust evidence to support this hypothesis.&lt;&#x2F;p&gt;
&lt;p&gt;It is crucial to approach findings regarding the cognitive abilities of non-human animals with skepticism and to carefully consider the implications of our conclusions. As always, further research is needed to fully understand the complex nature of animal cognition.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;theory-of-mind&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Theory of Mind?&lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;One example of a controversial topic in the field of animal cognition is the theory of mind, which refers to the ability to understand and predict the goals and intentions of others. The idea that chimpanzees possess a theory of mind has been widely discussed, with some researchers positing that they are able to represent behavior not simply as &quot;he takes the banana&quot; but also as &quot;he intends to eat the banana.&quot; However, the problem with making such claims is that it is nearly impossible to know the mental states of animals, as we cannot ask them directly about their intentions or motivations. As such, our explanations of animal mental states are often based on observed behaviors, which can be subject to interpretation and bias. While it is possible that chimpanzees possess a theory of mind, it is important to recognize that such a conclusion ultimately requires a discussion of consciousness, which is a complex and contentious topic.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;hare2001.png&quot; alt=&quot;hans&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Fig 1: A famous experimental method was conducted by Hare et al., 2001 (image taken from Hare et al. 2001). Hare has used chimps’ strategies for the food competition. In this experiment, the non-dominant chimp and dominant chimp were released into a room that has food, via two doors across from one another. The food was hidden by experimenters however non-dominant chimp was always know the exact place of the food but it was not known by the dominant chimp. Since a non-dominant chimpanzee is not allowed to take food from a dominant chimpanzee, the non-dominant chimp is reluctant to approach food.  The results were clear: the nondominant chimpanzees avoided food that the dominants could see. Thus, they concluded that chimpanzees understand others’ perceptual states.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;Some researchers have claimed that chimpanzees possess a theory of mind, while others have disputed this claim, offering alternative explanations for their behaviors. For example, Call and Tomasello (2008) have argued that chimpanzees have a theory of mind and that their behavior can be interpreted as representing not just actions, but also intentions. However, Povinelli and Vond (2003, 2004) and Penn (2008) have suggested that chimpanzees may be able to read behaviors rather than minds, based on their experiences with feeding behaviors and the consequences of approaching food in the presence of dominant individuals.&lt;&#x2F;p&gt;
&lt;p&gt;The debate over the existence of a theory of mind in chimpanzees highlights the complexity of studying animal cognition and the need for careful examination of experimental methods and findings. Further research is needed to fully understand the cognitive abilities of non-human animals.&lt;&#x2F;p&gt;
&lt;p&gt;It is important to approach findings about animal cognition with skepticism, and to consider the potential limitations and alternative explanations of such findings. As the story of Clever Hans illustrates, the perception of animals&#x27; mental states can be easily influenced by our own biases and assumptions. We must be vigilant in avoiding the pitfalls of anthropomorphism and seek to understand animal behavior in a rigorous and scientific manner. To this end, it is crucial to carefully consider potential &quot;killjoy explanations&quot; when studying animal cognition, in order to avoid overstating the capabilities and mental states of non-human animals (Shettleworth, 2010).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;can-structural-differences-depend-on-the-environment&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Can Structural differences depend on the environment?&lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The structural differences between laboratory-born and wild-born animal subjects can have a significant impact on our understanding of animal cognition. In a recent study, Janmaat (2019) highlighted the importance of considering brain plasticity, or the brain&#x27;s ability to adapt and change in response to environmental factors. This is especially relevant when comparing the cognitive abilities of laboratory-born and wild-born animals, as their living conditions may vary greatly. Janmaat&#x27;s study also found that while captive-based studies were cited more often than field-based studies, field-based studies were more balanced in their citations. This suggests that comparing the cognitive abilities of animals in their natural habitats to those in captivity can provide valuable insights into the evolution of cognitive abilities. Overall, it is crucial to approach studies of animal cognition with a robust methodology and a consideration of the potential biases and limitations of our findings.&lt;&#x2F;p&gt;
&lt;p&gt;In 1970, Gallup (1970) conducted a pioneering study to determine whether chimpanzees possess the cognitive ability of self-recognition. In this study, chimpanzees were first exposed to a mirror for several days. Afterwards, while under anesthesia, the chimps were marked with dye. After recovering from the anesthesia, the chimps were observed to touch the marks more frequently when a mirror was present than when it was absent. This mirror touch test is often taken as evidence of self-awareness or recognition. However, I believe that this method is not particularly robust when applied to animals. When we look in the mirror, we see our own reflections and we understand that the image is a representation of ourselves. However, we cannot assume that animals have the same cognitive process. This idea is based on our own assumptions and, as such, the mirror test is not a reliable way of determining self-awareness in animals. Povinelli (2000) suggests that chimpanzees may see the reflection in the mirror as a &quot;strange object&quot; that they are able to manipulate through their own movements. Therefore, it is possible that, although chimpanzees can detect marks in mirrors, they either do not understand the image to be a representation at all, or they understand it to be a representation of a body that they do not have any emotional connection to. Therefore, the mirror task does not necessarily involve a higher-order self-representation process.&lt;&#x2F;p&gt;
&lt;p&gt;It is worth noting that subsequent research by Gallup (1971) utilized laboratory-born chimpanzees that were raised in isolation from an early age, and failed to demonstrate self-recognition when subjected to mirror tasks. This finding supports the notion of brain plasticity, as highlighted by Janmaat (2019), and suggests that the environment in which an animal is raised can greatly impact its cognitive abilities. Furthermore, the failure of these laboratory-born chimpanzees to exhibit self-recognition in mirror tasks calls into question the validity of using such methods to assess self-awareness in non-human animals.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;conclusion&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Conclusion&lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;In conclusion, while animal behavior research is crucial to understanding the cognitive abilities of non-human animals, it is important to approach such findings with skepticism and to carefully consider the implications of our conclusions. The use of rigorous methodologies and a willingness to consider alternative explanations, or &quot;killjoy&quot; explanations, can help ensure that our understanding of animal cognition is grounded in robust evidence. Moreover, the consideration of factors such as brain plasticity and the differences between laboratory-born and wild-born animals can provide a more nuanced and comprehensive understanding of animal cognition. Further research is needed to fully explore these complex issues.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;references&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; References&lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Call, J., &amp;amp; Tomasello, M. (2008). Does the chimpanzee have a theory of mind? 30 years later. Trends in Cognitive Sciences, 12(5), 187–192. doi:10.1016&#x2F;j.tics.2008.02.010&lt;&#x2F;p&gt;
&lt;p&gt;Cook, P., &amp;amp; Wilson, M. (2010). Do young chimpanzees have extraordinary working memory? Psychonomic Bulletin &amp;amp; Review, 17(4), 599–600. doi:10.3758&#x2F;pbr.17.4.599&lt;&#x2F;p&gt;
&lt;p&gt;Gallup, G. G. (1970). Chimpanzees: Self-Recognition. Science, 167(3914), 86–87. doi:10.1126&#x2F;science.167.3914.86&lt;&#x2F;p&gt;
&lt;p&gt;Gallup, G. G., McClure, M. K., Hill, S. D., &amp;amp; Bundy, R. A. (1971). Capacity for Self-Recognition in Differentially Reared Chimpanzees. The Psychological Record, 21(1), 69–74. doi:10.1007&#x2F;bf03393991&lt;&#x2F;p&gt;
&lt;p&gt;Hare, B., Call, J., &amp;amp; Tomasello, M. (2001). Do chimpanzees know what conspecifics know? Animal Behaviour, 61(1), 139–151. doi:10.1006&#x2F;anbe.2000.1518&lt;&#x2F;p&gt;
&lt;p&gt;Humphrey, N. (2012). “This chimp will kick your ass at memory games — but how the hell does he do it?” Trends in Cognitive Sciences, 16(7), 353–355. doi:10.1016&#x2F;j.tics.2012.05.002&lt;&#x2F;p&gt;
&lt;p&gt;Inoue, S., &amp;amp; Matsuzawa, T. (2007). Working memory of numerals in chimpanzees. Current Biology, 17(23), R1004–R1005. doi:10.1016&#x2F;j.cub.2007.10.027&lt;&#x2F;p&gt;
&lt;p&gt;Janmaat, K. R. L. (2019). What animals do not do or fail to find: A novel observational approach for studying cognition in the wild. Evolutionary Anthropology: Issues, News, and Reviews. doi:10.1002&#x2F;evan.21794&lt;&#x2F;p&gt;
&lt;p&gt;Penn, D. C., Holyoak, K. J., &amp;amp; Povinelli, D. J. (2008). Darwin’s mistake: Explaining the discontinuity between human and nonhuman minds. Behavioral and Brain Sciences, 31, 109–178.&lt;&#x2F;p&gt;
&lt;p&gt;Povinelli, D. (2000). Folk physics for apes. Oxford, England: Oxford University Press.&lt;&#x2F;p&gt;
&lt;p&gt;Povinelli, D. J., &amp;amp; Vonk, J. (2003). Chimpanzee minds: Suspiciously human? Trends in Cognitive Sciences, 7, 157–160&lt;&#x2F;p&gt;
&lt;p&gt;Shettleworth, S. J. (2010). Clever animals and killjoy explanations in comparative psychology. Trends in Cognitive Sciences, 14(11), 477–481. doi:10.1016&#x2F;j.tics.2010.07.002&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Leonardo Da Vinci ADHD?</title>
        <published>2021-02-03T00:00:00+00:00</published>
        <updated>2021-02-03T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/adhd/"/>
        <id>https://engineering.videocall.rs/posts/adhd/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/adhd/">&lt;h3 id=&quot;revisiting-genius-was-leonardo-da-vinci-s-creative-spark-fueled-by-adhd&quot;&gt;Revisiting Genius: Was Leonardo da Vinci&#x27;s Creative Spark Fueled by ADHD?&lt;&#x2F;h3&gt;
&lt;p&gt;In the realm of art and science, few names shine as brightly as Leonardo da Vinci. A polymath of the highest order, his contributions to various fields continue to inspire awe and admiration. However, a recent article by Marco Catani and Paolo Mazzarello published in the journal BRAIN offers a fascinating new perspective on da Vinci&#x27;s life and work. The authors propose that da Vinci may have had Attention Deficit Hyperactivity Disorder (ADHD), a condition that could explain his well-documented struggles with procrastination and task completion.&lt;&#x2F;p&gt;
&lt;p&gt;I found this article both surprising and intriguing. The possibility that one of history&#x27;s greatest minds might have had ADHD challenges our conventional understanding of this condition and invites us to reconsider the relationship between neurodiversity and creativity. In this blog post, I aim to summarize the key arguments of Catani and Mazzarello&#x27;s article, discuss potential criticisms, and share my personal reflections on their hypothesis. Whether you&#x27;re a fan of da Vinci, interested in the history of science, or curious about the many facets of the human mind, I hope you&#x27;ll find this exploration as fascinating as I did.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;evidences-about-why-leonardo-had-adhd&quot;&gt;Evidences about why Leonardo had ADHD:&lt;&#x2F;h3&gt;
&lt;p&gt;The authors point to several key pieces of evidence to support their hypothesis. Firstly, da Vinci&#x27;s difficulties were pervasive since childhood, a fundamental characteristic of ADHD. Secondly, he was constantly on the go, often jumping from task to task, a trait common among those with ADHD. Lastly, da Vinci was left-handed and had a severe left hemisphere stroke at age 65, which left his language abilities intact. These clinical observations strongly indicate a reverse right-hemisphere dominance for language in Leonardo’s brain, a trait found in 55% of the general population but more prevalent in individuals with neurodevelopmental conditions, including ADHD.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;potential-criticism-killjoy-explanation&quot;&gt;Potential Criticism:(Killjoy explanation)&lt;&#x2F;h3&gt;
&lt;p&gt;While the hypothesis is intriguing, it&#x27;s important to note that it is speculative. Diagnosing historical figures with modern medical conditions is inherently challenging due to the lack of direct evidence. Moreover, da Vinci&#x27;s work habits could also be attributed to other factors, such as his wide range of interests, the socio-cultural context of his time, or his unique personality traits. ADHD is a complex disorder that should not be diagnosed based solely on anecdotal evidence or behavioral traits.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;dav.jpg&quot; alt=&quot;Saint Jerome in the Wilderness&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;*A work Leonardo started but never finished: Saint Jerome in the Wilderness.&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion:&lt;&#x2F;h3&gt;
&lt;p&gt;Despite potential criticisms, the hypothesis that Leonardo da Vinci may have had ADHD offers a fresh perspective on his life and work. It invites us to reconsider the traditional narrative of genius and suggests that traits often perceived as flaws, such as impulsivity and distractibility, can also fuel creativity and innovation. This idea not only humanizes da Vinci but also provides a source of inspiration for those living with ADHD today.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;references&quot;&gt;References:&lt;&#x2F;h3&gt;
&lt;p&gt;Catani M, Mazzarello P. Grey Matter Leonardo da Vinci: a genius driven to distraction. Brain. 2019 Jun 1;142(6):1842-1846. doi: 10.1093&#x2F;brain&#x2F;awz131. PMID: 31121603; PMCID: PMC6536914.&lt;&#x2F;p&gt;
</content>
        
    </entry>
</feed>
