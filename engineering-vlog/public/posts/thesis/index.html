<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><title>
         My thesis in TLDR
        
    </title><meta content="My thesis in TLDR" property=og:title><meta content="Videocall Engineering" property=og:description><meta content="Videocall Engineering" name=description><link href=/icon/favicon.png rel=icon type=image/png><link href=https://engineering.videocall.rs/fonts.css rel=stylesheet><link title="Videocall Engineering" href=https://engineering.videocall.rs/atom.xml rel=alternate type=application/atom+xml><link href=https://engineering.videocall.rs/theme/light.css rel=stylesheet><link href=https://engineering.videocall.rs/theme/dark.css id=darkModeStyle rel=stylesheet><script src=https://engineering.videocall.rs/js/themetoggle.js></script><script>setTheme(getSavedTheme());</script><link href=https://engineering.videocall.rs/main.css media=screen rel=stylesheet><script data-cf-beacon='{"token": "37a0f981a37240aea00b781e300bc6c0"}' defer src=https://static.cloudflareinsights.com/beacon.min.js></script></head><script>// Function to set the theme
        function setTheme(theme) {
            document.documentElement.className = theme;
            localStorage.setItem('theme', theme);
        }

        // Function to get the saved theme
        function getSavedTheme() {
            return localStorage.getItem('theme') || 'light';
        }

        // Set the theme on page load
        document.addEventListener('DOMContentLoaded', (event) => {
            setTheme(getSavedTheme());
        });</script><body><div class=content><header><div class=main><a href=https://engineering.videocall.rs>Videocall Engineering</a><div class=socials><a class=social href=https://twitter.com/emportent rel=me> <img alt=x src=https://engineering.videocall.rs/social_icons/x.svg> </a><a class=social href=https://github.com/altunenes rel=me> <img alt=github src=https://engineering.videocall.rs/social_icons/github.svg> </a><a href="https://scholar.google.com/citations?user=_OtEw5oAAAAJ&hl" class=social rel=me> <img alt=scholar src=https://engineering.videocall.rs/social_icons/googlescholar.svg> </a><a class=social href=https://www.linkedin.com/in/enes-altun-0a3076167 rel=me> <img alt=linkedin src=https://engineering.videocall.rs/social_icons/linkedin.svg> </a><a class=social href=https://www.instagram.com/altunanes/ rel=me> <img alt=instagram src=https://engineering.videocall.rs/social_icons/instagram.svg> </a><a class=social href=https://orcid.org/0000-0002-6478-6909 rel=me> <img alt=orcid src=https://engineering.videocall.rs/social_icons/orcid.svg> </a><a class=social href=https://www.shadertoy.com/user/altunenes rel=me> <img alt=shadertoy src=https://engineering.videocall.rs/social_icons/shadertoy.svg> </a><a class=social href=https://bsky.app/profile/altunenes.bsky.social rel=me> <img alt=bluesky src=https://engineering.videocall.rs/social_icons/bluesky.svg> </a></div></div><nav><a href=https://engineering.videocall.rs/posts style=margin-left:.7em>/posts</a><a href=https://engineering.videocall.rs/projects style=margin-left:.7em>/projects</a><a href=https://engineering.videocall.rs/about style=margin-left:.7em>/about</a><a href=https://engineering.videocall.rs/tags style=margin-left:.7em>/tags</a><a href=https://engineering.videocall.rs/CV style=margin-left:.7em>/CV</a><button aria-label="Toggle dark mode" id=dark-mode-toggle onclick=toggleTheme();><img alt="Light mode" id=sun-icon src=https://engineering.videocall.rs/feather/sun.svg> <img alt="Dark mode" id=moon-icon src=https://engineering.videocall.rs/feather/moon.svg></button></nav></header><script>function toggleTheme() {
    const html = document.documentElement;
    const currentTheme = html.classList.contains('dark') ? 'dark' : 'light';
    const newTheme = currentTheme === 'light' ? 'dark' : 'light';
    
    html.classList.remove(currentTheme);
    html.classList.add(newTheme);
    
    localStorage.setItem('theme', newTheme);
    updateThemeToggleIcons();
}

function updateThemeToggleIcons() {
    const sunIcon = document.getElementById('sun-icon');
    const moonIcon = document.getElementById('moon-icon');
    const isDarkMode = document.documentElement.classList.contains('dark');
    
    sunIcon.style.display = isDarkMode ? 'none' : 'inline';
    moonIcon.style.display = isDarkMode ? 'inline' : 'none';
}

// Set initial theme
const savedTheme = localStorage.getItem('theme') || 'light';
document.documentElement.classList.add(savedTheme);
updateThemeToggleIcons();</script><main><article><div class=title><div class=page-header>My thesis in TLDR<span class=primary-color style=font-size:1.6em>.</span></div><div class=meta>Posted on <time>2023-01-01</time></div></div><h1>Table of Contents</h1><ul><li><a href=https://engineering.videocall.rs/posts/thesis/#on-which-stage-we-perceive-differently-when-we-classified-faces>On which stage we perceive differently when we classified faces?</a><li><a href=https://engineering.videocall.rs/posts/thesis/#clarifying-the-term-race-in-my-research> Clarifying the Term 'Race' in My Research </a><li><a href=https://engineering.videocall.rs/posts/thesis/#how-i-tried-to-answer-this-question> How I tried to answer this question? </a> <ul><li><a href=https://engineering.videocall.rs/posts/thesis/#but-thats-not-enough> But thats not enough... </a><li><a href=https://engineering.videocall.rs/posts/thesis/#so-what-i-did> So what I did? </a><li><a href=https://engineering.videocall.rs/posts/thesis/#what-i-analyzed> What I analyzed? </a></ul><li><a href=https://engineering.videocall.rs/posts/thesis/#the-tools-which-electrodes-did-i-use> The Tools: Which Electrodes Did I Use? </a><li><a href=https://engineering.videocall.rs/posts/thesis/#what-i-found-the-key-takeaways> What I Found: The Key Takeaways</a><li><a href=https://engineering.videocall.rs/posts/thesis/#softwares-and-tools-i-used> Softwares and Tools I Used</a> <ul><li><a href=https://engineering.videocall.rs/posts/thesis/#open-source-contributions> Open-Source Contributions</a><li><a href=https://engineering.videocall.rs/posts/thesis/#proprietary-software> Proprietary Software</a><li><a href=https://engineering.videocall.rs/posts/thesis/#data-analysis> Data Analysis</a><li><a href=https://engineering.videocall.rs/posts/thesis/#the-fuel-coffee-music-and-friends> The Fuel: Coffee, Music and Friends</a></ul><li><a href=https://engineering.videocall.rs/posts/thesis/#paper-link>Paper Link</a></ul><section class=body><h4 id=on-which-stage-we-perceive-differently-when-we-classified-faces><span style=color:orange>On which stage we perceive differently when we classified faces?</span></h4><p>When I started reading about face perception, I found many studies. Most of these studies say that people see faces from their own race or species differently than others. But none of these studies answered my main question: At what point do we actually start to see these faces differently?<p>When does this happen? Right when the light hits our retinas? Or maybe a bit later, when the signals reach the occipital lobe at the back of our brains? Or is it even later, when our brain's "face recognition software" kicks in?<h3 id=clarifying-the-term-race-in-my-research><span style=color:orange> Clarifying the Term 'Race' in My Research </span></h3><p>In my thesis, I use the term "Race".<p>I want to make it clear that I don't see "race" as a biological concept, and I'm not approaching this from a "racial" perspective. Instead, I borrowed the term from scientific studies that discuss the "other-race effect in face perception."<p>In simpler words, when I say "Race" in my research, I'm talking about faces that you don't usually see in your day-to-day life. For example, in my experiments conducted in Antalya, I used "Asian" faces as the "other" category. Why? Because there aren't many Asian people in Antalya, so these faces would be less "familiar" to the participants.<h3 id=how-i-tried-to-answer-this-question><span style=color:orange> How I tried to answer this question? </span></h3><p>In the world of vision science, the visual hierarchy theory is a cornerstone. It tells us that there are three main stages when our brain classifies what we see:<ul><li>Superordinate Level: Is it alive or not?<li>Basic Level: What kind of thing is it? (e.g., a dog)<li>Subordinate Level: What specific type is it? (e.g., a pitbull)</ul><p>Interestingly, <a href=https://pubmed.ncbi.nlm.nih.gov/25208739/>research</a> has shown that our brain can spot an animal in just 120 milliseconds but takes a bit longer to figure out it's a dog. This quick initial categorization happens at the Basic Level, which aligns perfectly with the focus of my experiments.<p>So, if you see a pitbull, your brain first decides it's a living thing, then identifies it as a dog, and finally as a pitbull. Given this, I chose to zoom in on the "Basic Level" for my experiments. Participants were asked a simple question: "If you see a face, click on X; if you don't see a face, click on Y."<p>You might be wondering, "Why did I choose to focus on the 'Basic Level' and not go deeper into the 'Subordinate Level'?" The answer lies in my interest in low-level visual properties. By keeping the research at the Basic Level, I could delve into the nitty-gritty of early visual processing without getting entangled in the complexities that come with higher-level categorizations like gender or specific facial features.<p>Moreover, a glance at the current ERP-face literature reveals that many studies are already exploring questions like "What gender is this face?" or "Is this face familiar?", even some attentional oddbal tasks... These questions often require processing at the Subordinate Level, which involves more complex and high-level facial features. My aim was to strip away these complexities and get down to the basicsâ€”literally. This approach allowed me to isolate and study the fundamental visual cues that our brains use for face perception, making the research as low-level as possible.<h4 id=but-thats-not-enough><span style=color:orange> But thats not enough... </span></h4><p>This could be answer my question partly but I wanted to know more. So I decided to use "Spatial Frequency", more precisely "Coarse-to-Fine" theory in human vision. According to this theory, when we see an object in the world, we first perceive it's coarse features (low spatial frequency) and then fine features (high spatial frequency) so in the brain a fast-forward mechanism works like this: coarse features -> fine features -> classification.<p>See bellow image for more details:<table><thead><tr><th style=text-align:center><img alt=SF src=/images/sf.png><tbody><tr><td style=text-align:center><em>How brain process a visual object</em></table><h4 id=so-what-i-did><span style=color:orange> So what I did? </span></h4><p>Armed with Python code and some nifty image processing techniques like Fourier Transform, RMSE, and luminance normalization, I set out to explore Spatial Frequencies (SFs). I designed my experiment using E-Prime software. A big shoutout to my awesome colleagues Nurullah, Furkan, and Semih for their invaluable help! Together, we conducted the experiment in our lab and managed to gather data from 30 participants using EEG, all within two weeks.<h4 id=what-i-analyzed><span style=color:orange> What I analyzed? </span></h4><p>I dived deep into the Event-Related Potentials (ERPs) for each participant. Specifically, I looked at:<ul><li><strong>P100</strong>: Important for early visual processing<li><strong>N170</strong>: Crucial for face perception<li><strong>N250</strong>: Relevant for recognizing familiar faces</ul><h3 id=the-tools-which-electrodes-did-i-use><span style=color:orange> The Tools: Which Electrodes Did I Use? </span></h3><p>For the EEG recordings, I used a variety of electrodes but focused my reporting on those placed on the occipital lobe, which is the brain's visual processing center. The electrodes I reported are:<ul><li><strong>O1 and O2</strong>: Standard occipital electrodes<li><strong>PO3 and PO4</strong>: Parieto-occipital electrodes<li><strong>PO7 and PO8</strong>: Additional parieto-occipital electrodes</ul><h3 id=what-i-found-the-key-takeaways><span style=color:orange> What I Found: The Key Takeaways</span></h3><p>I discovered quite a bit through my research, but I've focused on reporting the most crucial findings.<ul><li><p><strong>No Other-Race Effect in Early Stages</strong>: While I did find some significant results in the N250 ERP (especially in the PO electrodes), there were no significant differences in the P100 and N170 ERPs between Asian and Caucasian faces. This suggests that, at least in the early stages of visual processing, we don't perceive faces of different races differently.</p><li><p><strong>Species Differences in Higher Frequencies</strong>: Interestingly, faces of other species didn't differ from human faces in the low spatial frequency domain. However, we do perceive them differently in the high spatial frequency (HSF) or broadband, especially in the P100 ERP. This difference becomes even more distinct in the N170 and N250 ERPs.</p></ul><p>My thesis is still under review, but I've conducted many detailed analyses and learned a lot. You can find the published version of my thesis <a href=https://www.tandfonline.com/doi/full/10.1080/13506285.2024.2415721#abstract>here</a>.<blockquote><p><strong>Note</strong>: Due to the length of the paper, I didn't include the N250 results in the pre-print version. Rest assured, they are thoroughly reported in my full thesis.</blockquote><h3 id=softwares-and-tools-i-used><span style=color:orange> Softwares and Tools I Used</span></h3><h4 id=open-source-contributions><span style=color:orange> Open-Source Contributions</span></h4><p>I love open-source and have contributed some tools that I wrote myself:<ul><li><p><strong><a href=https://github.com/altunenes/easylab>EasyLab</a></strong>: I created a Python GUI as a gift to my lab! It's perfect for resizing images, renaming them with a prefix for easy reading in E-Prime 3.0, and even processing Spatial Frequencies using Fourier and Butterworth filters.</p><li><p><strong><a href=https://github.com/altunenes/scramblery>Scramblery</a></strong>: This tool is for scrambling images. I used scrambled images as a control condition in my research. It's open-source, and there's even a cool JavaScript version that runs online!</p></ul><p>Try it out here: <a href=https://altunenes.github.io/scramblery/scramblerydemo.html>Demo</a><ul><li><strong><a href=https://github.com/altunenes/butter2d>butter2d</a></strong>: Rust implementation for Butterworth filter.</ul><h4 id=proprietary-software><span style=color:orange> Proprietary Software</span></h4><ul><li><p><strong>BrainVision</strong>: I used this for my EEG recordings and initial data filtering. Unfortunately, it's not open-source.</p><li><p><strong>E-Prime 3.0</strong>: I used this for my experiment design. It's buggy and makes you want to pull your hair out, but it's the best option out there for designing experiments. It's also not open-source. Made me think about creating an open-source alternative in the future using Rust while I was using it. Maybe one day!</p></ul><h4 id=data-analysis><span style=color:orange> Data Analysis</span></h4><ul><li><strong>R</strong>: After collecting the data, I did all my analyses using R. You can check out my workflow in this <a href=https://gist.github.com/altunenes/7081d34140335dd7764a92a7bfd12f1d>gist file</a>.</ul><h4 id=the-fuel-coffee-music-and-friends><span style=color:orange> The Fuel: Coffee, Music and Friends</span></h4><ul><li><strong>Coffee</strong>: Consumed in large quantities. I wasn't picky about the brand.<li><strong>Music</strong>: Candlemass kept me company during those long hours of research. I dig into the doom.<li><strong>Friends</strong>: I couln't have done it without my friends. They were both my emotional and technical support. Thank you Nurullah, Furkan and Semih! I'm forever grateful for your help. &LT3 Friendship in the lab is the best thing ever. I'm so lucky to have you guys!</ul><h3 id=paper-link>Paper Link</h3><p>You can read the more boring parts on <a href=https://www.tandfonline.com/doi/full/10.1080/13506285.2024.2415721#abstract>here</a></section><div class=post-tags><nav class="nav tags"><ul class=tags><li><a href=https://engineering.videocall.rs/tags/vision/>vision</a><li><a href=https://engineering.videocall.rs/tags/thesis/>thesis</a><li><a href=https://engineering.videocall.rs/tags/face/>face</a><li><a href=https://engineering.videocall.rs/tags/brain/>brain</a><li><a href=https://engineering.videocall.rs/tags/eeg/>eeg</a></ul></nav></div></article></main><div class=giscus></div><script async crossorigin data-category=General data-category-id=DIC_kwDOL5Nyoc4CfU7n data-emit-metadata=0 data-input-position=bottom data-lang=en data-mapping=pathname data-reactions-enabled=1 data-repo=altunenes/altunenes.github.io data-repo-id=R_kgDOL5NyoQ data-strict=0 data-theme=preferred_color_scheme src=https://giscus.app/client.js></script></div>