<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Videocall Engineering - brain</title>
    <subtitle>Videocall Engineering</subtitle>
    <link rel="self" type="application/atom+xml" href="https://engineering.videocall.rs/tags/brain/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://engineering.videocall.rs"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2023-09-12T00:00:00+00:00</updated>
    <id>https://engineering.videocall.rs/tags/brain/atom.xml</id>
    <entry xml:lang="en">
        <title>About the Reverse Phi Motion Effect</title>
        <published>2023-09-12T00:00:00+00:00</published>
        <updated>2023-09-12T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/rphi/"/>
        <id>https://engineering.videocall.rs/posts/rphi/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/rphi/">&lt;h3 id=&quot;decoding-the-reverse-phi-illusion&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Decoding the Reverse Phi Illusion &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;After diving into the code for reverse phi motion, I found myself pondering its intricacies for days. While I can articulate the phenomenon in terms of programming or mathematics, the scientific literature seemed to offer limited insights into whether this illusion is purely retinal or a result of complex neural interactions, such as those involving the geniculate nucleus or the motion-sensitive neurons in V1.&lt;&#x2F;p&gt;
&lt;p&gt;an example of Reverse Phi illusion I coded in Rust &lt;a href=&quot;https:&#x2F;&#x2F;journals.sagepub.com&#x2F;doi&#x2F;full&#x2F;10.1177&#x2F;2041669518815708&quot;&gt;Flynn &amp;amp; Shapiro, 2018 &lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;video width=&quot;320&quot; height=&quot;240&quot; controls&gt;
    &lt;source src=&quot;https:&#x2F;&#x2F;user-images.githubusercontent.com&#x2F;54986652&#x2F;248949171-4d361b74-e377-4409-9286-525614ff92bf.mp4&quot; type=&quot;video&#x2F;mp4&quot;&gt;
    Your browser does not support the video tag.
  &lt;&#x2F;video&gt;
&lt;&#x2F;div&gt;
&lt;h4 id=&quot;a-glimpse-into-comparative-vision-science&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; A Glimpse into Comparative Vision Science &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;While exploring this topic, I stumbled upon an intriguing article titled &lt;a href=&quot;https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;nn.4050&quot;&gt;&quot;Common Circuit Design in Fly and Mammalian Motion Vision&quot;&lt;&#x2F;a&gt;. Although the paper primarily focuses on flies and mice, its findings can be extrapolated to human vision.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;on-and-off-pathways-the-universal-language-of-vision&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; ON and OFF Pathways: The Universal Language of Vision &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Both flies and humans possess separate pathways for processing ON and OFF signals generated by photoreceptors that respond to changes in luminance. These pathways originate at different synapses in the visual system and have distinct anatomical and functional properties. According to a &lt;a href=&quot;https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;29261684&quot;&gt;2017 study&lt;&#x2F;a&gt;, these ON and OFF pathways are sensitive to specific interactions, which may also be applicable to human psychophysics.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;demo-example&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Demo Example &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;To better understand ON and OFF pathways, check out this interactive shader: &lt;a href=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;view&#x2F;XtdyWj&quot;&gt;ShaderToy Example&lt;&#x2F;a&gt;. This shader illustrates how ON pathways are more activated by a bright stimulus on a dark background, while OFF pathways are more activated by a dark stimulus on a bright background.&lt;&#x2F;p&gt;
&lt;p&gt;Note: That shader code not belong to me!&lt;&#x2F;p&gt;
&lt;h4 id=&quot;the-role-of-correlation-based-algorithms&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; The Role of Correlation-Based Algorithms &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;The article also discusses how both species utilize correlation-based algorithms for motion detection. These algorithms compare temporal changes in luminance at different spatial locations using a multiplication or correlation process. This is in line with another &lt;a href=&quot;https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;18848956&quot;&gt;study&lt;&#x2F;a&gt; that showed equal efficiency in correlating dots of opposite contrast and of similar contrast in reverse-phi motion stimuli.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;reverse-phi-more-than-just-stimulus-properties&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Reverse Phi: More Than Just Stimulus Properties &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;In essence, reverse phi motion is not merely a result of low-level stimulus properties. It appears to be a complex interplay between ON and OFF signals processed through discrete pathways. These interactions can even reverse the direction of motion when contrast changes accompany the discrete motion, as suggested by &lt;a href=&quot;https:&#x2F;&#x2F;dblp.org&#x2F;rec&#x2F;journals&#x2F;neco&#x2F;MoK03&quot;&gt;biophysical simulations&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;why-parallel-processing&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Why Parallel Processing? &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;You might wonder why our visual system even employs this kind of parallel processing. The authors of the article provide an insightful explanation. They suggest that separating ON and OFF signals simplifies the task for motion-sensitive neurons, making it easier to correlate two positive input signals. This seems to resolve a significant biophysical challenge in implementing correlation mechanisms.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;final-thoughts-the-elegance-of-on-and-off-pathways-in-motion-perception&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Final Thoughts: The Elegance of ON and OFF Pathways in Motion Perception &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;As we draw this exploration to a close, let&#x27;s revisit a profound insight from the paper &quot;Common Circuit Design in Fly and Mammalian Motion Vision&quot;. The paper asks an essential question: What could be the advantage of separating ON and OFF signals in both the mouse and fly visual systems?&lt;&#x2F;p&gt;
&lt;p&gt;The separation of ON and OFF pathways in our visual system serves a practical, computational purpose. Motion perception, at its core, involves the temporal correlation of similar events at distinct spatial locations. When a bright object moves across our field of vision, the neurons responsible for detecting this motion have to process both the increasing and the decreasing luminance as the object passes by.&lt;&#x2F;p&gt;
&lt;p&gt;The challenge lies in the biophysics: there&#x27;s no known mechanism that allows a postsynaptic neuron to get excited both when two inputs increase and when they decrease their membrane potential. This is where the genius of biological design comes into play. By segregating ON and OFF signals, each dealing exclusively with brightness increments or decrements, the visual system effectively simplifies the task for motion-sensitive neurons. Now, they only have to correlate two positive input signals, significantly easing the computational burden.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>My thesis in TLDR</title>
        <published>2023-01-01T00:00:00+00:00</published>
        <updated>2023-01-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/thesis/"/>
        <id>https://engineering.videocall.rs/posts/thesis/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/thesis/">&lt;h4 id=&quot;on-which-stage-we-perceive-differently-when-we-classified-faces&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;On which stage we perceive differently when we classified faces?&lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;When I started reading about face perception, I found many studies. Most of these studies say that people see faces from their own race or species differently than others. But none of these studies answered my main question: At what point do we actually start to see these faces differently?&lt;&#x2F;p&gt;
&lt;p&gt;When does this happen? Right when the light hits our retinas? Or maybe a bit later, when the signals reach the occipital lobe at the back of our brains? Or is it even later, when our brain&#x27;s &quot;face recognition software&quot; kicks in?&lt;&#x2F;p&gt;
&lt;h3 id=&quot;clarifying-the-term-race-in-my-research&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Clarifying the Term &#x27;Race&#x27; in My Research &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;In my thesis, I use the term &quot;Race&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;I want to make it clear that I don&#x27;t see &quot;race&quot; as a biological concept, and I&#x27;m not approaching this from a &quot;racial&quot; perspective. Instead, I borrowed the term from scientific studies that discuss the &quot;other-race effect in face perception.&quot;&lt;&#x2F;p&gt;
&lt;p&gt;In simpler words, when I say &quot;Race&quot; in my research, I&#x27;m talking about faces that you don&#x27;t usually see in your day-to-day life. For example, in my experiments conducted in Antalya, I used &quot;Asian&quot; faces as the &quot;other&quot; category. Why? Because there aren&#x27;t many Asian people in Antalya, so these faces would be less &quot;familiar&quot; to the participants.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;how-i-tried-to-answer-this-question&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; How I tried to answer this question? &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;In the world of vision science, the visual hierarchy theory is a cornerstone. It tells us that there are three main stages when our brain classifies what we see:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Superordinate Level: Is it alive or not?&lt;&#x2F;li&gt;
&lt;li&gt;Basic Level: What kind of thing is it? (e.g., a dog)&lt;&#x2F;li&gt;
&lt;li&gt;Subordinate Level: What specific type is it? (e.g., a pitbull)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Interestingly, &lt;a href=&quot;https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;25208739&#x2F;&quot;&gt;research&lt;&#x2F;a&gt; has shown that our brain can spot an animal in just 120 milliseconds but takes a bit longer to figure out it&#x27;s a dog. This quick initial categorization happens at the Basic Level, which aligns perfectly with the focus of my experiments.&lt;&#x2F;p&gt;
&lt;p&gt;So, if you see a pitbull, your brain first decides it&#x27;s a living thing, then identifies it as a dog, and finally as a pitbull. Given this, I chose to zoom in on the &quot;Basic Level&quot; for my experiments. Participants were asked a simple question: &quot;If you see a face, click on X; if you don&#x27;t see a face, click on Y.&quot;&lt;&#x2F;p&gt;
&lt;p&gt;You might be wondering, &quot;Why did I choose to focus on the &#x27;Basic Level&#x27; and not go deeper into the &#x27;Subordinate Level&#x27;?&quot; The answer lies in my interest in low-level visual properties. By keeping the research at the Basic Level, I could delve into the nitty-gritty of early visual processing without getting entangled in the complexities that come with higher-level categorizations like gender or specific facial features.&lt;&#x2F;p&gt;
&lt;p&gt;Moreover, a glance at the current ERP-face literature reveals that many studies are already exploring questions like &quot;What gender is this face?&quot; or &quot;Is this face familiar?&quot;, even some attentional oddbal tasks... These questions often require processing at the Subordinate Level, which involves more complex and high-level facial features. My aim was to strip away these complexities and get down to the basics—literally. This approach allowed me to isolate and study the fundamental visual cues that our brains use for face perception, making the research as low-level as possible.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;but-thats-not-enough&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; But thats not enough... &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;This could be answer my question partly but I wanted to know more. So I decided to use &quot;Spatial Frequency&quot;, more precisely &quot;Coarse-to-Fine&quot; theory in human vision. According to this theory, when we see an object in the world, we first perceive it&#x27;s coarse features (low spatial frequency) and then
fine features (high spatial frequency) so in the brain a fast-forward mechanism works like this: coarse features -&amp;gt; fine features -&amp;gt; classification.&lt;&#x2F;p&gt;
&lt;p&gt;See bellow image for more details:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;sf.png&quot; alt=&quot;SF&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;How brain process a visual object&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h4 id=&quot;so-what-i-did&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; So what I did? &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Armed with Python code and some nifty image processing techniques like Fourier Transform, RMSE, and luminance normalization, I set out to explore Spatial Frequencies (SFs). I designed my experiment using E-Prime software. A big shoutout to my awesome colleagues Nurullah, Furkan, and Semih for their invaluable help! Together, we conducted the experiment in our lab and managed to gather data from 30 participants using EEG, all within two weeks.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-i-analyzed&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; What I analyzed? &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;I dived deep into the Event-Related Potentials (ERPs) for each participant. Specifically, I looked at:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;P100&lt;&#x2F;strong&gt;: Important for early visual processing&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;N170&lt;&#x2F;strong&gt;: Crucial for face perception&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;N250&lt;&#x2F;strong&gt;: Relevant for recognizing familiar faces&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;the-tools-which-electrodes-did-i-use&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; The Tools: Which Electrodes Did I Use? &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;For the EEG recordings, I used a variety of electrodes but focused my reporting on those placed on the occipital lobe, which is the brain&#x27;s visual processing center. The electrodes I reported are:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;O1 and O2&lt;&#x2F;strong&gt;: Standard occipital electrodes&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;PO3 and PO4&lt;&#x2F;strong&gt;: Parieto-occipital electrodes&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;PO7 and PO8&lt;&#x2F;strong&gt;: Additional parieto-occipital electrodes&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;what-i-found-the-key-takeaways&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; What I Found: The Key Takeaways&lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;I discovered quite a bit through my research, but I&#x27;ve focused on reporting the most crucial findings.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;No Other-Race Effect in Early Stages&lt;&#x2F;strong&gt;: While I did find some significant results in the N250 ERP (especially in the PO electrodes), there were no significant differences in the P100 and N170 ERPs between Asian and Caucasian faces. This suggests that, at least in the early stages of visual processing, we don&#x27;t perceive faces of different races differently.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Species Differences in Higher Frequencies&lt;&#x2F;strong&gt;: Interestingly, faces of other species didn&#x27;t differ from human faces in the low spatial frequency domain. However, we do perceive them differently in the high spatial frequency (HSF) or broadband, especially in the P100 ERP. This difference becomes even more distinct in the N170 and N250 ERPs.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;My thesis is still under review, but I&#x27;ve conducted many detailed analyses and learned a lot. You can find the published version of my thesis &lt;a href=&quot;https:&#x2F;&#x2F;www.tandfonline.com&#x2F;doi&#x2F;full&#x2F;10.1080&#x2F;13506285.2024.2415721#abstract&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;&#x2F;strong&gt;: Due to the length of the paper, I didn&#x27;t include the N250 results in the pre-print version. Rest assured, they are thoroughly reported in my full thesis.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;softwares-and-tools-i-used&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Softwares and Tools I Used&lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;open-source-contributions&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Open-Source Contributions&lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;I love open-source and have contributed some tools that I wrote myself:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;easylab&quot;&gt;EasyLab&lt;&#x2F;a&gt;&lt;&#x2F;strong&gt;: I created a Python GUI as a gift to my lab! It&#x27;s perfect for resizing images, renaming them with a prefix for easy reading in E-Prime 3.0, and even processing Spatial Frequencies using Fourier and Butterworth filters.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;scramblery&quot;&gt;Scramblery&lt;&#x2F;a&gt;&lt;&#x2F;strong&gt;: This tool is for scrambling images. I used scrambled images as a control condition in my research. It&#x27;s open-source, and there&#x27;s even a cool JavaScript version that runs online!&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Try it out here: &lt;a href=&quot;https:&#x2F;&#x2F;altunenes.github.io&#x2F;scramblery&#x2F;scramblerydemo.html&quot;&gt;Demo&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;butter2d&quot;&gt;butter2d&lt;&#x2F;a&gt;&lt;&#x2F;strong&gt;: Rust implementation for Butterworth filter.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;proprietary-software&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Proprietary Software&lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;BrainVision&lt;&#x2F;strong&gt;: I used this for my EEG recordings and initial data filtering. Unfortunately, it&#x27;s not open-source.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;E-Prime 3.0&lt;&#x2F;strong&gt;: I used this for my experiment design. It&#x27;s buggy and makes you want to pull your hair out, but it&#x27;s the best option out there for designing experiments. It&#x27;s also not open-source. Made me think about creating an open-source alternative in the future using Rust while I was using it. Maybe one day!&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;data-analysis&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Data Analysis&lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;R&lt;&#x2F;strong&gt;: After collecting the data, I did all my analyses using R. You can check out my workflow in this &lt;a href=&quot;https:&#x2F;&#x2F;gist.github.com&#x2F;altunenes&#x2F;7081d34140335dd7764a92a7bfd12f1d&quot;&gt;gist file&lt;&#x2F;a&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;the-fuel-coffee-music-and-friends&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; The Fuel: Coffee, Music and Friends&lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Coffee&lt;&#x2F;strong&gt;: Consumed in large quantities. I wasn&#x27;t picky about the brand.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Music&lt;&#x2F;strong&gt;: Candlemass kept me company during those long hours of research. I dig into the doom.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Friends&lt;&#x2F;strong&gt;: I couln&#x27;t have done it without my friends. They were both my emotional and technical support. Thank you Nurullah, Furkan and Semih! I&#x27;m forever grateful for your help. &amp;lt;3
Friendship in the lab is the best thing ever. I&#x27;m so lucky to have you guys!&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;paper-link&quot;&gt;Paper Link&lt;&#x2F;h3&gt;
&lt;p&gt;You can read the more boring parts on &lt;a href=&quot;https:&#x2F;&#x2F;www.tandfonline.com&#x2F;doi&#x2F;full&#x2F;10.1080&#x2F;13506285.2024.2415721#abstract&quot;&gt;here&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Why I love optical illusions</title>
        <published>2022-08-02T00:00:00+00:00</published>
        <updated>2022-08-02T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/illusions/"/>
        <id>https://engineering.videocall.rs/posts/illusions/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/illusions/">&lt;p&gt;Ever stumbled upon something so interesting that you just couldn&#x27;t let it go? That&#x27;s what happened to me with optical illusions. My first exposure to optical illusions was in my undergraduate intro textbooks, and that was the point from which I was captured. So, whenever I see a new illusion, mostly on Twitter, my day is ruined because my brain just won&#x27;t let it go until I try to reproduce it with coding and understand the mechanism at a low level.&lt;&#x2F;p&gt;
&lt;p&gt;Four years (2018-2022) ago my coding skills were. well, let&#x27;s say, they were just born. But the challenge to create these optical illusions was very tempting. I wanted to dig deeper into this, so as to control the illusion. I saw how with just small changes in the code, it became stronger or disappeared.&lt;&#x2F;p&gt;
&lt;p&gt;It had been a very steep learning curve, but every line of code was a step closer to finally unraveling the mysteries of human vision. After so much coding and debugging, I finally reached a repository using which you can display and manipulate almost all famous optical illusions with just a few lines of code &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;sorceress&quot;&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;sorceress&lt;&#x2F;font&gt;&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;. And for those curious minds, I&#x27;ve also documented the science and explanations behind each illusion. Not only illusions explained, but you can also find the API documentation for humans, of course &lt;a href=&quot;https:&#x2F;&#x2F;altunenes.github.io&#x2F;sorceress&#x2F;explanations%20of%20illusions&#x2F;&quot;&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;here&lt;&#x2F;font&gt;&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Sometimes I look at my old code and cringe a little, but then I smile—it shows me only how far I came. From a total beginner to this comprehensive repo on Optical Illusions, the journey has been quite something. I don&#x27;t really contribute to this repository since I mostly write my code in Rust. So if you are in search of more dynamic, solid, and the latest illusions, check my other repository
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&quot;&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;Rust&lt;&#x2F;font&gt;&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;.
.&lt;&#x2F;p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;video width=&quot;320&quot; height=&quot;240&quot; controls&gt;
    &lt;source src=&quot;https:&#x2F;&#x2F;user-images.githubusercontent.com&#x2F;54986652&#x2F;248949171-4d361b74-e377-4409-9286-525614ff92bf.mp4&quot; type=&quot;video&#x2F;mp4&quot;&gt;
    Your browser does not support the video tag.
  &lt;&#x2F;video&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;But why in the world, you may ask, why all this effort? Well, to be totally honest, I really don&#x27;t know.&lt;&#x2F;p&gt;
&lt;p&gt;This is actually a question that I always come across, especially on interviews that I mostly bomb. But the answer probably lies in the complexity of human vision. I believe most of the understanding of vision from us is based on implicit assumptions. Optical illusions challenge these assumptions more like little experiments and with an irresistibly captivating factor about them: seeing how easily our vision can be played with. And let&#x27;s not forget the human curiosity factor. Perhaps one of the questions for research could be the fascination of optical illusions and, in fact, lifelong attempts to understand them. Yes, I do realize that up to now I haven&#x27;t had any concrete answer to the question. I tried to play tricks with your brain through the illusion to let you think I have a concrete answer. I am sorry for that.&lt;&#x2F;p&gt;
</content>
        
    </entry>
</feed>
