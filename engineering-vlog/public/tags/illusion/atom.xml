<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Videocall Engineering - illusion</title>
    <subtitle>Videocall Engineering</subtitle>
    <link rel="self" type="application/atom+xml" href="https://engineering.videocall.rs/tags/illusion/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://engineering.videocall.rs"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2025-06-22T00:00:00+00:00</updated>
    <id>https://engineering.videocall.rs/tags/illusion/atom.xml</id>
    <entry xml:lang="en">
        <title>How LLMs See Illusory Faces</title>
        <published>2025-06-22T00:00:00+00:00</published>
        <updated>2025-06-22T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/pareidolia/"/>
        <id>https://engineering.videocall.rs/posts/pareidolia/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/pareidolia/">&lt;h4 id=&quot;background&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Background &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Seeing faces in inanimate objects —a phenomenon called pareidolia— is a common human experience. With today&#x27;s powerful Vision-Language Models (VLMs), a simple question arises: do they see these illusory faces too? Probing their &quot;vision&quot; this way isn&#x27;t just a fun experiment. It&#x27;s a practical way to understand their inner workings, challenge our assumptions when building with them, and explore the gap between artificial and biological sight.&lt;&#x2F;p&gt;
&lt;p&gt;This question grew out of my master&#x27;s research, where I used EEG to study how the human brain processes these very illusions, which led to an &lt;a href=&quot;https:&#x2F;&#x2F;papers.ssrn.com&#x2F;sol3&#x2F;papers.cfm?abstract_id=4341900&quot;&gt;ERP paper&lt;&#x2F;a&gt; on the topic.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-is-spatial-frequency-and-coarse-to-fine-theory&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; What is Spatial Frequency and Coarse-to-Fine Theory? &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;So, how do you fairly test an AI&#x27;s perception? My plan was to see if it falls for the same visual shortcuts our brains do.&lt;&#x2F;p&gt;
&lt;p&gt;This is based on a key idea in human vision called the &lt;strong&gt;Coarse-to-Fine&lt;&#x2F;strong&gt; theory. In short, human brain processes the blurry, general, coarse, &quot;gist&quot; of something first, and then uses that initial guess to figure out the finer details more quickly. The technical way to separate the &quot;gist&quot; from the &quot;details&quot; is with &lt;strong&gt;spatial frequencies&lt;&#x2F;strong&gt;, which can be isolated using techniques like 2D Fourier filtering.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Low Spatial Frequencies (LSF)&lt;&#x2F;strong&gt; are the blurry, large-scale shapes.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;High Spatial Frequencies (HSF)&lt;&#x2F;strong&gt; are the sharp edges and fine textures.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;You experience this all the time. Think about recognizing someone from across the street—you see their overall shape long before you see their eyes. While not about spatial frequency directly, a study on hierarchical processing shows a similar &quot;general first&quot; principle: people can spot an &lt;em&gt;animal&lt;&#x2F;em&gt; in an image in just 120ms, but need longer to identify it as a &lt;em&gt;dog&lt;&#x2F;em&gt; (&lt;a href=&quot;https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;25208739&#x2F;&quot;&gt;Wu et al., 2014&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
&lt;p&gt;My whole experiment was designed around this: would the AI also see a face in the blur, but get confused by the sharp details? To test this, I needed to isolate these frequencies.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;sf.png&quot; alt=&quot;SF&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Visualizing the Coarse-to-Fine theory. The left shows an image broken into coarse (Low Frequency) and fine (High Frequency) information. The right shows how the brain processes the coarse &#x27;gist&#x27; first to guide perception.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;To do this, I used a Butterworth filter from my own Rust tool, which you can try out here: &lt;a href=&quot;https:&#x2F;&#x2F;altunenes.github.io&#x2F;butter2d&#x2F;&quot;&gt;butter2d&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-llms-see-illusions&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; How LLMs see illusions &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Research shows that modern VLMs are not objective, infallible observers; they can be &quot;fooled&quot; by classic visual illusions, and their susceptibility often increases with model scale &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00047&quot;&gt;Shen et al., 2023&lt;&#x2F;a&gt; . This suggests they are learning statistical heuristics from their training data that mimic human perception, rather than developing a deep, structural understanding of the world.&lt;&#x2F;p&gt;
&lt;p&gt;This makes pareidolia a particularly interesting test. It&#x27;s not a geometric trick, but an illusion driven by a powerful, top-down, and likely evolutionary bias to find faces in our environment.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;the-experiment&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; The Experiment &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;To test how Gemini handles pareidolia, I took several images and created three versions of each (using butterworth filter): the original &lt;strong&gt;Broadband (BB)&lt;&#x2F;strong&gt;, a blurry &lt;strong&gt;Low Spatial Frequency (LSF)&lt;&#x2F;strong&gt; version, and a sharp-edged &lt;strong&gt;High Spatial Frequency (HSF)&lt;&#x2F;strong&gt; version. I then fed them to the model with a simple prompt. To avoid any &quot;memory&quot; or context-priming effects, each images was processed in a completely separate session.&lt;&#x2F;p&gt;
&lt;p&gt;Here was the prompt:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&quot;What are the three most prominent objects you see in this image? Respond in a JSON format where each object has a &#x27;name&#x27; and a &#x27;confidence_score&#x27;.&quot;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Here are the results.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;test-1-the-wire-face&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Test 1: The Wire Face &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;This image of server cables, which I found on X (formerly Twitter), has an uncanny facial structure. As hypothesized, the LLM completely missed the face in the broadband and HSF versions, describing only the literal content. However, when presented with the LSF version, where only the coarse, global shape remains, it immediately and confidently identified a &lt;strong&gt;&quot;Face&quot;&lt;&#x2F;strong&gt;. So the fine HSF details of the wires and components seem to break the illusion for the model, while the blurry LSF version provides the ideal template for a face.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;W.png&quot; alt=&quot;W&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;LSF (left), Broadband (middle), HSF (right) versions of the &#x27;Wire Face&#x27;.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h4 id=&quot;test-2-the-church-face&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Test 2: The Church Face &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Next, I used one of the most famous pareidolia images on the internet. My rationale was that if the model&#x27;s perception is purely a function of its training data, it would have surely seen this image and would recognize the face (or illusory face). Once again, it failed to see the face in the BB and HSF versions, focusing only on the architecture. But in the LSF version, it correctly identified a &lt;strong&gt;&quot;Face (Pareidolia)&quot;&lt;&#x2F;strong&gt;. This suggests the model&#x27;s failure isn&#x27;t just about a lack of training data. The high-frequency details of the building&#x27;s facade actively mask the illusion for the AI, even for a classic example.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;C.png&quot; alt=&quot;C&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The famous &#x27;Church Face&#x27; pareidolia.&lt;&#x2F;em&gt;&lt;br&gt;&lt;em&gt;Note that the model only sees a &quot;Face (Pareidolia)&quot; in the LSF version, describing only architectural elements in the others.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h4 id=&quot;test-3-the-oval-hypothesis&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Test 3: The Oval Hypothesis &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;The previous results sparked a new idea: perhaps the LLM&#x27;s internal &quot;face template&quot; is strongly biased towards the oval, rounded shapes of human faces? The &quot;Wire Face&quot; is very angular. To test this, I selected two images with a more circular structure.&lt;&#x2F;p&gt;
&lt;p&gt;The first, an electrical component, followed the now-established pattern. A &lt;strong&gt;&quot;Illusory Face&quot;&lt;&#x2F;strong&gt; was detected in the LSF version, but the BB and HSF versions were seen only as literal machine parts.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;E.png&quot; alt=&quot;E&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;An illusory face in an electrical component.&lt;&#x2F;em&gt;&lt;br&gt;&lt;em&gt;The LSF version triggered a &quot;Pareidolia Face&quot; detection, while the detailed versions only yielded descriptions of machine parts.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The second image, however, gave a breakthrough result. This time, the LLM saw a face in &lt;strong&gt;all three versions!&lt;&#x2F;strong&gt; It identified a &quot;Face&quot; in both LSF and BB, and even a &lt;strong&gt;&quot;Illusory Face&quot;&lt;&#x2F;strong&gt; in the sharp HSF image. This is a brilliant finding. It suggests that when an object&#x27;s structure is a strong enough match for the AI&#x27;s internal face template (a round shape, two distinct &quot;eyes,&quot; a &quot;mouth&quot;), it can overcome the distracting HSF noise. This is also highly consistent with human vision, where HSF information is vital for analyzing the fine features &lt;em&gt;of a face&lt;&#x2F;em&gt; once it has been detected.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;M.png&quot; alt=&quot;M&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;A illusory face on a can or clock mechanism that the LLM saw in all versions.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h4 id=&quot;test-4-the-illusory-robot&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Test 4: The Illusory Robot &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;This next test uses a common object that happens to have face-like features: a set of viewpoint binoculars. The results show another interesting form of interpretation by the model. In the LSF version, the blurry shape with two prominent circles triggers an anthropomorphic classification: &lt;strong&gt;&quot;robot&quot;&lt;&#x2F;strong&gt;. The model defaults to a familiar humanoid template. However, once the HSF details are available in the broadband and sharp versions, the model corrects its initial &quot;guess&quot; and accurately identifies the object as &lt;strong&gt;&quot;viewpoint binoculars&quot;&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;R.png&quot; alt=&quot;R&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;An LSF-induced &quot;robot&quot; is corrected into binoculars with more detail.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h4 id=&quot;test-5-the-holistic-face-paintings&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Test 5: The Holistic Face Paintings &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Finally, I moved from pure pareidolia to a different kind of illusion: composite portraits, famously painted by artists like Giuseppe Arcimboldo. In these images, the face is &lt;strong&gt;intentionally&lt;&#x2F;strong&gt; constructed from other objects. These aren&#x27;t really pareidolia in the same way; they are deliberate artistic constructions where recognizing the face requires &lt;strong&gt;holistic processing&lt;&#x2F;strong&gt;—seeing the overall arrangement rather than just the sum of the individual parts. How would the LLM fare?&lt;&#x2F;p&gt;
&lt;p&gt;The first painting is a face constructed from a landscape. Interestingly, the model identified a &lt;strong&gt;&quot;large face&quot;&lt;&#x2F;strong&gt; in both the broadband and high-frequency versions. This is a departure from the earlier pareidolia examples. Here, the individual components (trees, rocks) don&#x27;t look like facial features on their own, but their careful arrangement creates a powerful holistic impression that the model was able to perceive, even with all the fine details present.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;P.png&quot; alt=&quot;P&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;A composite face, testing holistic perception.&lt;&#x2F;em&gt;&lt;br&gt;&lt;em&gt;Notably, the model identified the face in all three versions, even describing it as an &#x27;optical illusion&#x27; in the BB.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;I tried a second, similar painting of a shepherd in a landscape forming a face. The results were just as intriguing. In the full-detail versions, the model successfully identified both the whole (&lt;strong&gt;&quot;Face&quot;&lt;&#x2F;strong&gt;) and the parts (&lt;strong&gt;&quot;Sheep&quot;&lt;&#x2F;strong&gt;, &lt;strong&gt;&quot;Shepherd&quot;&lt;&#x2F;strong&gt;). It seemed to parse the image on multiple levels simultaneously.&lt;&#x2F;p&gt;
&lt;p&gt;However, an interesting twist occurred in the LSF version. The blur, which helped reveal faces in the pareidolia examples, seemed to &lt;em&gt;weaken&lt;&#x2F;em&gt; the illusion here. The model&#x27;s top guess for the LSF version was &lt;strong&gt;&quot;Tree&quot;&lt;&#x2F;strong&gt;, not &quot;Face&quot;. This might suggest that for these complex, deliberately constructed images, the precise arrangement and HSF details are actually &lt;em&gt;critical&lt;&#x2F;em&gt; for the holistic face to emerge, and blurring them can break the carefully crafted composition. It&#x27;s a fascinating case where the general rule (LSF reveals faces) is reversed, highlighting the complexity of both human and machine perception.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;P2.png&quot; alt=&quot;P2&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Another composite face, where blur seemed to hinder, rather than help, perception.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h3 id=&quot;conclusion-and-final-thoughts&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Conclusion and Final Thoughts &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;For anyone working with or building on top of these AI systems, I believe understanding these kinds of behaviors is important. An AI&#x27;s failure to see a pattern that is obvious to us—or its tendency to see one only under specific conditions like blurring—highlights the inherent differences in how they process visual information.&lt;&#x2F;p&gt;
&lt;p&gt;This method of probing with spatial frequencies and illusions could serve as a simple, fun, intuitive benchmark for tracking the progress of future vision models. As new architectures are developed, seeing how they handle these edge cases can tell us a lot about whether they are developing more robust, human-like perception or simply becoming better at pattern-matching their training data.&lt;&#x2F;p&gt;
&lt;p&gt;Of course, there are clear limitations here. I only used one model, Gemini 2.5 Pro, primarily because company I work provides free access to it. Other powerful models from OpenAI, Anthropic, or elsewhere might react to these images in completely different ways. The number of images was also small.&lt;&#x2F;p&gt;
&lt;p&gt;Also you might be realized that model&#x27;s own distinction between a &quot;Face&quot; and a &quot;Pareidolia Face&quot;. What is the difference? When the model uses the simple &quot;Face&quot; label, does it believe it&#x27;s seeing a real person or animal? Is the &quot;Pareidolia&quot; tag an admission that it recognizes the illusion?&lt;&#x2F;p&gt;
&lt;p&gt;Perhaps the most important takeaway is the value of using novel stimuli. The real test for these models isn&#x27;t showing them the famous &quot;Church Face&quot; again, but presenting them with the new, illusory faces we discover in our daily lives—a pattern in a coffee stain, the front of a new car, or a strangely-shaped vegetable. These &quot;wild&quot; pareidolia images, which the AI could not have been trained on, are the truest test of whether they are learning to &lt;em&gt;see&lt;&#x2F;em&gt; or just to &lt;em&gt;recognize&lt;&#x2F;em&gt;. And for me, that&#x27;s an experiment that never gets old.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Illusory Movement of Dotted Lines but Gabor Version</title>
        <published>2024-12-05T00:00:00+00:00</published>
        <updated>2024-12-05T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/gabordots/"/>
        <id>https://engineering.videocall.rs/posts/gabordots/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/gabordots/">&lt;h2 id=&quot;introduction&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Introduction&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Today, I was exploring &lt;a href=&quot;https:&#x2F;&#x2F;michaelbach.de&#x2F;ot&#x2F;mot-dottedLines&#x2F;index.html&quot;&gt;Michael Bach&#x27;s&lt;&#x2F;a&gt; comments about some illusions and I stumbled upon an illusion called &quot;Dotted Line Motion Illusion&quot; that I hadn&#x27;t known before. I wanted to read more about it because the effect didn&#x27;t seem to work well for me. Both Bach and the original article used a rectangular checkerboard design for the illusion. I tried to reproduce the code in ShaderToy and noticed that the scale of the rectangles and the background color significantly affect the strength of the illusion, at least from my perception.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;plain-version&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Plain Version&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;First, investigate the illusion below. Click the play button, track the red disc as it moves, and notice how the checkered lines seem to shift. It works best on a big screen.&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;450&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;XcKXRV?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;After finishing the coding, I pondered whether &quot;The reason is that the black&#x2F;white contrast signals between adjacent dots along the length of the line are stronger than black&#x2F;grey or white&#x2F;grey contrast signals across the line, and the motion is computed as a vector sum of local contrast-weighted motion signals.&quot; could be an explanation, and then, could Gabor patches be more effective here?&lt;&#x2F;p&gt;
&lt;h2 id=&quot;gabor-version&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Gabor Version&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Here&#x27;s what I did. Now, try this and see which one appears stronger. Interestingly, even on a smaller screen, this version works much better for me and it&#x27;s really functioning very nicely.&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;450&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;McKSRK?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;h2 id=&quot;exploring-reverse-effects&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Exploring Reverse Effects&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Even more interestingly, I discovered a reverse effect when I animated the phase offset. Follow the red dot again, and you&#x27;ll notice that the phase movement stops at some point.&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;450&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;4fyXz3?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;h2 id=&quot;reference-paper&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Reference Paper&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;For more detailed information on the scientific background of these visual phenomena, refer to the following paper:&lt;&#x2F;p&gt;
&lt;p&gt;Ito, H., Anstis, S., &amp;amp; Cavanagh, P. (2009). Illusory Movement of Dotted Lines. Perception, 38(9), 1405-1409. &lt;a href=&quot;https:&#x2F;&#x2F;doi.org&#x2F;10.1068&#x2F;p6383&quot;&gt;https:&#x2F;&#x2F;doi.org&#x2F;10.1068&#x2F;p6383&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Reverse Phi Motion Using Gabors</title>
        <published>2023-10-15T00:00:00+00:00</published>
        <updated>2023-10-15T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/gaborillusion/"/>
        <id>https://engineering.videocall.rs/posts/gaborillusion/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/gaborillusion/">&lt;h3 id=&quot;new-gabor-illusion&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; New Gabor Illusion &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;While diving into some hands-on coding with &lt;a href=&quot;https:&#x2F;&#x2F;altunenes.github.io&#x2F;posts&#x2F;2023&#x2F;10&#x2F;gabor&#x2F;&quot;&gt;Gabor patches&lt;&#x2F;a&gt;, a fascinating idea struck me: What if I sculpted shapes like triangles or ellipses, infused them with some sine phases, and voila—could I possibly evoke a simple reverse phi effect? 🤔🔄&lt;&#x2F;p&gt;
&lt;p&gt;To my delight, I believe I succeeded! 🎉 Navigate your cursor and hit the space bar to unveil the effect. 🖱️⌨️ Your insights and thoughts are eagerly awaited! =)&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;dsdfzS?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;or look at that =) not: unlike other &quot;reverse phi motion&quot; effects, this one doesn&#x27;t have &quot;thin edges&quot; with sine phases. Its just Gabors... 😁&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;dscfzs?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Asahi illusion on the Fractal!</title>
        <published>2023-09-30T00:00:00+00:00</published>
        <updated>2023-09-30T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/asahi/"/>
        <id>https://engineering.videocall.rs/posts/asahi/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/asahi/">&lt;h2 id=&quot;background&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Background &lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;While experimenting with shader code, I stumbled upon a fascinating visual phenomenon. When focusing on the center of a particular design, surrounded by colorful petals, the center appears brighter than it actually is.&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;DsfyRX?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;Dr. Bruno Laeng&#x27;s study unveiled that our brain is deceived into triggering a pupillary light reflex, causing our pupils to constrict as if protecting our eyes from intense light, like the sun&#x27;s rays.&lt;&#x2F;p&gt;
&lt;p&gt;The Asahi illusion&#x27;s effect on the pupil isn&#x27;t instantaneous. In humans, there&#x27;s a notable delay between the onset of the illusion and the pupillary response. This delay might be attributed to the time required for the brain&#x27;s processing mechanisms to influence the pupillary light reflex.&lt;&#x2F;p&gt;
&lt;p&gt;Interestingly, the Visual Cortex (V1) seems to play a pivotal role. The V1 response to the Asahi illusion precedes the pupil constriction, suggesting its potential involvement in modulating the Autonomic Nervous System (ANS). However, the exact pathways, be it direct projections or intricate subcortical synapses, remain a topic of ongoing &lt;a href=&quot;https:&#x2F;&#x2F;academic.oup.com&#x2F;cercor&#x2F;article&#x2F;33&#x2F;12&#x2F;7952&#x2F;7084649?login=false&quot;&gt;research&lt;&#x2F;a&gt; .&lt;&#x2F;p&gt;
&lt;p&gt;here is another one I coded after the above one.&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;MX23Wz?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;43SGDh?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;Note, you can download the asahi illusion demos with using interactive GUI I implemented (easy to change the parameters like colors etc) on here: (I compiled them with Rust :-) &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&quot;&gt;Source code:&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;asahi-demo-downloads&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Asahi Demo Downloads &lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Software Version&lt;&#x2F;th&gt;&lt;th&gt;Operating System&lt;&#x2F;th&gt;&lt;th&gt;Download Link&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Asahi&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;macOS&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi-macos-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;Ubuntu&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi-ubuntu-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;Windows&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi-windows-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Asahi2&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;macOS&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi2-macos-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;Ubuntu&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi2-ubuntu-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;Windows&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi2-windows-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>About the Reverse Phi Motion Effect</title>
        <published>2023-09-12T00:00:00+00:00</published>
        <updated>2023-09-12T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/rphi/"/>
        <id>https://engineering.videocall.rs/posts/rphi/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/rphi/">&lt;h3 id=&quot;decoding-the-reverse-phi-illusion&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Decoding the Reverse Phi Illusion &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;After diving into the code for reverse phi motion, I found myself pondering its intricacies for days. While I can articulate the phenomenon in terms of programming or mathematics, the scientific literature seemed to offer limited insights into whether this illusion is purely retinal or a result of complex neural interactions, such as those involving the geniculate nucleus or the motion-sensitive neurons in V1.&lt;&#x2F;p&gt;
&lt;p&gt;an example of Reverse Phi illusion I coded in Rust &lt;a href=&quot;https:&#x2F;&#x2F;journals.sagepub.com&#x2F;doi&#x2F;full&#x2F;10.1177&#x2F;2041669518815708&quot;&gt;Flynn &amp;amp; Shapiro, 2018 &lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;video width=&quot;320&quot; height=&quot;240&quot; controls&gt;
    &lt;source src=&quot;https:&#x2F;&#x2F;user-images.githubusercontent.com&#x2F;54986652&#x2F;248949171-4d361b74-e377-4409-9286-525614ff92bf.mp4&quot; type=&quot;video&#x2F;mp4&quot;&gt;
    Your browser does not support the video tag.
  &lt;&#x2F;video&gt;
&lt;&#x2F;div&gt;
&lt;h4 id=&quot;a-glimpse-into-comparative-vision-science&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; A Glimpse into Comparative Vision Science &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;While exploring this topic, I stumbled upon an intriguing article titled &lt;a href=&quot;https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;nn.4050&quot;&gt;&quot;Common Circuit Design in Fly and Mammalian Motion Vision&quot;&lt;&#x2F;a&gt;. Although the paper primarily focuses on flies and mice, its findings can be extrapolated to human vision.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;on-and-off-pathways-the-universal-language-of-vision&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; ON and OFF Pathways: The Universal Language of Vision &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Both flies and humans possess separate pathways for processing ON and OFF signals generated by photoreceptors that respond to changes in luminance. These pathways originate at different synapses in the visual system and have distinct anatomical and functional properties. According to a &lt;a href=&quot;https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;29261684&quot;&gt;2017 study&lt;&#x2F;a&gt;, these ON and OFF pathways are sensitive to specific interactions, which may also be applicable to human psychophysics.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;demo-example&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Demo Example &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;To better understand ON and OFF pathways, check out this interactive shader: &lt;a href=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;view&#x2F;XtdyWj&quot;&gt;ShaderToy Example&lt;&#x2F;a&gt;. This shader illustrates how ON pathways are more activated by a bright stimulus on a dark background, while OFF pathways are more activated by a dark stimulus on a bright background.&lt;&#x2F;p&gt;
&lt;p&gt;Note: That shader code not belong to me!&lt;&#x2F;p&gt;
&lt;h4 id=&quot;the-role-of-correlation-based-algorithms&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; The Role of Correlation-Based Algorithms &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;The article also discusses how both species utilize correlation-based algorithms for motion detection. These algorithms compare temporal changes in luminance at different spatial locations using a multiplication or correlation process. This is in line with another &lt;a href=&quot;https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;18848956&quot;&gt;study&lt;&#x2F;a&gt; that showed equal efficiency in correlating dots of opposite contrast and of similar contrast in reverse-phi motion stimuli.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;reverse-phi-more-than-just-stimulus-properties&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Reverse Phi: More Than Just Stimulus Properties &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;In essence, reverse phi motion is not merely a result of low-level stimulus properties. It appears to be a complex interplay between ON and OFF signals processed through discrete pathways. These interactions can even reverse the direction of motion when contrast changes accompany the discrete motion, as suggested by &lt;a href=&quot;https:&#x2F;&#x2F;dblp.org&#x2F;rec&#x2F;journals&#x2F;neco&#x2F;MoK03&quot;&gt;biophysical simulations&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;why-parallel-processing&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Why Parallel Processing? &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;You might wonder why our visual system even employs this kind of parallel processing. The authors of the article provide an insightful explanation. They suggest that separating ON and OFF signals simplifies the task for motion-sensitive neurons, making it easier to correlate two positive input signals. This seems to resolve a significant biophysical challenge in implementing correlation mechanisms.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;final-thoughts-the-elegance-of-on-and-off-pathways-in-motion-perception&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Final Thoughts: The Elegance of ON and OFF Pathways in Motion Perception &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;As we draw this exploration to a close, let&#x27;s revisit a profound insight from the paper &quot;Common Circuit Design in Fly and Mammalian Motion Vision&quot;. The paper asks an essential question: What could be the advantage of separating ON and OFF signals in both the mouse and fly visual systems?&lt;&#x2F;p&gt;
&lt;p&gt;The separation of ON and OFF pathways in our visual system serves a practical, computational purpose. Motion perception, at its core, involves the temporal correlation of similar events at distinct spatial locations. When a bright object moves across our field of vision, the neurons responsible for detecting this motion have to process both the increasing and the decreasing luminance as the object passes by.&lt;&#x2F;p&gt;
&lt;p&gt;The challenge lies in the biophysics: there&#x27;s no known mechanism that allows a postsynaptic neuron to get excited both when two inputs increase and when they decrease their membrane potential. This is where the genius of biological design comes into play. By segregating ON and OFF signals, each dealing exclusively with brightness increments or decrements, the visual system effectively simplifies the task for motion-sensitive neurons. Now, they only have to correlate two positive input signals, significantly easing the computational burden.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Why I love optical illusions</title>
        <published>2022-08-02T00:00:00+00:00</published>
        <updated>2022-08-02T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/illusions/"/>
        <id>https://engineering.videocall.rs/posts/illusions/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/illusions/">&lt;p&gt;Ever stumbled upon something so interesting that you just couldn&#x27;t let it go? That&#x27;s what happened to me with optical illusions. My first exposure to optical illusions was in my undergraduate intro textbooks, and that was the point from which I was captured. So, whenever I see a new illusion, mostly on Twitter, my day is ruined because my brain just won&#x27;t let it go until I try to reproduce it with coding and understand the mechanism at a low level.&lt;&#x2F;p&gt;
&lt;p&gt;Four years (2018-2022) ago my coding skills were. well, let&#x27;s say, they were just born. But the challenge to create these optical illusions was very tempting. I wanted to dig deeper into this, so as to control the illusion. I saw how with just small changes in the code, it became stronger or disappeared.&lt;&#x2F;p&gt;
&lt;p&gt;It had been a very steep learning curve, but every line of code was a step closer to finally unraveling the mysteries of human vision. After so much coding and debugging, I finally reached a repository using which you can display and manipulate almost all famous optical illusions with just a few lines of code &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;sorceress&quot;&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;sorceress&lt;&#x2F;font&gt;&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;. And for those curious minds, I&#x27;ve also documented the science and explanations behind each illusion. Not only illusions explained, but you can also find the API documentation for humans, of course &lt;a href=&quot;https:&#x2F;&#x2F;altunenes.github.io&#x2F;sorceress&#x2F;explanations%20of%20illusions&#x2F;&quot;&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;here&lt;&#x2F;font&gt;&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Sometimes I look at my old code and cringe a little, but then I smile—it shows me only how far I came. From a total beginner to this comprehensive repo on Optical Illusions, the journey has been quite something. I don&#x27;t really contribute to this repository since I mostly write my code in Rust. So if you are in search of more dynamic, solid, and the latest illusions, check my other repository
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&quot;&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;Rust&lt;&#x2F;font&gt;&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;.
.&lt;&#x2F;p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;video width=&quot;320&quot; height=&quot;240&quot; controls&gt;
    &lt;source src=&quot;https:&#x2F;&#x2F;user-images.githubusercontent.com&#x2F;54986652&#x2F;248949171-4d361b74-e377-4409-9286-525614ff92bf.mp4&quot; type=&quot;video&#x2F;mp4&quot;&gt;
    Your browser does not support the video tag.
  &lt;&#x2F;video&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;But why in the world, you may ask, why all this effort? Well, to be totally honest, I really don&#x27;t know.&lt;&#x2F;p&gt;
&lt;p&gt;This is actually a question that I always come across, especially on interviews that I mostly bomb. But the answer probably lies in the complexity of human vision. I believe most of the understanding of vision from us is based on implicit assumptions. Optical illusions challenge these assumptions more like little experiments and with an irresistibly captivating factor about them: seeing how easily our vision can be played with. And let&#x27;s not forget the human curiosity factor. Perhaps one of the questions for research could be the fascination of optical illusions and, in fact, lifelong attempts to understand them. Yes, I do realize that up to now I haven&#x27;t had any concrete answer to the question. I tried to play tricks with your brain through the illusion to let you think I have a concrete answer. I am sorry for that.&lt;&#x2F;p&gt;
</content>
        
    </entry>
</feed>
