<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Videocall Engineering - vision</title>
    <subtitle>Videocall Engineering</subtitle>
    <link rel="self" type="application/atom+xml" href="https://engineering.videocall.rs/tags/vision/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://engineering.videocall.rs"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2025-06-22T00:00:00+00:00</updated>
    <id>https://engineering.videocall.rs/tags/vision/atom.xml</id>
    <entry xml:lang="en">
        <title>How LLMs See Illusory Faces</title>
        <published>2025-06-22T00:00:00+00:00</published>
        <updated>2025-06-22T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/pareidolia/"/>
        <id>https://engineering.videocall.rs/posts/pareidolia/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/pareidolia/">&lt;h4 id=&quot;background&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Background &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Seeing faces in inanimate objects —a phenomenon called pareidolia— is a common human experience. With today&#x27;s powerful Vision-Language Models (VLMs), a simple question arises: do they see these illusory faces too? Probing their &quot;vision&quot; this way isn&#x27;t just a fun experiment. It&#x27;s a practical way to understand their inner workings, challenge our assumptions when building with them, and explore the gap between artificial and biological sight.&lt;&#x2F;p&gt;
&lt;p&gt;This question grew out of my master&#x27;s research, where I used EEG to study how the human brain processes these very illusions, which led to an &lt;a href=&quot;https:&#x2F;&#x2F;papers.ssrn.com&#x2F;sol3&#x2F;papers.cfm?abstract_id=4341900&quot;&gt;ERP paper&lt;&#x2F;a&gt; on the topic.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-is-spatial-frequency-and-coarse-to-fine-theory&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; What is Spatial Frequency and Coarse-to-Fine Theory? &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;So, how do you fairly test an AI&#x27;s perception? My plan was to see if it falls for the same visual shortcuts our brains do.&lt;&#x2F;p&gt;
&lt;p&gt;This is based on a key idea in human vision called the &lt;strong&gt;Coarse-to-Fine&lt;&#x2F;strong&gt; theory. In short, human brain processes the blurry, general, coarse, &quot;gist&quot; of something first, and then uses that initial guess to figure out the finer details more quickly. The technical way to separate the &quot;gist&quot; from the &quot;details&quot; is with &lt;strong&gt;spatial frequencies&lt;&#x2F;strong&gt;, which can be isolated using techniques like 2D Fourier filtering.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Low Spatial Frequencies (LSF)&lt;&#x2F;strong&gt; are the blurry, large-scale shapes.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;High Spatial Frequencies (HSF)&lt;&#x2F;strong&gt; are the sharp edges and fine textures.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;You experience this all the time. Think about recognizing someone from across the street—you see their overall shape long before you see their eyes. While not about spatial frequency directly, a study on hierarchical processing shows a similar &quot;general first&quot; principle: people can spot an &lt;em&gt;animal&lt;&#x2F;em&gt; in an image in just 120ms, but need longer to identify it as a &lt;em&gt;dog&lt;&#x2F;em&gt; (&lt;a href=&quot;https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;25208739&#x2F;&quot;&gt;Wu et al., 2014&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
&lt;p&gt;My whole experiment was designed around this: would the AI also see a face in the blur, but get confused by the sharp details? To test this, I needed to isolate these frequencies.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;sf.png&quot; alt=&quot;SF&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Visualizing the Coarse-to-Fine theory. The left shows an image broken into coarse (Low Frequency) and fine (High Frequency) information. The right shows how the brain processes the coarse &#x27;gist&#x27; first to guide perception.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;To do this, I used a Butterworth filter from my own Rust tool, which you can try out here: &lt;a href=&quot;https:&#x2F;&#x2F;altunenes.github.io&#x2F;butter2d&#x2F;&quot;&gt;butter2d&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-llms-see-illusions&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; How LLMs see illusions &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Research shows that modern VLMs are not objective, infallible observers; they can be &quot;fooled&quot; by classic visual illusions, and their susceptibility often increases with model scale &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00047&quot;&gt;Shen et al., 2023&lt;&#x2F;a&gt; . This suggests they are learning statistical heuristics from their training data that mimic human perception, rather than developing a deep, structural understanding of the world.&lt;&#x2F;p&gt;
&lt;p&gt;This makes pareidolia a particularly interesting test. It&#x27;s not a geometric trick, but an illusion driven by a powerful, top-down, and likely evolutionary bias to find faces in our environment.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;the-experiment&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; The Experiment &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;To test how Gemini handles pareidolia, I took several images and created three versions of each (using butterworth filter): the original &lt;strong&gt;Broadband (BB)&lt;&#x2F;strong&gt;, a blurry &lt;strong&gt;Low Spatial Frequency (LSF)&lt;&#x2F;strong&gt; version, and a sharp-edged &lt;strong&gt;High Spatial Frequency (HSF)&lt;&#x2F;strong&gt; version. I then fed them to the model with a simple prompt. To avoid any &quot;memory&quot; or context-priming effects, each images was processed in a completely separate session.&lt;&#x2F;p&gt;
&lt;p&gt;Here was the prompt:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&quot;What are the three most prominent objects you see in this image? Respond in a JSON format where each object has a &#x27;name&#x27; and a &#x27;confidence_score&#x27;.&quot;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Here are the results.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;test-1-the-wire-face&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Test 1: The Wire Face &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;This image of server cables, which I found on X (formerly Twitter), has an uncanny facial structure. As hypothesized, the LLM completely missed the face in the broadband and HSF versions, describing only the literal content. However, when presented with the LSF version, where only the coarse, global shape remains, it immediately and confidently identified a &lt;strong&gt;&quot;Face&quot;&lt;&#x2F;strong&gt;. So the fine HSF details of the wires and components seem to break the illusion for the model, while the blurry LSF version provides the ideal template for a face.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;W.png&quot; alt=&quot;W&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;LSF (left), Broadband (middle), HSF (right) versions of the &#x27;Wire Face&#x27;.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h4 id=&quot;test-2-the-church-face&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Test 2: The Church Face &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Next, I used one of the most famous pareidolia images on the internet. My rationale was that if the model&#x27;s perception is purely a function of its training data, it would have surely seen this image and would recognize the face (or illusory face). Once again, it failed to see the face in the BB and HSF versions, focusing only on the architecture. But in the LSF version, it correctly identified a &lt;strong&gt;&quot;Face (Pareidolia)&quot;&lt;&#x2F;strong&gt;. This suggests the model&#x27;s failure isn&#x27;t just about a lack of training data. The high-frequency details of the building&#x27;s facade actively mask the illusion for the AI, even for a classic example.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;C.png&quot; alt=&quot;C&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The famous &#x27;Church Face&#x27; pareidolia.&lt;&#x2F;em&gt;&lt;br&gt;&lt;em&gt;Note that the model only sees a &quot;Face (Pareidolia)&quot; in the LSF version, describing only architectural elements in the others.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h4 id=&quot;test-3-the-oval-hypothesis&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Test 3: The Oval Hypothesis &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;The previous results sparked a new idea: perhaps the LLM&#x27;s internal &quot;face template&quot; is strongly biased towards the oval, rounded shapes of human faces? The &quot;Wire Face&quot; is very angular. To test this, I selected two images with a more circular structure.&lt;&#x2F;p&gt;
&lt;p&gt;The first, an electrical component, followed the now-established pattern. A &lt;strong&gt;&quot;Illusory Face&quot;&lt;&#x2F;strong&gt; was detected in the LSF version, but the BB and HSF versions were seen only as literal machine parts.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;E.png&quot; alt=&quot;E&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;An illusory face in an electrical component.&lt;&#x2F;em&gt;&lt;br&gt;&lt;em&gt;The LSF version triggered a &quot;Pareidolia Face&quot; detection, while the detailed versions only yielded descriptions of machine parts.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The second image, however, gave a breakthrough result. This time, the LLM saw a face in &lt;strong&gt;all three versions!&lt;&#x2F;strong&gt; It identified a &quot;Face&quot; in both LSF and BB, and even a &lt;strong&gt;&quot;Illusory Face&quot;&lt;&#x2F;strong&gt; in the sharp HSF image. This is a brilliant finding. It suggests that when an object&#x27;s structure is a strong enough match for the AI&#x27;s internal face template (a round shape, two distinct &quot;eyes,&quot; a &quot;mouth&quot;), it can overcome the distracting HSF noise. This is also highly consistent with human vision, where HSF information is vital for analyzing the fine features &lt;em&gt;of a face&lt;&#x2F;em&gt; once it has been detected.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;M.png&quot; alt=&quot;M&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;A illusory face on a can or clock mechanism that the LLM saw in all versions.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h4 id=&quot;test-4-the-illusory-robot&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Test 4: The Illusory Robot &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;This next test uses a common object that happens to have face-like features: a set of viewpoint binoculars. The results show another interesting form of interpretation by the model. In the LSF version, the blurry shape with two prominent circles triggers an anthropomorphic classification: &lt;strong&gt;&quot;robot&quot;&lt;&#x2F;strong&gt;. The model defaults to a familiar humanoid template. However, once the HSF details are available in the broadband and sharp versions, the model corrects its initial &quot;guess&quot; and accurately identifies the object as &lt;strong&gt;&quot;viewpoint binoculars&quot;&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;R.png&quot; alt=&quot;R&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;An LSF-induced &quot;robot&quot; is corrected into binoculars with more detail.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h4 id=&quot;test-5-the-holistic-face-paintings&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Test 5: The Holistic Face Paintings &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Finally, I moved from pure pareidolia to a different kind of illusion: composite portraits, famously painted by artists like Giuseppe Arcimboldo. In these images, the face is &lt;strong&gt;intentionally&lt;&#x2F;strong&gt; constructed from other objects. These aren&#x27;t really pareidolia in the same way; they are deliberate artistic constructions where recognizing the face requires &lt;strong&gt;holistic processing&lt;&#x2F;strong&gt;—seeing the overall arrangement rather than just the sum of the individual parts. How would the LLM fare?&lt;&#x2F;p&gt;
&lt;p&gt;The first painting is a face constructed from a landscape. Interestingly, the model identified a &lt;strong&gt;&quot;large face&quot;&lt;&#x2F;strong&gt; in both the broadband and high-frequency versions. This is a departure from the earlier pareidolia examples. Here, the individual components (trees, rocks) don&#x27;t look like facial features on their own, but their careful arrangement creates a powerful holistic impression that the model was able to perceive, even with all the fine details present.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;P.png&quot; alt=&quot;P&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;A composite face, testing holistic perception.&lt;&#x2F;em&gt;&lt;br&gt;&lt;em&gt;Notably, the model identified the face in all three versions, even describing it as an &#x27;optical illusion&#x27; in the BB.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;I tried a second, similar painting of a shepherd in a landscape forming a face. The results were just as intriguing. In the full-detail versions, the model successfully identified both the whole (&lt;strong&gt;&quot;Face&quot;&lt;&#x2F;strong&gt;) and the parts (&lt;strong&gt;&quot;Sheep&quot;&lt;&#x2F;strong&gt;, &lt;strong&gt;&quot;Shepherd&quot;&lt;&#x2F;strong&gt;). It seemed to parse the image on multiple levels simultaneously.&lt;&#x2F;p&gt;
&lt;p&gt;However, an interesting twist occurred in the LSF version. The blur, which helped reveal faces in the pareidolia examples, seemed to &lt;em&gt;weaken&lt;&#x2F;em&gt; the illusion here. The model&#x27;s top guess for the LSF version was &lt;strong&gt;&quot;Tree&quot;&lt;&#x2F;strong&gt;, not &quot;Face&quot;. This might suggest that for these complex, deliberately constructed images, the precise arrangement and HSF details are actually &lt;em&gt;critical&lt;&#x2F;em&gt; for the holistic face to emerge, and blurring them can break the carefully crafted composition. It&#x27;s a fascinating case where the general rule (LSF reveals faces) is reversed, highlighting the complexity of both human and machine perception.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;P2.png&quot; alt=&quot;P2&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Another composite face, where blur seemed to hinder, rather than help, perception.&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h3 id=&quot;conclusion-and-final-thoughts&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Conclusion and Final Thoughts &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;For anyone working with or building on top of these AI systems, I believe understanding these kinds of behaviors is important. An AI&#x27;s failure to see a pattern that is obvious to us—or its tendency to see one only under specific conditions like blurring—highlights the inherent differences in how they process visual information.&lt;&#x2F;p&gt;
&lt;p&gt;This method of probing with spatial frequencies and illusions could serve as a simple, fun, intuitive benchmark for tracking the progress of future vision models. As new architectures are developed, seeing how they handle these edge cases can tell us a lot about whether they are developing more robust, human-like perception or simply becoming better at pattern-matching their training data.&lt;&#x2F;p&gt;
&lt;p&gt;Of course, there are clear limitations here. I only used one model, Gemini 2.5 Pro, primarily because company I work provides free access to it. Other powerful models from OpenAI, Anthropic, or elsewhere might react to these images in completely different ways. The number of images was also small.&lt;&#x2F;p&gt;
&lt;p&gt;Also you might be realized that model&#x27;s own distinction between a &quot;Face&quot; and a &quot;Pareidolia Face&quot;. What is the difference? When the model uses the simple &quot;Face&quot; label, does it believe it&#x27;s seeing a real person or animal? Is the &quot;Pareidolia&quot; tag an admission that it recognizes the illusion?&lt;&#x2F;p&gt;
&lt;p&gt;Perhaps the most important takeaway is the value of using novel stimuli. The real test for these models isn&#x27;t showing them the famous &quot;Church Face&quot; again, but presenting them with the new, illusory faces we discover in our daily lives—a pattern in a coffee stain, the front of a new car, or a strangely-shaped vegetable. These &quot;wild&quot; pareidolia images, which the AI could not have been trained on, are the truest test of whether they are learning to &lt;em&gt;see&lt;&#x2F;em&gt; or just to &lt;em&gt;recognize&lt;&#x2F;em&gt;. And for me, that&#x27;s an experiment that never gets old.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Illusory Movement of Dotted Lines but Gabor Version</title>
        <published>2024-12-05T00:00:00+00:00</published>
        <updated>2024-12-05T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/gabordots/"/>
        <id>https://engineering.videocall.rs/posts/gabordots/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/gabordots/">&lt;h2 id=&quot;introduction&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Introduction&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Today, I was exploring &lt;a href=&quot;https:&#x2F;&#x2F;michaelbach.de&#x2F;ot&#x2F;mot-dottedLines&#x2F;index.html&quot;&gt;Michael Bach&#x27;s&lt;&#x2F;a&gt; comments about some illusions and I stumbled upon an illusion called &quot;Dotted Line Motion Illusion&quot; that I hadn&#x27;t known before. I wanted to read more about it because the effect didn&#x27;t seem to work well for me. Both Bach and the original article used a rectangular checkerboard design for the illusion. I tried to reproduce the code in ShaderToy and noticed that the scale of the rectangles and the background color significantly affect the strength of the illusion, at least from my perception.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;plain-version&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Plain Version&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;First, investigate the illusion below. Click the play button, track the red disc as it moves, and notice how the checkered lines seem to shift. It works best on a big screen.&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;450&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;XcKXRV?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;After finishing the coding, I pondered whether &quot;The reason is that the black&#x2F;white contrast signals between adjacent dots along the length of the line are stronger than black&#x2F;grey or white&#x2F;grey contrast signals across the line, and the motion is computed as a vector sum of local contrast-weighted motion signals.&quot; could be an explanation, and then, could Gabor patches be more effective here?&lt;&#x2F;p&gt;
&lt;h2 id=&quot;gabor-version&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Gabor Version&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Here&#x27;s what I did. Now, try this and see which one appears stronger. Interestingly, even on a smaller screen, this version works much better for me and it&#x27;s really functioning very nicely.&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;450&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;McKSRK?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;h2 id=&quot;exploring-reverse-effects&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Exploring Reverse Effects&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Even more interestingly, I discovered a reverse effect when I animated the phase offset. Follow the red dot again, and you&#x27;ll notice that the phase movement stops at some point.&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;450&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;4fyXz3?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;h2 id=&quot;reference-paper&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;Reference Paper&lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;For more detailed information on the scientific background of these visual phenomena, refer to the following paper:&lt;&#x2F;p&gt;
&lt;p&gt;Ito, H., Anstis, S., &amp;amp; Cavanagh, P. (2009). Illusory Movement of Dotted Lines. Perception, 38(9), 1405-1409. &lt;a href=&quot;https:&#x2F;&#x2F;doi.org&#x2F;10.1068&#x2F;p6383&quot;&gt;https:&#x2F;&#x2F;doi.org&#x2F;10.1068&#x2F;p6383&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Gabor Patches on the Texture!</title>
        <published>2023-10-16T00:00:00+00:00</published>
        <updated>2023-10-16T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/imgabor/"/>
        <id>https://engineering.videocall.rs/posts/imgabor/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/imgabor/">&lt;p&gt;Below is a video featuring the outcome of my experiment. Take a close look and tell me, do you perceive the face in the image more in the horizontal or vertical orientation of the Gabor patches?&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-media-max-width=&quot;560&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;with different orientations of Gabors... Actually interesting, because this reminds me of research underscoring the important role of horizontal info in face perception, revealing its crucial impact on facial encoding&#x2F;processing while vertical info often reduces these effects. 🙂 &lt;a href=&quot;https:&#x2F;&#x2F;t.co&#x2F;mUcCwJ4SBc&quot;&gt;https:&#x2F;&#x2F;t.co&#x2F;mUcCwJ4SBc&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;t.co&#x2F;ghXyMWS7CO&quot;&gt;pic.twitter.com&#x2F;ghXyMWS7CO&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;&amp;mdash; enes altun (@emportent) &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;emportent&#x2F;status&#x2F;1713689728195690576?ref_src=twsrc%5Etfw&quot;&gt;October 15, 2023&lt;&#x2F;a&gt;&lt;&#x2F;blockquote&gt; &lt;script async src=&quot;https:&#x2F;&#x2F;platform.twitter.com&#x2F;widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;&#x2F;script&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;If you&#x27;ve been following my previous posts, you&#x27;re already familiar with the concept of Gabor patches and how they&#x27;re a fantastic tool for understanding various aspects of visual perception. What&#x27;s particularly intriguing is how these patches can alter our perception of orientation.&lt;&#x2F;p&gt;
&lt;p&gt;findings of &lt;a href=&quot;https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;19757911&#x2F;&quot;&gt;Dakin and Watt (2009)&lt;&#x2F;a&gt; and others(there are a lot of papers about that issue), our perception of faces is significantly influenced by horizontal information compared to vertical. Some call this as &lt;a href=&quot;https:&#x2F;&#x2F;royalsocietypublishing.org&#x2F;doi&#x2F;10.1098&#x2F;rspb.2023.1118#:~:text=The%20radial%20bias%20may%20modulate,the%20individual%20differences%20we%20observe.&quot;&gt;radial bias&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Building on this intriguing concept, Dakin and Watt&#x27;s study delves even deeper into our visual system&#x27;s preference for horizontal features in faces. They introduced the idea of facial &#x27;bar codes,&#x27; unique clusters of horizontal lines, akin to commercial bar codes, that our brains use for quick and efficient face recognition. This theory elegantly explains why we&#x27;re so adept at recognizing faces under various conditions and why certain transformations, like inverting a face, make recognition remarkably challenging. It&#x27;s a compelling reminder of how our visual system has fine-tuned itself over millennia, optimizing certain perceptual shortcuts for survival.&lt;&#x2F;p&gt;
&lt;p&gt;In a nutshell, when the Gabor patches are aligned horizontally, we&#x27;re likely to perceive the face more clearly. This phenomenon ties back to how our brains process faces, giving preferential treatment to horizontal features. It&#x27;s all about how the human visual system has evolved to prioritize certain spatial frequencies and orientations, especially when it comes to recognizing faces - one of the most crucial visual tasks we perform.&lt;&#x2F;p&gt;
&lt;p&gt;But hey, don&#x27;t just take my word for it! Dive into the research, play around with the code, and explore the captivating realm of Gabor patches and visual perception. Who knows what other secrets are waiting to be uncovered?&lt;&#x2F;p&gt;
&lt;p&gt;Note: This code is computationally intensive, so only run it if you&#x27;re confident in your GPU&#x27;s capabilities. Also, please be cautious when adjusting the numbers—small changes can have big impacts!&quot;&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;420&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;Dd3fRB?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Reverse Phi Motion Using Gabors</title>
        <published>2023-10-15T00:00:00+00:00</published>
        <updated>2023-10-15T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/gaborillusion/"/>
        <id>https://engineering.videocall.rs/posts/gaborillusion/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/gaborillusion/">&lt;h3 id=&quot;new-gabor-illusion&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; New Gabor Illusion &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;While diving into some hands-on coding with &lt;a href=&quot;https:&#x2F;&#x2F;altunenes.github.io&#x2F;posts&#x2F;2023&#x2F;10&#x2F;gabor&#x2F;&quot;&gt;Gabor patches&lt;&#x2F;a&gt;, a fascinating idea struck me: What if I sculpted shapes like triangles or ellipses, infused them with some sine phases, and voila—could I possibly evoke a simple reverse phi effect? 🤔🔄&lt;&#x2F;p&gt;
&lt;p&gt;To my delight, I believe I succeeded! 🎉 Navigate your cursor and hit the space bar to unveil the effect. 🖱️⌨️ Your insights and thoughts are eagerly awaited! =)&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;dsdfzS?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;or look at that =) not: unlike other &quot;reverse phi motion&quot; effects, this one doesn&#x27;t have &quot;thin edges&quot; with sine phases. Its just Gabors... 😁&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;dscfzs?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Asahi illusion on the Fractal!</title>
        <published>2023-09-30T00:00:00+00:00</published>
        <updated>2023-09-30T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/asahi/"/>
        <id>https://engineering.videocall.rs/posts/asahi/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/asahi/">&lt;h2 id=&quot;background&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Background &lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;While experimenting with shader code, I stumbled upon a fascinating visual phenomenon. When focusing on the center of a particular design, surrounded by colorful petals, the center appears brighter than it actually is.&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;DsfyRX?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;Dr. Bruno Laeng&#x27;s study unveiled that our brain is deceived into triggering a pupillary light reflex, causing our pupils to constrict as if protecting our eyes from intense light, like the sun&#x27;s rays.&lt;&#x2F;p&gt;
&lt;p&gt;The Asahi illusion&#x27;s effect on the pupil isn&#x27;t instantaneous. In humans, there&#x27;s a notable delay between the onset of the illusion and the pupillary response. This delay might be attributed to the time required for the brain&#x27;s processing mechanisms to influence the pupillary light reflex.&lt;&#x2F;p&gt;
&lt;p&gt;Interestingly, the Visual Cortex (V1) seems to play a pivotal role. The V1 response to the Asahi illusion precedes the pupil constriction, suggesting its potential involvement in modulating the Autonomic Nervous System (ANS). However, the exact pathways, be it direct projections or intricate subcortical synapses, remain a topic of ongoing &lt;a href=&quot;https:&#x2F;&#x2F;academic.oup.com&#x2F;cercor&#x2F;article&#x2F;33&#x2F;12&#x2F;7952&#x2F;7084649?login=false&quot;&gt;research&lt;&#x2F;a&gt; .&lt;&#x2F;p&gt;
&lt;p&gt;here is another one I coded after the above one.&lt;&#x2F;p&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;MX23Wz?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; src=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;embed&#x2F;43SGDh?gui=true&amp;t=10&amp;paused=true&amp;muted=false&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;Note, you can download the asahi illusion demos with using interactive GUI I implemented (easy to change the parameters like colors etc) on here: (I compiled them with Rust :-) &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&quot;&gt;Source code:&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;asahi-demo-downloads&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Asahi Demo Downloads &lt;&#x2F;span&gt;&lt;&#x2F;h2&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Software Version&lt;&#x2F;th&gt;&lt;th&gt;Operating System&lt;&#x2F;th&gt;&lt;th&gt;Download Link&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Asahi&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;macOS&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi-macos-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;Ubuntu&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi-ubuntu-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;Windows&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi-windows-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Asahi2&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;macOS&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi2-macos-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;Ubuntu&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi2-ubuntu-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;&#x2F;td&gt;&lt;td&gt;Windows&lt;&#x2F;td&gt;&lt;td&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&#x2F;releases&#x2F;download&#x2F;v1.0.4&#x2F;asahi2-windows-latest.zip&quot;&gt;Download&lt;&#x2F;a&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>About the Reverse Phi Motion Effect</title>
        <published>2023-09-12T00:00:00+00:00</published>
        <updated>2023-09-12T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/rphi/"/>
        <id>https://engineering.videocall.rs/posts/rphi/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/rphi/">&lt;h3 id=&quot;decoding-the-reverse-phi-illusion&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Decoding the Reverse Phi Illusion &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;After diving into the code for reverse phi motion, I found myself pondering its intricacies for days. While I can articulate the phenomenon in terms of programming or mathematics, the scientific literature seemed to offer limited insights into whether this illusion is purely retinal or a result of complex neural interactions, such as those involving the geniculate nucleus or the motion-sensitive neurons in V1.&lt;&#x2F;p&gt;
&lt;p&gt;an example of Reverse Phi illusion I coded in Rust &lt;a href=&quot;https:&#x2F;&#x2F;journals.sagepub.com&#x2F;doi&#x2F;full&#x2F;10.1177&#x2F;2041669518815708&quot;&gt;Flynn &amp;amp; Shapiro, 2018 &lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;video width=&quot;320&quot; height=&quot;240&quot; controls&gt;
    &lt;source src=&quot;https:&#x2F;&#x2F;user-images.githubusercontent.com&#x2F;54986652&#x2F;248949171-4d361b74-e377-4409-9286-525614ff92bf.mp4&quot; type=&quot;video&#x2F;mp4&quot;&gt;
    Your browser does not support the video tag.
  &lt;&#x2F;video&gt;
&lt;&#x2F;div&gt;
&lt;h4 id=&quot;a-glimpse-into-comparative-vision-science&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; A Glimpse into Comparative Vision Science &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;While exploring this topic, I stumbled upon an intriguing article titled &lt;a href=&quot;https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;nn.4050&quot;&gt;&quot;Common Circuit Design in Fly and Mammalian Motion Vision&quot;&lt;&#x2F;a&gt;. Although the paper primarily focuses on flies and mice, its findings can be extrapolated to human vision.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;on-and-off-pathways-the-universal-language-of-vision&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; ON and OFF Pathways: The Universal Language of Vision &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Both flies and humans possess separate pathways for processing ON and OFF signals generated by photoreceptors that respond to changes in luminance. These pathways originate at different synapses in the visual system and have distinct anatomical and functional properties. According to a &lt;a href=&quot;https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;29261684&quot;&gt;2017 study&lt;&#x2F;a&gt;, these ON and OFF pathways are sensitive to specific interactions, which may also be applicable to human psychophysics.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;demo-example&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Demo Example &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;To better understand ON and OFF pathways, check out this interactive shader: &lt;a href=&quot;https:&#x2F;&#x2F;www.shadertoy.com&#x2F;view&#x2F;XtdyWj&quot;&gt;ShaderToy Example&lt;&#x2F;a&gt;. This shader illustrates how ON pathways are more activated by a bright stimulus on a dark background, while OFF pathways are more activated by a dark stimulus on a bright background.&lt;&#x2F;p&gt;
&lt;p&gt;Note: That shader code not belong to me!&lt;&#x2F;p&gt;
&lt;h4 id=&quot;the-role-of-correlation-based-algorithms&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; The Role of Correlation-Based Algorithms &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;The article also discusses how both species utilize correlation-based algorithms for motion detection. These algorithms compare temporal changes in luminance at different spatial locations using a multiplication or correlation process. This is in line with another &lt;a href=&quot;https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;18848956&quot;&gt;study&lt;&#x2F;a&gt; that showed equal efficiency in correlating dots of opposite contrast and of similar contrast in reverse-phi motion stimuli.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;reverse-phi-more-than-just-stimulus-properties&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Reverse Phi: More Than Just Stimulus Properties &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;In essence, reverse phi motion is not merely a result of low-level stimulus properties. It appears to be a complex interplay between ON and OFF signals processed through discrete pathways. These interactions can even reverse the direction of motion when contrast changes accompany the discrete motion, as suggested by &lt;a href=&quot;https:&#x2F;&#x2F;dblp.org&#x2F;rec&#x2F;journals&#x2F;neco&#x2F;MoK03&quot;&gt;biophysical simulations&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;why-parallel-processing&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Why Parallel Processing? &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;You might wonder why our visual system even employs this kind of parallel processing. The authors of the article provide an insightful explanation. They suggest that separating ON and OFF signals simplifies the task for motion-sensitive neurons, making it easier to correlate two positive input signals. This seems to resolve a significant biophysical challenge in implementing correlation mechanisms.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;final-thoughts-the-elegance-of-on-and-off-pathways-in-motion-perception&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Final Thoughts: The Elegance of ON and OFF Pathways in Motion Perception &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;As we draw this exploration to a close, let&#x27;s revisit a profound insight from the paper &quot;Common Circuit Design in Fly and Mammalian Motion Vision&quot;. The paper asks an essential question: What could be the advantage of separating ON and OFF signals in both the mouse and fly visual systems?&lt;&#x2F;p&gt;
&lt;p&gt;The separation of ON and OFF pathways in our visual system serves a practical, computational purpose. Motion perception, at its core, involves the temporal correlation of similar events at distinct spatial locations. When a bright object moves across our field of vision, the neurons responsible for detecting this motion have to process both the increasing and the decreasing luminance as the object passes by.&lt;&#x2F;p&gt;
&lt;p&gt;The challenge lies in the biophysics: there&#x27;s no known mechanism that allows a postsynaptic neuron to get excited both when two inputs increase and when they decrease their membrane potential. This is where the genius of biological design comes into play. By segregating ON and OFF signals, each dealing exclusively with brightness increments or decrements, the visual system effectively simplifies the task for motion-sensitive neurons. Now, they only have to correlate two positive input signals, significantly easing the computational burden.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>My thesis in TLDR</title>
        <published>2023-01-01T00:00:00+00:00</published>
        <updated>2023-01-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/thesis/"/>
        <id>https://engineering.videocall.rs/posts/thesis/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/thesis/">&lt;h4 id=&quot;on-which-stage-we-perceive-differently-when-we-classified-faces&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt;On which stage we perceive differently when we classified faces?&lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;When I started reading about face perception, I found many studies. Most of these studies say that people see faces from their own race or species differently than others. But none of these studies answered my main question: At what point do we actually start to see these faces differently?&lt;&#x2F;p&gt;
&lt;p&gt;When does this happen? Right when the light hits our retinas? Or maybe a bit later, when the signals reach the occipital lobe at the back of our brains? Or is it even later, when our brain&#x27;s &quot;face recognition software&quot; kicks in?&lt;&#x2F;p&gt;
&lt;h3 id=&quot;clarifying-the-term-race-in-my-research&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Clarifying the Term &#x27;Race&#x27; in My Research &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;In my thesis, I use the term &quot;Race&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;I want to make it clear that I don&#x27;t see &quot;race&quot; as a biological concept, and I&#x27;m not approaching this from a &quot;racial&quot; perspective. Instead, I borrowed the term from scientific studies that discuss the &quot;other-race effect in face perception.&quot;&lt;&#x2F;p&gt;
&lt;p&gt;In simpler words, when I say &quot;Race&quot; in my research, I&#x27;m talking about faces that you don&#x27;t usually see in your day-to-day life. For example, in my experiments conducted in Antalya, I used &quot;Asian&quot; faces as the &quot;other&quot; category. Why? Because there aren&#x27;t many Asian people in Antalya, so these faces would be less &quot;familiar&quot; to the participants.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;how-i-tried-to-answer-this-question&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; How I tried to answer this question? &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;In the world of vision science, the visual hierarchy theory is a cornerstone. It tells us that there are three main stages when our brain classifies what we see:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Superordinate Level: Is it alive or not?&lt;&#x2F;li&gt;
&lt;li&gt;Basic Level: What kind of thing is it? (e.g., a dog)&lt;&#x2F;li&gt;
&lt;li&gt;Subordinate Level: What specific type is it? (e.g., a pitbull)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Interestingly, &lt;a href=&quot;https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;25208739&#x2F;&quot;&gt;research&lt;&#x2F;a&gt; has shown that our brain can spot an animal in just 120 milliseconds but takes a bit longer to figure out it&#x27;s a dog. This quick initial categorization happens at the Basic Level, which aligns perfectly with the focus of my experiments.&lt;&#x2F;p&gt;
&lt;p&gt;So, if you see a pitbull, your brain first decides it&#x27;s a living thing, then identifies it as a dog, and finally as a pitbull. Given this, I chose to zoom in on the &quot;Basic Level&quot; for my experiments. Participants were asked a simple question: &quot;If you see a face, click on X; if you don&#x27;t see a face, click on Y.&quot;&lt;&#x2F;p&gt;
&lt;p&gt;You might be wondering, &quot;Why did I choose to focus on the &#x27;Basic Level&#x27; and not go deeper into the &#x27;Subordinate Level&#x27;?&quot; The answer lies in my interest in low-level visual properties. By keeping the research at the Basic Level, I could delve into the nitty-gritty of early visual processing without getting entangled in the complexities that come with higher-level categorizations like gender or specific facial features.&lt;&#x2F;p&gt;
&lt;p&gt;Moreover, a glance at the current ERP-face literature reveals that many studies are already exploring questions like &quot;What gender is this face?&quot; or &quot;Is this face familiar?&quot;, even some attentional oddbal tasks... These questions often require processing at the Subordinate Level, which involves more complex and high-level facial features. My aim was to strip away these complexities and get down to the basics—literally. This approach allowed me to isolate and study the fundamental visual cues that our brains use for face perception, making the research as low-level as possible.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;but-thats-not-enough&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; But thats not enough... &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;This could be answer my question partly but I wanted to know more. So I decided to use &quot;Spatial Frequency&quot;, more precisely &quot;Coarse-to-Fine&quot; theory in human vision. According to this theory, when we see an object in the world, we first perceive it&#x27;s coarse features (low spatial frequency) and then
fine features (high spatial frequency) so in the brain a fast-forward mechanism works like this: coarse features -&amp;gt; fine features -&amp;gt; classification.&lt;&#x2F;p&gt;
&lt;p&gt;See bellow image for more details:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;&#x2F;images&#x2F;sf.png&quot; alt=&quot;SF&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;How brain process a visual object&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h4 id=&quot;so-what-i-did&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; So what I did? &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Armed with Python code and some nifty image processing techniques like Fourier Transform, RMSE, and luminance normalization, I set out to explore Spatial Frequencies (SFs). I designed my experiment using E-Prime software. A big shoutout to my awesome colleagues Nurullah, Furkan, and Semih for their invaluable help! Together, we conducted the experiment in our lab and managed to gather data from 30 participants using EEG, all within two weeks.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-i-analyzed&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; What I analyzed? &lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;I dived deep into the Event-Related Potentials (ERPs) for each participant. Specifically, I looked at:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;P100&lt;&#x2F;strong&gt;: Important for early visual processing&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;N170&lt;&#x2F;strong&gt;: Crucial for face perception&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;N250&lt;&#x2F;strong&gt;: Relevant for recognizing familiar faces&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;the-tools-which-electrodes-did-i-use&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; The Tools: Which Electrodes Did I Use? &lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;For the EEG recordings, I used a variety of electrodes but focused my reporting on those placed on the occipital lobe, which is the brain&#x27;s visual processing center. The electrodes I reported are:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;O1 and O2&lt;&#x2F;strong&gt;: Standard occipital electrodes&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;PO3 and PO4&lt;&#x2F;strong&gt;: Parieto-occipital electrodes&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;PO7 and PO8&lt;&#x2F;strong&gt;: Additional parieto-occipital electrodes&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;what-i-found-the-key-takeaways&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; What I Found: The Key Takeaways&lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;I discovered quite a bit through my research, but I&#x27;ve focused on reporting the most crucial findings.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;No Other-Race Effect in Early Stages&lt;&#x2F;strong&gt;: While I did find some significant results in the N250 ERP (especially in the PO electrodes), there were no significant differences in the P100 and N170 ERPs between Asian and Caucasian faces. This suggests that, at least in the early stages of visual processing, we don&#x27;t perceive faces of different races differently.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Species Differences in Higher Frequencies&lt;&#x2F;strong&gt;: Interestingly, faces of other species didn&#x27;t differ from human faces in the low spatial frequency domain. However, we do perceive them differently in the high spatial frequency (HSF) or broadband, especially in the P100 ERP. This difference becomes even more distinct in the N170 and N250 ERPs.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;My thesis is still under review, but I&#x27;ve conducted many detailed analyses and learned a lot. You can find the published version of my thesis &lt;a href=&quot;https:&#x2F;&#x2F;www.tandfonline.com&#x2F;doi&#x2F;full&#x2F;10.1080&#x2F;13506285.2024.2415721#abstract&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;&#x2F;strong&gt;: Due to the length of the paper, I didn&#x27;t include the N250 results in the pre-print version. Rest assured, they are thoroughly reported in my full thesis.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;softwares-and-tools-i-used&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Softwares and Tools I Used&lt;&#x2F;span&gt;&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;open-source-contributions&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Open-Source Contributions&lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;I love open-source and have contributed some tools that I wrote myself:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;easylab&quot;&gt;EasyLab&lt;&#x2F;a&gt;&lt;&#x2F;strong&gt;: I created a Python GUI as a gift to my lab! It&#x27;s perfect for resizing images, renaming them with a prefix for easy reading in E-Prime 3.0, and even processing Spatial Frequencies using Fourier and Butterworth filters.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;scramblery&quot;&gt;Scramblery&lt;&#x2F;a&gt;&lt;&#x2F;strong&gt;: This tool is for scrambling images. I used scrambled images as a control condition in my research. It&#x27;s open-source, and there&#x27;s even a cool JavaScript version that runs online!&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Try it out here: &lt;a href=&quot;https:&#x2F;&#x2F;altunenes.github.io&#x2F;scramblery&#x2F;scramblerydemo.html&quot;&gt;Demo&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;butter2d&quot;&gt;butter2d&lt;&#x2F;a&gt;&lt;&#x2F;strong&gt;: Rust implementation for Butterworth filter.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;proprietary-software&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Proprietary Software&lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;BrainVision&lt;&#x2F;strong&gt;: I used this for my EEG recordings and initial data filtering. Unfortunately, it&#x27;s not open-source.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;E-Prime 3.0&lt;&#x2F;strong&gt;: I used this for my experiment design. It&#x27;s buggy and makes you want to pull your hair out, but it&#x27;s the best option out there for designing experiments. It&#x27;s also not open-source. Made me think about creating an open-source alternative in the future using Rust while I was using it. Maybe one day!&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;data-analysis&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; Data Analysis&lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;R&lt;&#x2F;strong&gt;: After collecting the data, I did all my analyses using R. You can check out my workflow in this &lt;a href=&quot;https:&#x2F;&#x2F;gist.github.com&#x2F;altunenes&#x2F;7081d34140335dd7764a92a7bfd12f1d&quot;&gt;gist file&lt;&#x2F;a&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;the-fuel-coffee-music-and-friends&quot;&gt;&lt;span style=&quot;color:orange;&quot;&gt; The Fuel: Coffee, Music and Friends&lt;&#x2F;span&gt;&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Coffee&lt;&#x2F;strong&gt;: Consumed in large quantities. I wasn&#x27;t picky about the brand.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Music&lt;&#x2F;strong&gt;: Candlemass kept me company during those long hours of research. I dig into the doom.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Friends&lt;&#x2F;strong&gt;: I couln&#x27;t have done it without my friends. They were both my emotional and technical support. Thank you Nurullah, Furkan and Semih! I&#x27;m forever grateful for your help. &amp;lt;3
Friendship in the lab is the best thing ever. I&#x27;m so lucky to have you guys!&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;paper-link&quot;&gt;Paper Link&lt;&#x2F;h3&gt;
&lt;p&gt;You can read the more boring parts on &lt;a href=&quot;https:&#x2F;&#x2F;www.tandfonline.com&#x2F;doi&#x2F;full&#x2F;10.1080&#x2F;13506285.2024.2415721#abstract&quot;&gt;here&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Why I love optical illusions</title>
        <published>2022-08-02T00:00:00+00:00</published>
        <updated>2022-08-02T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://engineering.videocall.rs/posts/illusions/"/>
        <id>https://engineering.videocall.rs/posts/illusions/</id>
        
        <content type="html" xml:base="https://engineering.videocall.rs/posts/illusions/">&lt;p&gt;Ever stumbled upon something so interesting that you just couldn&#x27;t let it go? That&#x27;s what happened to me with optical illusions. My first exposure to optical illusions was in my undergraduate intro textbooks, and that was the point from which I was captured. So, whenever I see a new illusion, mostly on Twitter, my day is ruined because my brain just won&#x27;t let it go until I try to reproduce it with coding and understand the mechanism at a low level.&lt;&#x2F;p&gt;
&lt;p&gt;Four years (2018-2022) ago my coding skills were. well, let&#x27;s say, they were just born. But the challenge to create these optical illusions was very tempting. I wanted to dig deeper into this, so as to control the illusion. I saw how with just small changes in the code, it became stronger or disappeared.&lt;&#x2F;p&gt;
&lt;p&gt;It had been a very steep learning curve, but every line of code was a step closer to finally unraveling the mysteries of human vision. After so much coding and debugging, I finally reached a repository using which you can display and manipulate almost all famous optical illusions with just a few lines of code &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;sorceress&quot;&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;sorceress&lt;&#x2F;font&gt;&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;. And for those curious minds, I&#x27;ve also documented the science and explanations behind each illusion. Not only illusions explained, but you can also find the API documentation for humans, of course &lt;a href=&quot;https:&#x2F;&#x2F;altunenes.github.io&#x2F;sorceress&#x2F;explanations%20of%20illusions&#x2F;&quot;&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;here&lt;&#x2F;font&gt;&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Sometimes I look at my old code and cringe a little, but then I smile—it shows me only how far I came. From a total beginner to this comprehensive repo on Optical Illusions, the journey has been quite something. I don&#x27;t really contribute to this repository since I mostly write my code in Rust. So if you are in search of more dynamic, solid, and the latest illusions, check my other repository
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;altunenes&#x2F;rusty_art&quot;&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;Rust&lt;&#x2F;font&gt;&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;.
.&lt;&#x2F;p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;video width=&quot;320&quot; height=&quot;240&quot; controls&gt;
    &lt;source src=&quot;https:&#x2F;&#x2F;user-images.githubusercontent.com&#x2F;54986652&#x2F;248949171-4d361b74-e377-4409-9286-525614ff92bf.mp4&quot; type=&quot;video&#x2F;mp4&quot;&gt;
    Your browser does not support the video tag.
  &lt;&#x2F;video&gt;
&lt;&#x2F;div&gt;
&lt;p&gt;But why in the world, you may ask, why all this effort? Well, to be totally honest, I really don&#x27;t know.&lt;&#x2F;p&gt;
&lt;p&gt;This is actually a question that I always come across, especially on interviews that I mostly bomb. But the answer probably lies in the complexity of human vision. I believe most of the understanding of vision from us is based on implicit assumptions. Optical illusions challenge these assumptions more like little experiments and with an irresistibly captivating factor about them: seeing how easily our vision can be played with. And let&#x27;s not forget the human curiosity factor. Perhaps one of the questions for research could be the fascination of optical illusions and, in fact, lifelong attempts to understand them. Yes, I do realize that up to now I haven&#x27;t had any concrete answer to the question. I tried to play tricks with your brain through the illusion to let you think I have a concrete answer. I am sorry for that.&lt;&#x2F;p&gt;
</content>
        
    </entry>
</feed>
